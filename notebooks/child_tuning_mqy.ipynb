{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7292808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d03e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In utils/hub.py: default_cache_path: /raid3/xd/.cache/torch/hub->/nas/xd/.cache/torch/transformers/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src')\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid3/xd/.cache/torch'  # deliberately set this wrong path to avoid migrating cache\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8,7\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, fields, asdict\n",
    "import itertools\n",
    "from itertools import chain, product\n",
    "import math\n",
    "from functools import reduce, partial\n",
    "from collections.abc import Iterable\n",
    "from collections import namedtuple \n",
    "import traceback\n",
    "import pickle, gzip\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# from torch.multiprocessing import Pool\n",
    "# torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer#, pipeline\n",
    "# from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "# from transformers import T5Tokenizer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "# from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c8f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _ as __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cba5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_utils ... done 0:00:00.000042\n",
      "utils ... done 0:00:00.003467\n",
      "child_utils ... In const.py: Loading tokenizer ... done 0:00:00.190295\n",
      "done 0:00:03.915501\n",
      "tasks ... done 0:00:00.001828\n",
      "model_utils ... done 0:00:08.370531\n",
      "weight_analysis ... done 0:00:00.000030\n"
     ]
    }
   ],
   "source": [
    "from common_utils import Timer\n",
    "with Timer('common_utils'): from common_utils import *\n",
    "with Timer('utils'): from utils import *\n",
    "with Timer('child_utils'): from child_utils_mqy import *\n",
    "from child_utils import _str, _cxt2str, _item2str, _s, _be\n",
    "from child_frames import *\n",
    "with Timer('tasks'): from tasks import *\n",
    "with Timer('model_utils'): from model_utils_mqy import *\n",
    "with Timer('weight_analysis'): from weight_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# cache_dir = '/mnt/nvme1/xd/.cache/torch/transformers/'  # for gpt-neox-20b on elderberry\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ab655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B/cpu ... done 0:05:07.162101\n"
     ]
    }
   ],
   "source": [
    "# curl -x http://192.168.50.1:1081 -L -O [-C -] https://huggingface.co/google/ul2/resolve/main/pytorch_model.bin  # -C for 断点续传\n",
    "s2s_model_names = ['google/t5-xl-lm-adapt', 'google/t5-xxl-lm-adapt', 'bigscience/T0p', 'bigscience/T0_3B', \n",
    "    'allenai/tk-instruct-3b-pos', 'allenai/tk-instruct-3b-def-pos', 'google/ul2']\n",
    "gpt_model_names = ['EleutherAI/gpt-j-6B/cpu', #'EleutherAI/gpt-j-6B',\n",
    "#                    'EleutherAI/gpt-neox-20b/cpu', #'EleutherAI/gpt-neox-20b'\n",
    "                  ]#, 'gpt2-xl', 'gpt2']\n",
    "#                    'KoboldAI/fairseq-dense-6.7B', 'KoboldAI/fairseq-dense-13B']\n",
    "for model_name in s2s_model_names[:0] + gpt_model_names[:1]:#, 'gpt2-xl', 'EleutherAI/gpt-neo-1.3B', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name in models: continue\n",
    "    with Timer(model_name):\n",
    "        model_cls = AutoModelForCausalLM if any(s in model_name for s in ['gpt', 'fairseq-dense']) else T5ForConditionalGeneration\n",
    "        # _cache_dir = cache_dir.replace('/nas/', '/nas2/') if 'gpt' not in model_name else cache_dir\n",
    "        kwargs = dict(cache_dir=cache_dir, proxies=proxies, low_cpu_mem_usage=True)\n",
    "        if '/cpu' in model_name or 'gpt-j' not in model_name and 'gpt-neox' not in model_name:\n",
    "            model = model_cls.from_pretrained(model_name.replace('/cpu', ''), cache_dir=cache_dir, proxies=proxies)\n",
    "        elif 'gpt-j' in model_name:\n",
    "            device = 6\n",
    "            model = model_cls.from_pretrained(model_name, revision=\"float16\", torch_dtype=torch.float16, **kwargs).to(device)\n",
    "        elif 'gpt-neox' in model_name:\n",
    "            device = 8; device_map = {'gpt_neox': device, 'embed_out': device}\n",
    "            model = model_cls.from_pretrained(model_name, device_map=device_map, load_in_8bit=True, **kwargs)\n",
    "        if hasattr(model.config, 'use_cache'): model.config.use_cache = False  # save GPU mem\n",
    "        # if model_name in ['EleutherAI/gpt-neox-20b']: model = model.half()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name.replace('/cpu', ''), cache_dir=cache_dir)\n",
    "        unify(model)\n",
    "        models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc28755",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = gpt_model_names[0]  # engines[4]\n",
    "model, tokenizer = models[model_name]\n",
    "model_name_gpu = model_name.replace('/cpu', '')\n",
    "model_gpu = models[model_name_gpu][0] if model_name_gpu in models else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b77b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = model.transformer.h\n",
    "for i, b in enumerate(blocks): b.layer = i\n",
    "ln_f = model.transformer.ln_f\n",
    "L, H, embed_dim = len(blocks), blocks[0].attn.num_heads, blocks[0].attn.embed_dim\n",
    "\n",
    "# we = model.transformer.wte.weight.data\n",
    "# wu = model.lm_head.weight.data\n",
    "\n",
    "# es = [we]\n",
    "# for b in blocks[:1]: es.append(es[-1] + mlp_forward(b, es[-1]))\n",
    "# model.es = es\n",
    "# weBTAs = [es[i].T @ es[i] for i in range(2)]\n",
    "# model.weBTAs = weBTAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175ae7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloning GPTJBlock0.ln_1 to cuda:5 ... done 0:00:07.951887\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.036742\n",
      "cloning GPTJBlock1.ln_1 to cuda:5 ... done 0:00:00.000591\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.031830\n",
      "cloning GPTJBlock2.ln_1 to cuda:5 ... done 0:00:00.000548\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.028515\n",
      "cloning GPTJBlock3.ln_1 to cuda:5 ... done 0:00:00.000540\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.029912\n",
      "cloning GPTJBlock4.ln_1 to cuda:5 ... done 0:00:00.000809\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.028312\n",
      "cloning GPTJBlock5.ln_1 to cuda:5 ... done 0:00:00.000953\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027501\n",
      "cloning GPTJBlock6.ln_1 to cuda:5 ... done 0:00:00.000948\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027716\n",
      "cloning GPTJBlock7.ln_1 to cuda:5 ... done 0:00:00.000762\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027576\n",
      "cloning GPTJBlock8.ln_1 to cuda:5 ... done 0:00:00.000520\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027851\n",
      "cloning GPTJBlock9.ln_1 to cuda:5 ... done 0:00:00.000531\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027478\n",
      "cloning GPTJBlock10.ln_1 to cuda:5 ... done 0:00:00.000523\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027494\n",
      "cloning GPTJBlock11.ln_1 to cuda:5 ... done 0:00:00.000521\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027367\n",
      "cloning GPTJBlock12.ln_1 to cuda:5 ... done 0:00:00.000514\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.028007\n",
      "cloning GPTJBlock13.ln_1 to cuda:5 ... done 0:00:00.000537\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.056861\n",
      "cloning GPTJBlock14.ln_1 to cuda:5 ... done 0:00:00.000561\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.030233\n",
      "cloning GPTJBlock15.ln_1 to cuda:5 ... done 0:00:00.000541\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024030\n",
      "cloning GPTJBlock16.ln_1 to cuda:5 ... done 0:00:00.000510\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.023848\n",
      "cloning GPTJBlock17.ln_1 to cuda:5 ... done 0:00:00.000526\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.023457\n",
      "cloning GPTJBlock18.ln_1 to cuda:5 ... done 0:00:00.000511\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.023718\n",
      "cloning GPTJBlock19.ln_1 to cuda:5 ... done 0:00:00.000509\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.023873\n",
      "cloning GPTJBlock20.ln_1 to cuda:5 ... done 0:00:00.000555\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.029541\n",
      "cloning GPTJBlock21.ln_1 to cuda:5 ... done 0:00:00.000863\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024804\n",
      "cloning GPTJBlock22.ln_1 to cuda:5 ... done 0:00:00.001128\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024716\n",
      "cloning GPTJBlock23.ln_1 to cuda:5 ... done 0:00:00.001009\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024443\n",
      "cloning GPTJBlock24.ln_1 to cuda:5 ... done 0:00:00.000545\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024680\n",
      "cloning GPTJBlock25.ln_1 to cuda:5 ... done 0:00:00.000568\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024367\n",
      "cloning GPTJBlock26.ln_1 to cuda:5 ... done 0:00:00.000660\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024583\n",
      "cloning GPTJBlock27.ln_1 to cuda:5 ... done 0:00:00.000546\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.023872\n",
      "cloning GPTJModel.ln_f to cuda:5 ... done 0:00:00.000563\n",
      "cloning GPTJForCausalLM.lm_head to cuda:5 ... done 0:00:00.367232\n",
      "mem_usage before / after clone_model_to: [0, 0, 654] / [1290, 1292, 1946]\n"
     ]
    }
   ],
   "source": [
    "# clone model \n",
    "device = torch.device('cuda:5')\n",
    "_ = clone_model_to(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706055f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "#     (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(genders_of_persons).use(['equal', 'child', 'sibling'])], MlM_gen, None, None,\n",
    "#     ), \n",
    "#      (lambda: [TreeSet(types_of_things).use(['equal', 'child', 'sibling']), TreeSet(types_of_things).use(['equal', 'child', 'sibling'])], MlM_gen, None, None,\n",
    "#      ), \n",
    "#     (lambda: [TreeSet(countries_of_cities).use(['equal', 'child', 'sibling']), TreeSet(countries_of_cities).use(['equal', 'child', 'sibling'])], MlM_gen, None, None,\n",
    "#     ),\n",
    "#     (lambda: fork_vocab(TreeSet(capabilities_of_things), [['equal', 'child', 'sibling']] * 2), MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: [SymSet(person_adjs).use(['equal']), SymSet(person_adjs).use(['opposite'])], MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: fork_vocab(PoSet(temporal_posets), [['equal'], ['prev']]), MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: fork_vocab(PoSet(temporal_posets), [['equal'], ['next']]), MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: [TreeSet(types_of_characters).use('child'), TreeSet(types_of_things).use('child')], partial(MlM_gen, cxt_sample_fn=enumerate_sample, query=1),\n",
    "#      partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {i[1]}\", f\"{the_(i[1])} is {i[0]}'s\"]), lambda q, _: f\"{q}\", \" likes\"\n",
    "#     ),\n",
    "   (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(types_of_things).use(['equal', 'child', 'sibling'])], MlM_gen,\n",
    "    partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {a_(i[1])}\", f\"{_be(the_(i[1]))} {i[0]}'s\"]), lambda q, _: f\"{the_(q)} likes\",\n",
    "#      partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {a_(i[1])}\", f\"{_be(the_(i[1]))} {i[0]}'s\"]), lambda q, _: f\"What {the_(q)} has is a kind of\",\n",
    "   ), # t: 21-5, 15-8, 19. p: 16-7, 18-5, [3-12, 13-7]. p+: 16-7, 16-0. 13-7:induction head qk, thing->type ov\n",
    "#    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(countries_of_cities).use(['child', 'equal', 'sibling'])], MlM_gen,\n",
    "#     partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} likes {i[1]}\", f\"{i[1]} attracts {the_(i[0], uppercase=False)}\"]), lambda q, _: f'{the_(q)} wanna go to',\n",
    "#    ), # t: 19-12 >> 16-10 = 12-7\n",
    "#    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(capabilities_of_things).use(['child'])], MlM_gen,\n",
    "#     partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {a_(i[1])}\", f\"{_be(the_(i[1]))} {i[0]}'s\"]), lambda q, _: f'{q} can'\n",
    "#    ),\n",
    "     (lambda: [TreeSet(genders_of_persons).use('equal'), SymSet(person_adjs).use('equal')], MlM_gen,\n",
    "      partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} is {i[1]}\", f\"{i[1].capitalize()} is {i[0]}\"]), lambda q, _: f\"Yes, {q} looks\", \" like\"\n",
    "     ),\n",
    "    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), SymSet(person_adjs).use(['opposite'])], MlM_gen,\n",
    "     partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} is {i[1]}\", f\"{capitalize(i[1])} is {i[0]}\"]), lambda q, _: f\"{the_(q)} is not\",\n",
    "    ), # t: 16-14, somewhat 14-7 # verbose acc: gpj-j > curie-001 > davinci-001 > gpt-neox!? abstract acc: gpt-neox > gpt-j. all poor (inc. davinci-002!)\n",
    "#     (lambda: [TreeSet(genders_of_persons).use('equal'), PoSet(temporal_posets).use('equal')], MlM_gen,\n",
    "#      partial(_cxt2str, item2str=lambda i, _: [f'{i[0]} arrived {wrap_noun2(i[1])}', f'{wrap_noun2(i[1]).capitalize()} arrived {i[0]}']), lambda q, _: f\"So {q}'s arrival time\", ' is'\n",
    "#     ),\n",
    "    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), PoSet(temporal_posets).use(['prev'])], MlM_gen,\n",
    "     partial(_cxt2str, item2str=lambda i, _: [f'{i[0]} arrived {prep_(i[1])}', f'{capitalize(prep_(i[1]))} arrived {i[0]}']), lambda q, _: f'{the_(q)} arrived just before'\n",
    "    ),\n",
    "    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), PoSet(temporal_posets).use(['next'])], MlM_gen,\n",
    "     partial(_cxt2str, item2str=lambda i, _: [f'{i[0]} arrived {prep_(i[1])}', f'{capitalize(prep_(i[1]))} arrived {i[0]}']), lambda q, _: f'{the_(q)} arrived just after'\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f62b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(key); print_tree(r.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b113734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "results = {}; key = None\n",
    "#print(device)\n",
    "#model.lm_head = copy.deepcopy(model_gpu.lm_head)\n",
    "#model.lm_head.to('cpu')\n",
    "#print(model.lm_head.weight.device)\n",
    "#models.keys()\n",
    "#print(save_results)\n",
    "# model_gpu.lm_head.to(device)\n",
    "# print(model_gpu.lm_head.weight.device)\n",
    "# switch_model_to(model, device)\n",
    "# print('lm_head', model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6acd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}; key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f13e0c0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-->task: (<function <lambda> at 0x7f617b50e050>, <function MlM_gen at 0x7f6050194680>, functools.partial(<function _cxt2str at 0x7f60501949e0>, item2str=<function <lambda> at 0x7f617b50e0e0>), <function <lambda> at 0x7f617b50e170>) rel0_i:0, rel1_i:0, do_swap_qa:False, do_negate:False, do_rm_query:False, rev_item2str:False, do_g2c:False\n",
      "\n",
      "== MlM_gen[genders_of_persons.TreeSet.equal,types_of_things.TreeSet.equal][cxt_len=3] == \n",
      "David has a laptop. Sharon has a rabbit. Barbara has a plum. Barbara likes plum\n",
      "Donna has spaghetti. Michael has a plum. John has a jersey. John likes jersey\n",
      "Steven has vodka. Anthony has a rabbit. Thomas has a pig. Thomas likes pig\n",
      "John has a goat. Laura has pink. Jeff has a jersey. Laura likes pink\n",
      "0.8258377909660339 0.7222222222222222\n",
      "attribute_tree ... attribute_tree 2 []\n",
      "_topk 10\n",
      "In attribute_tree: attribute_step , topk=10 ... attr step fns []\n",
      "post_forward_fn []\n",
      "num_points 3 0 4\n",
      "scaled_x_ [('mlp_mask', torch.Size([4, 28])), ('embed_mask', torch.Size([4, 1])), ('head_mask', torch.Size([4, 28, 16]))]\n",
      "y torch.Size([4])\n",
      "inner attr 0 mlp_mask torch.Size([1, 28]) torch.Size([4, 28])\n",
      "inner attr 1 embed_mask torch.Size([1, 1]) torch.Size([4, 1])\n",
      "inner attr 2 head_mask torch.Size([1, 28, 16]) torch.Size([4, 28, 16])\n",
      "attr step fns []\n",
      "post_forward_fn []\n",
      "num_points 3 0 4\n",
      "scaled_x_ [('mlp_mask', torch.Size([4, 28])), ('embed_mask', torch.Size([4, 1])), ('head_mask', torch.Size([4, 28, 16]))]\n",
      "y torch.Size([4])\n",
      "inner attr 0 mlp_mask torch.Size([1, 28]) torch.Size([4, 28])\n",
      "inner attr 1 embed_mask torch.Size([1, 1]) torch.Size([4, 1])\n",
      "inner attr 2 head_mask torch.Size([1, 28, 16]) torch.Size([4, 28, 16])\n",
      "done 0:00:00.428433\n",
      "top_heads [(18, 13), (16, 7), (13, 7), (16, 0), (23, 14), (19, 4), (18, 9), (17, 9), (21, 13), (20, 1)]\n",
      "top_heads [(18, 13), (16, 7), (13, 7), (16, 0), (23, 14), (19, 4), (18, 9), (17, 9), (21, 13), (20, 1)]\n",
      "In attribute_tree: attribute_step stage2  ... attr step fns []\n",
      "post_forward_fn []\n",
      "num_points 3 0 4\n",
      "scaled_x_ [('attn_weights', torch.Size([4, 10, 221, 221]))]\n",
      "y torch.Size([4])\n",
      "inner attr 0 attn_weights torch.Size([1, 10, 221, 221]) torch.Size([4, 10, 221, 221])\n",
      "attr step fns []\n",
      "post_forward_fn []\n",
      "num_points 3 0 4\n",
      "scaled_x_ [('attn_weights', torch.Size([4, 10, 228, 228]))]\n",
      "y torch.Size([4])\n",
      "inner attr 0 attn_weights torch.Size([1, 10, 228, 228]) torch.Size([4, 10, 228, 228])\n",
      "done 0:00:00.077450\n",
      "In _add_node: add @0 18-13 100 B->A0 93/22/96/-3.3 attn/ans0s\n",
      "In _add_node: add @0 18-13 100 B->A0 93/22/96/-3.3 attn:B->~<s>\n",
      "In _add_node: add @1 16-7 92 B->A0 95/33/95/-3.5 attn/ans0s\n",
      "In _add_node: add @1 16-7 92 B->A0 95/33/95/-3.5 attn:B->~<s>\n",
      "In _add_node: add @2 13-7 59 B->A0 92/48/91/-3.2 attn/ans0s\n",
      "In _add_node: add @2 13-7 59 B->A0 92/48/91/-3.2 attn:B->~<s>\n",
      "In _add_node: add @3 16-0 55 B->A0 94/36/67/-3.2 attn/ans0s\n",
      "In _add_node: add @3 16-0 55 B->A0 94/36/67/-3.2 attn:B->~<s>\n",
      "In _add_node: add @4 23-14 49 B->A0 93/21/-10/-3.8 attn/ans0s\n",
      "In _add_node: add @5 19-4 37 B->A0 84/15/77/-4.0 attn/ans0s\n",
      "In _add_node: add @5 19-4 37 B->A0 84/15/77/-4.0 attn:B->~<s>\n",
      "In _add_node: add @7 17-9 33 B->A0 91/22/88/-3.3 attn/ans0s\n",
      "In _add_node: add @7 17-9 33 B->A0 91/22/88/-3.3 attn:B->~<s>\n",
      "In _add_node: add @6 18-9 34 B->B 54/29/34/-3.7\n",
      "In _add_node: add @40 14-6 7 B->A0/54/52 attn/ans0s\n",
      "In _add_node: add @40 14-6 7 B->A0/54/52 attn:B->~<s>\n",
      "In _add_node: add @396 13-2 -2 B->A0/37/7 attn/ans0s\n",
      "In _add_node: add @396 13-2 -2 B->A0/37/7 attn:B->~<s>\n",
      "In _add_node: add @470 20-0 -31 B->A0/37/72 attn/ans0s\n",
      "In _add_node: add @470 20-0 -31 B->A0/37/72 attn:B->~<s>\n",
      "In _add_node: add @[0,1,2,3,4,5,7,8,9] 18-13,16-7,13-7,16-0,23-14,19-4,17-9,21-13 29,20-1 29 B->A0 79\n",
      "In _add_node: add @[6] 18-9 unk 0\n",
      "done 0:00:00.695829\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}; key = None\n",
    "nrows, k_shot = 12, 3; cxt_len = 3; save_results = True\n",
    "batch_size = 2; verbose = False #not save_results or batch_size <= 8\n",
    "rel1_kwargs = {'x_f': None}  # {'x_f': _s, 'y_f': a_, 'skip_inv_f':False}\n",
    "for task,        rel0_i, rel1_i, do_swap_qa, do_negate, do_rm_query, rev_item2str, do_g2c in product(\n",
    "    #tasks[0:1], [0],[1,],[False],  [True],[True,],[False,],[False,]):\n",
    "    tasks[0:1], [0],[0,],[False],  [False],[False,],[False,],[False,]):\n",
    "    print('\\n-->task:',task, f'rel0_i:{rel0_i}, rel1_i:{rel1_i}, do_swap_qa:{do_swap_qa}, do_negate:{do_negate}, do_rm_query:{do_rm_query}, rev_item2str:{rev_item2str}, do_g2c:{do_g2c}')\n",
    "#     tasks[0:1],range(1,2),range(1),[True],[False,True],[True],[False,],[False,],[False,]):\n",
    "    seed(42)\n",
    "    args = dict(cxt_len=cxt_len, rev_item2str=rev_item2str, abstract=False)\n",
    "    trans_args = dict(rel0_i=rel0_i, rel1_i=rel1_i, rel1_kwargs=rel1_kwargs, do_swap_qa=do_swap_qa, do_negate=do_negate,\n",
    "                      do_rm_query=do_rm_query, do_g2c=do_g2c)\n",
    "    task = transform_task(task, **trans_args)\n",
    "    if task is None: print('task is None! skip.'); continue\n",
    "    res_key = f'{task2str(task)}[{args2str(args)}]'# + composed_heads2str(model)\n",
    "    if key is not None and res_key != key: continue\n",
    "    if not validate_args(task, args, trans_args): print(f'invalid args {res_key}! skip.'); continue\n",
    "    print(f'\\n== {res_key} == {args2str(trans_args)}')\n",
    "    r = results[res_key] if save_results and res_key in results else None\n",
    "#     if r is not None: print('duplicate task!'); continue \n",
    "    r = generate_and_predict_batch(model if save_results else model_gpu, tokenizer, task, nrows, k_shot, batch_size,\n",
    "            custom_forward=save_results, result=r, verbose=verbose, **args)\n",
    "    if save_results: results[res_key] = r\n",
    "#     if True or not save_results or getattr(r, 'mean_acc', 0) < 0.45: continue\n",
    "    #switch_model_to(model_gpu, device)\n",
    "    #print(model.lm_head)\n",
    "    if True or r.root is None: r.root = add_node(None, layer=L, label_type='labels')\n",
    "    r.root = attribute_tree_on(r.data_tuples, model, r.root, -1, topk=10, k_shot=k_shot, mix=True, device=device, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fe67beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "inputs_embeds torch.Size([1, 221, 4096])\n",
      "position_embeds None\n",
      "attn_outputs 28 torch.Size([1, 221, 4096])\n",
      "values 28 torch.Size([1, 16, 221, 256])\n",
      "attn_outs 28 torch.Size([1, 16, 221, 256])\n",
      "head_inputs None\n",
      "head_outputs None\n",
      "intermediates ()\n",
      "mlp_outputs 28 torch.Size([1, 221, 4096])\n",
      "hidden_states 30 torch.Size([1, 221, 4096])\n",
      "attentions 28 torch.Size([1, 16, 221, 221])\n",
      "logits torch.Size([1, 221, 50400])\n",
      "labels None\n",
      "loss None\n",
      "attn_attr odict_keys([])\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "print(len(r.data_tuples))\n",
    "outputs = r.data_tuples[0][-1]\n",
    "fields = dataclasses.fields(outputs)\n",
    "# print(fields)\n",
    "for fd in fields:\n",
    "#     print(fd.name)\n",
    "    v = getattr(outputs, fd.name)\n",
    "#     print(fd.name, type(v))\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(fd.name, v.shape)\n",
    "    elif isinstance(v, tuple) and len(v) >0:\n",
    "        print(fd.name, len(v),v[0].shape)\n",
    "    elif isinstance(v, dict):\n",
    "        print(fd.name, v.keys())\n",
    "    else:\n",
    "        print(fd.name, v)\n",
    "    \n",
    "#    print(outpuddtsfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ef1fc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint AttrData and Attributions\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step -1\n",
      "topi None\n",
      "layer 28\n",
      "head None\n",
      "H 64\n",
      "label_type labels\n",
      "attn_pattern None\n",
      "attribute_k False\n",
      "attr attr\n",
      "ap_scores [['bos->ans0', torch.Size([28, 16])], ['bos->ans]', torch.Size([28, 16])], ['bos->query', torch.Size([28, 16])], ['bos->ans0+', torch.Size([28, 16])], ['bos->tgt', torch.Size([28, 16])], ['bos->sep', torch.Size([28, 16])], ['bos->sep+', torch.Size([28, 16])], ['bos->query-', torch.Size([28, 16])], ['bos->bos', torch.Size([28, 16])]]\n",
      "head_label_types {(16, 7, 'attn_labels:bos->~<s>,3'), (13, 7, 'attn_labels:bos->~<s>,3'), (17, 9, 'attn_labels:bos->~<s>,3'), (18, 13, 'attn_labels:bos->~<s>,3'), (19, 4, 'attn_labels:bos->~<s>,3'), (16, 0, 'attn_labels:bos->~<s>,3')}\n",
      "attr_ap_scores [['bos->ans0', {(18, 13): tensor(0.9330), (16, 7): tensor(0.9523), (13, 7): tensor(0.9237), (16, 0): tensor(0.9425), (23, 14): tensor(0.9308), (19, 4): tensor(0.8385), (18, 9): tensor(0.0857), (17, 9): tensor(0.9146), (21, 13): tensor(0.9429), (20, 1): tensor(0.4251)}], ['bos->ans]', {(18, 13): tensor(-0.0083), (16, 7): tensor(-0.0262), (13, 7): tensor(-0.0272), (16, 0): tensor(-0.0105), (23, 14): tensor(-0.0186), (19, 4): tensor(-0.0137), (18, 9): tensor(0.0033), (17, 9): tensor(-0.0090), (21, 13): tensor(-0.0177), (20, 1): tensor(-0.0051)}], ['bos->query', {(18, 13): tensor(0.0001), (16, 7): tensor(-6.4889e-05), (13, 7): tensor(0.0053), (16, 0): tensor(0.0005), (23, 14): tensor(0.0004), (19, 4): tensor(0.0012), (18, 9): tensor(0.0496), (17, 9): tensor(0.0013), (21, 13): tensor(0.0001), (20, 1): tensor(0.0001)}], ['bos->ans0+', {(18, 13): tensor(2.8242e-05), (16, 7): tensor(0.0006), (13, 7): tensor(0.0128), (16, 0): tensor(-0.0001), (23, 14): tensor(2.4805e-05), (19, 4): tensor(0.0002), (18, 9): tensor(0.0037), (17, 9): tensor(3.2163e-05), (21, 13): tensor(6.4406e-05), (20, 1): tensor(0.0046)}], ['bos->tgt', {(18, 13): tensor(-3.3244e-05), (16, 7): tensor(4.4031e-05), (13, 7): tensor(0.0026), (16, 0): tensor(1.2713e-05), (23, 14): tensor(0.0011), (19, 4): tensor(-0.0008), (18, 9): tensor(-0.0025), (17, 9): tensor(-0.0012), (21, 13): tensor(0.0005), (20, 1): tensor(0.0007)}], ['bos->sep', {(18, 13): tensor(2.8242e-05), (16, 7): tensor(0.0006), (13, 7): tensor(0.0128), (16, 0): tensor(-0.0001), (23, 14): tensor(2.4805e-05), (19, 4): tensor(0.0002), (18, 9): tensor(0.0037), (17, 9): tensor(3.2163e-05), (21, 13): tensor(6.4406e-05), (20, 1): tensor(0.0046)}], ['bos->sep+', {(18, 13): tensor(-0.0002), (16, 7): tensor(-0.0005), (13, 7): tensor(0.0020), (16, 0): tensor(0.0007), (23, 14): tensor(-0.0092), (19, 4): tensor(-0.0028), (18, 9): tensor(0.0231), (17, 9): tensor(0.0003), (21, 13): tensor(0.0001), (20, 1): tensor(-0.0002)}], ['bos->query-', {(18, 13): tensor(2.8398e-05), (16, 7): tensor(0.0002), (13, 7): tensor(0.0050), (16, 0): tensor(-6.3704e-05), (23, 14): tensor(-4.0509e-05), (19, 4): tensor(0.0002), (18, 9): tensor(-0.0003), (17, 9): tensor(-0.0002), (21, 13): tensor(-1.2465e-05), (20, 1): tensor(0.0062)}], ['bos->bos', {(18, 13): tensor(-6.1492e-05), (16, 7): tensor(0.0009), (13, 7): tensor(0.0015), (16, 0): tensor(0.0010), (23, 14): tensor(0.0004), (19, 4): tensor(0.0089), (18, 9): tensor(0.5400), (17, 9): tensor(0.0015), (21, 13): tensor(0.0002), (20, 1): tensor(-0.0061)}]]\n",
      "ap_score None\n",
      "attr_ap_score None\n",
      "icl_ap_score None\n",
      "top_score None\n",
      "sensitivity None\n",
      "_attn_pattern None\n",
      "_attr_ap_score None\n",
      "top_heads [(18, 13), (16, 7), (13, 7), (16, 0), (23, 14), (19, 4), (18, 9), (17, 9), (21, 13), (20, 1)]\n",
      "dummy False\n",
      "mixed False\n",
      "--------------Attributions-----------------\n",
      "embed torch.Size([1])\n",
      "attn 0.0\n",
      "head torch.Size([28, 16])\n",
      "neuron 0.0\n",
      "mlp torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print AttrData and Attributions\n",
    "'''\n",
    "\n",
    "attrdata = r.root.data\n",
    "fields = dataclasses.fields(attrdata)\n",
    "for fd in fields:\n",
    "#     print(fd.name, type(getattr(attrdata, fd.name)))\n",
    "    v = getattr(attrdata, fd.name)\n",
    "    if isinstance(v, dict):\n",
    "        print(fd.name, [[_k,(_v.shape if isinstance(_v, torch.Tensor) else _v)] for _k,_v in v.items()])\n",
    "    else:\n",
    "        print(fd.name, v if not isinstance(v, Attributions) else 'attr'  )\n",
    "print('--------------Attributions-----------------')\n",
    "attribution = attrdata.attr \n",
    "fields = dataclasses.fields(attribution)\n",
    "for fd in fields:\n",
    "    v = getattr(attribution, fd.name)\n",
    "    print(fd.name, v if not isinstance(v, torch.Tensor) else v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e587b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ┌@[0,1,2,3,4,5,7,8,9] 18-13,16-7,13-7,16-0,23-14,19-4,17-9,21-13 29,20-1 29 B->A0 79\n",
      " ├@[6] 18-9 unk 0\n",
      " ├@0 18-13 100 B->A0 93/22/96/-3.3 attn/ans0s\n",
      " ├@0 18-13 100 B->A0 93/22/96/-3.3 attn:B->~<s>\n",
      " ├@1 16-7 92 B->A0 95/33/95/-3.5 attn/ans0s\n",
      " ├@1 16-7 92 B->A0 95/33/95/-3.5 attn:B->~<s>\n",
      " ├@2 13-7 59 B->A0 92/48/91/-3.2 attn/ans0s\n",
      " ├@2 13-7 59 B->A0 92/48/91/-3.2 attn:B->~<s>\n",
      " ├@3 16-0 55 B->A0 94/36/67/-3.2 attn/ans0s\n",
      " ├@3 16-0 55 B->A0 94/36/67/-3.2 attn:B->~<s>\n",
      " ├@4 23-14 49 B->A0 93/21/-10/-3.8 attn/ans0s\n",
      " ┤\n",
      " ├@5 19-4 37 B->A0 84/15/77/-4.0 attn/ans0s\n",
      " ├@5 19-4 37 B->A0 84/15/77/-4.0 attn:B->~<s>\n",
      " ├@7 17-9 33 B->A0 91/22/88/-3.3 attn/ans0s\n",
      " ├@7 17-9 33 B->A0 91/22/88/-3.3 attn:B->~<s>\n",
      " ├@40 14-6 7 B->A0/54/52 attn/ans0s\n",
      " ├@40 14-6 7 B->A0/54/52 attn:B->~<s>\n",
      " ├@396 13-2 -2 B->A0/37/7 attn/ans0s\n",
      " ├@396 13-2 -2 B->A0/37/7 attn:B->~<s>\n",
      " ├@470 20-0 -31 B->A0/37/72 attn/ans0s\n",
      " ├@470 20-0 -31 B->A0/37/72 attn:B->~<s>\n",
      " └@6 18-9 34 B->B 54/29/34/-3.7\n"
     ]
    }
   ],
   "source": [
    "print_tree(r.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9ae8689e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-38f9a7988a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'node' is not defined"
     ]
    }
   ],
   "source": [
    "node = node.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9476640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, layer=16, head=7, topi=[0], head_attr_fn=get_head_mlp_attr)#, label_type=f'argmax_attn_labels')  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f07fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0], head_attr_fn=get_head_mlp_attr)#, label_type=f'argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4e419",
   "metadata": {},
   "source": [
    "### fr->en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0101f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, en2fr.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c870a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ef9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(node));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, en2fr.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples, model, tokenizer, node, topi=[0,1,2], k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697be2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2], label_type='argmax_attn_labels')  # head_attr_fn=get_head_mlp_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(result.root.children[1].children[0].children[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68713987",
   "metadata": {},
   "source": [
    "### did->does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83926d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, does2did.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b511c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node.parent.parent.parent, topi=[0,1], head_attr_fn=get_head_mlp_attr, mix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, input_ids, labels, ranges, *args, o in data_tuples:\n",
    "    show_predictions(tokenizer, *args, logits=o.logits, labels=labels, k_shot=k_shot, topk=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.node = result.node.parent.parent.parent\n",
    "result.node = result.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657592c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[1,0,2,7], head_attr_fn=get_head_mlp_attr, label_type='attn_labels')  # head_attr_fn=get_head_mlp_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb091b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples:\n",
    "    plot_attn_attr(data_tuple, model, tokenizer, node, 17, 16, attn_patterns=None, k_shot=0, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node.parent, topi=[0,1,6,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27296b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0, 1, 2], k_shot=k_shot)  # head_attr_fn=get_head_mlp_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0185b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, does2did.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0, 7], head_attr_fn=get_head_mlp_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6460cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(node));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01df267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, does2did.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2,3], label_type='attn_labels')  # head_attr_fn=get_head_mlp_attr, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a102409",
   "metadata": {},
   "source": [
    "### thing->capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb25f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d207a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, capabilities_of_things.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node, topi=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node, topi=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0541f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, capabilities_of_things.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc85adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[2,1,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd405a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(node));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, capabilities_of_things.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, input_ids, labels, ranges, *args, o in data_tuples:\n",
    "    loss, top1_corrects, answer_probs, candidate_probs = show_predictions(\n",
    "        tokenizer, *args, logits=o.logits, labels=labels, loss_reduction='mean',\n",
    "        candidates=None, k_shot=k_shot, topk=3, verbose=True)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854622df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node.parent.parent.parent, topi=[0], k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0, 1, 2, 3], k_shot=k_shot, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d60291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[1,2,0], label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c9c89",
   "metadata": {},
   "source": [
    "### capital->country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb37da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, country2capital.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node, topi=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd05881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28882636",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, country2capital.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, country2capital.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2])#, label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261a178",
   "metadata": {},
   "source": [
    "### person_adjs.opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b3633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebe210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a226df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,])#, label_type='attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8dd04",
   "metadata": {},
   "source": [
    "### thing->type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d50e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")  # old full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe516f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94799e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b745c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2])#, label_type='attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(1234); torch.cuda.empty_cache()\n",
    "model_names = ['EleutherAI/gpt-j-6B/cpu', 'EleutherAI/gpt-neox-20b', #'EleutherAI/gpt-neox-20b/cpu', \n",
    "               'text-curie-001', 'text-davinci-001', 'text-davinci-002'][:1]\n",
    "metrics = dict(losses=defaultdict(list), accuracies=defaultdict(list))\n",
    "\n",
    "def batch_predict(model, tokenizer):\n",
    "    return [predict(model, tokenizer, text, examples, k_shot=k_shot, custom_forward=False, # avoid computing head_inputs\n",
    "                    bos_token=bos_token, eos_token=eos_token, verbose=len(model_names) == 1)[1]\n",
    "            for text, examples in zip(texts, all_examples)]\n",
    "    \n",
    "with Timer('pmapped batch_predict'):\n",
    "    parallel = len(model_names) > 1\n",
    "    pool = Pool(len(model_names)) if parallel else itertools  # with Pool(len(model_names)) as pool:\n",
    "    results = pool.starmap(batch_predict, [models[model_name] for model_name in model_names])\n",
    "    if parallel: pool.close(); pool.join()\n",
    "            \n",
    "# query2acc, query2loss = defaultdict(list), defaultdict(list)\n",
    "for model_name, r in zip(model_names, results):\n",
    "    _, tokenizer = models[model_name]\n",
    "    for i, (loss, top1_corrects, answer_indices, answer_probs, candidate_probs) in enumerate(r):#.get()\n",
    "        acc = top1_corrects[k_shot:] # np.array(top1_corrects[k_shot:]).mean()\n",
    "        metrics['losses'][model_name].append(loss); metrics['accuracies'][model_name].append(acc)\n",
    "        if batch_size == 1: print(model_name, loss, acc)\n",
    "#         queries = [e[1] for e in _examples_list[i]][k_shot:]\n",
    "#         for q, a, l in zip(queries, acc, loss): query2acc[q].append(float(a)); query2loss[q].append(l)\n",
    "# print(sorted([(q, np.array(v).mean()) for q, v in query2acc.items()], key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89581697",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    for model_name in model_names[:]:\n",
    "        print(metric, model_name, np.array(metrics[metric][model_name]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(a, b):\n",
    "    print(a.dtype, a.size(), b.dtype, b.size())\n",
    "    print('allclose:', torch.allclose(a, b), 'equal:', torch.equal(a, b))\n",
    "    print((a == b).float().mean())\n",
    "    print((a - b).float().abs().mean(), a.float().abs().mean(), b.float().abs().mean())\n",
    "#     print((a - b).max(), (a - b).min())\n",
    "#     print(a[a - b == (a - b).max()])\n",
    "#     print(a[a - b == (a - b).min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc4497",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# text, _examples = texts[0], _examples_list[0]\n",
    "torch.cuda.empty_cache()\n",
    "if True: #def predict2(model, tokenizer, text, _examples):\n",
    "    examples, input_ids, tokens, bos_indices, eos_indices, answers, labels = make_data_tuple(\n",
    "        text, tokenizer, k_shot=k_shot, bos_token=bos_token, eos_token=eos_token)\n",
    "    candidates = [[tokenizer.encode(' ' + token)[0] for token in cands[0]] for _, _, cands, _ in _examples]\n",
    "    with torch.no_grad():\n",
    "        with Timer(): o0 = model(input_ids.to(model.device), output_attentions=True, output_hidden_states=True)\n",
    "        with Timer(): o1 = forward0(model, input_ids.to(model.device), labels=labels.to(model.device),\n",
    "                by_head=['head_input0', 'head_output0'], attn_weights=None, output_hidden_states=True)\n",
    "        for o in [o0, o1]:\n",
    "            logits = o.logits\n",
    "            if isinstance(logits, torch.Tensor): logits = logits.to('cpu').float()# softmax on cpu needs float32\n",
    "            loss, top1_corrects, answer_probs, candidate_probs = show_predictions(\n",
    "                examples, tokenizer, logits, bos_indices, eos_indices, answers, labels, loss_reduction='none',\n",
    "                candidates=candidates, k_shot=k_shot, topk=3, verbose=True)\n",
    "            print('\\n')\n",
    "#     return loss, top1_corrects, answer_probs, candidate_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74c135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a661b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    for model_name in model_names[:]:\n",
    "        print(metric, model_name, np.array(metrics[metric][model_name]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaa1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    for model_name in model_names[:]:\n",
    "        print(metric, model_name, np.array(metrics[metric][model_name])[:,:27].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    _ = plt.figure(figsize=(10, 3));\n",
    "    for model_name in model_names[:2]:\n",
    "        plt.plot(np.array(metrics[metric][model_name])[:].mean(0), label=f'{model_name}');\n",
    "    _ = plt.legend();  _ = plt.title(metric); _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60515185",
   "metadata": {},
   "outputs": [],
   "source": [
    "time2prep = {tuple(clock_of_day): 'at', tuple(days_of_week): 'on', tuple(months): 'in'}\n",
    "def lookup_item2str(item, vocab=None):\n",
    "    if vocab[0] in [clock_of_day, days_of_week, months]:\n",
    "        prep = time2prep[tuple(vocab[0])]\n",
    "        return f'{item[1]} came {prep} {item[0]}'\n",
    "    elif vocab[0] == digits:\n",
    "        return f'{item[1]} is {item[0]}'\n",
    "def lookup_query2str(query, vocab=None, rel_name=None):\n",
    "    if vocab[0] in [clock_of_day, days_of_week, months]:\n",
    "        prep = time2prep[tuple(vocab[0])]\n",
    "        prep = {'prev': 'just before', 'next': 'just after', 'same': prep}[rel_name]\n",
    "        return f'Who came {prep} {query}?'\n",
    "    elif vocab[0] == digits:\n",
    "        prep = {'prev': 'a year younger than', 'next': 'a year younger than', 'same': ''}[rel_name]\n",
    "        return f'Who is {prep} {query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Aaren is a boy. Harlow is a girl.\n",
    "Harlow called Aaren.\n",
    "Harlow: \"Are you a girl?\"\n",
    "Aaren: \"'''\n",
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "model, tokenizer = models[model_name]\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "logits = model(input_ids.to(getattr(model, 'device', 'cpu'))).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topk(*logits[0][-1].softmax(-1).topk(5), indices_fn=tokenizer.convert_ids_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob_dist(logits.top_logprobs[-1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5780be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The capital of Canada is'\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids\n",
    "list(zip(tokenizer.convert_ids_to_tokens(input_ids[0]), input_ids[0].numpy()))\n",
    "outputs = model.generate(input_ids, max_length=10)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = 5; k_shot = nrows // 2 + 1\n",
    "# for pairs in [drop_first_and_last, ]:\n",
    "nrows = 6;  k_shot = 3\n",
    "for pairs in reversible_transformations + irreversible_transformations:\n",
    "    seps = [' -> ', '->'] if random.random() < 0.5 else ['->', ' -> ']\n",
    "    # seps = [' -> ', ' -> ']\n",
    "    samples = ['\\n' + '\\n'.join(a + seps[0] + b for a, b in sample(pairs, nrows)) + '\\n']\n",
    "    for s in samples: data_tuples.append(list(make_data_tuple(s, tokenizer, k_shot=k_shot, bos_token=tokenizer.tokenize(seps[0])[0])))\n",
    "    samples = ['\\n' + '\\n'.join(b + seps[1] + a for a, b in sample(pairs, nrows)) + '\\n' if pairs in reversible_transformations else \n",
    "                '\\n' + '\\n'.join(a + seps[1] + b for a, b in sample(pairs, nrows)) + '\\n']\n",
    "    for s in samples: data_tuples.append(list(make_data_tuple(s, tokenizer, k_shot=k_shot, bos_token=tokenizer.tokenize(seps[1])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sys.path.insert(0, '/nas/xd/projects/ec')\n",
    "# from child_utils import loadPBETasks, retrieveJSONTasks\n",
    "# challenge, challengeCheating = loadPBETasks('/nas/xd/projects/ec/PBE_Strings_Track')\n",
    "# challenge2, challengeCheating2 = loadPBETasks('/nas/xd/projects/ec/data/sygus')\n",
    "# tasks = retrieveJSONTasks(\"/nas/xd/projects/ec/data/list_tasks.json\")\n",
    "# tasks2 = retrieveJSONTasks(\"/nas/xd/projects/ec/data/list_tasks2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxy_utils import get_examples_behind, get_examples_before, get_examples_query_before, \\\n",
    "    get_examples_query_behid, get_examples_query_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432fcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversible_transformations = [list(digit2cardinal.items()), noun2adj, lxy, verb_form, country2capital, en2fr, antonyms]\n",
    "irreversible_transformations = [capabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cbab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc67151",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for model_name, (model, tokenizer) in models.items():\n",
    "    if any(model_name.startswith(s) for s in ['gpt2-', 'KoboldAI/fairseq-dense', 'text-davinci-001', ]): continue\n",
    "    if not model_name == 'EleutherAI/gpt-j-6B': continue\n",
    "    if not isinstance(model, types.FunctionType): _ = model.eval()\n",
    "    with Timer(model_name): outputs = model(**inputs)\n",
    "    options_ids_list = [[tokenizer.encode(' ' + option)[0] for option in options] for cxt, query, options, ans in _examples]\n",
    "    mask_logits_fn = partial(mask_logits, indices=bos_indices, kept_ids=options_ids_list)\n",
    "    loss, all_top1_correct = show_predictions(text, examples, tokenizer, outputs.logits, bos_indices, eos_indices, answers, labels,\n",
    "                    mask_logits_fn=None, topk=3, loss_reduction='mean', show_range=range(k_shot, len(examples)), sep='\\t')\n",
    "    print(loss, all_top1_correct, '\\n')\n",
    "    losses.append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "    if model_name == 'EleutherAI/gpt-j-6B': break\n",
    "print(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_functions = [prev(), next()]\n",
    "rel_fns = [prevs, nexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834550",
   "metadata": {},
   "source": [
    "**TODO: read children books for more posets**  \n",
    "**TODO: Prompt gpt3 to elicit the posets it knows**  \n",
    "$x \\to f(x)$ where $f \\in \\{\\text{prev/next in posets of numbers/letters/months/days, antonym, hypernym, hyponym, ...}\\}$  \n",
    "$x \\to f^2(x)$  \n",
    "one poset or mixed posets  \n",
    "$x, f(x).~y \\to Ff^{[-1]}(y)$ one poset or mixed posets  \n",
    "$x, f^k(x).~y \\to Ff^{[-1]}(y)~/Ff^{[-]k}(y)$  \n",
    "$x, f(f(x))~/f(f(x)), x \\to f(x)$ in between, the simplest form of sequence completion  \n",
    "$x, f(x) \\to Gf$ where $Gf \\in \\{<, >\\}$  \n",
    "$x, f(x); y, g(y) \\to Ff \\stackrel{?}{=} g^{[-1]}$ where $\\text{output} \\in \\{\\text{True}, \\text{False}\\}$  \n",
    "sort\n",
    "\n",
    "There is a *natural* monotone map/functor $F$ between posets/sets $A$ and $B$.  Compose the computation (set operations, sorting etc.) between $A$ and $B$ with $F$ to make harder tasks.  \n",
    "$P(A) ,P(B) \\to F(P(A)) \\setminus ~/ \\cap ~/ \\triangle P(B)$. Harder form of set difference/intersection.  \n",
    "$P(A) \\to F(\\text{sorted}(P(A)))$. Harder form of sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17373019",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_valid = 192, 64\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(s.count('Yes') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,\n",
    "    lr_scheduler_type='constant', learning_rate=5e-3, num_train_epochs=4,\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,\n",
    "    no_cuda=True, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
