{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7292808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d03e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid3/xd/.cache/torch/hub\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src')\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid3/xd/.cache/torch'  # deliberately set this wrong path to avoid migrating cache\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8,7\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, fields, asdict\n",
    "import itertools\n",
    "from itertools import chain, product\n",
    "import math\n",
    "from functools import reduce, partial\n",
    "from collections.abc import Iterable\n",
    "from collections import namedtuple \n",
    "import traceback\n",
    "import pickle, gzip\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# from torch.multiprocessing import Pool\n",
    "# torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer#, pipeline\n",
    "# from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "# from transformers import T5Tokenizer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "# from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c8f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _ as __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cba5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_utils ... done 0:00:00.000046\n",
      "utils ... done 0:00:00.004730\n",
      "child_utils ... Loading tokenizer ... done 0:00:07.196294\n",
      "done 0:00:10.928885\n",
      "tasks ... done 0:00:00.002677\n",
      "model_utils ... done 0:00:07.045234\n",
      "weight_analysis ... done 0:00:00.000032\n"
     ]
    }
   ],
   "source": [
    "from common_utils import Timer\n",
    "with Timer('common_utils'): from common_utils import *\n",
    "with Timer('utils'): from utils import *\n",
    "with Timer('child_utils'): from child_utils import *\n",
    "from child_utils import _str, _cxt2str, _item2str, _s, _be\n",
    "from child_frames import *\n",
    "with Timer('tasks'): from tasks import *\n",
    "with Timer('model_utils'): from model_utils import *\n",
    "with Timer('weight_analysis'): from weight_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# cache_dir = '/mnt/nvme1/xd/.cache/torch/transformers/'  # for gpt-neox-20b on elderberry\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -x http://192.168.50.1:1081 -L -O [-C -] https://huggingface.co/google/ul2/resolve/main/pytorch_model.bin  # -C for 断点续传\n",
    "s2s_model_names = ['google/t5-xl-lm-adapt', 'google/t5-xxl-lm-adapt', 'bigscience/T0p', 'bigscience/T0_3B', \n",
    "    'allenai/tk-instruct-3b-pos', 'allenai/tk-instruct-3b-def-pos', 'google/ul2']\n",
    "gpt_model_names = ['EleutherAI/gpt-j-6B/cpu', #'EleutherAI/gpt-j-6B',\n",
    "#                    'EleutherAI/gpt-neox-20b/cpu', #'EleutherAI/gpt-neox-20b'\n",
    "                  ]#, 'gpt2-xl', 'gpt2']\n",
    "#                    'KoboldAI/fairseq-dense-6.7B', 'KoboldAI/fairseq-dense-13B']\n",
    "for model_name in s2s_model_names[:0] + gpt_model_names[:1]:#, 'gpt2-xl', 'EleutherAI/gpt-neo-1.3B', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name in models: continue\n",
    "    with Timer(model_name):\n",
    "        model_cls = AutoModelForCausalLM if any(s in model_name for s in ['gpt', 'fairseq-dense']) else T5ForConditionalGeneration\n",
    "        # _cache_dir = cache_dir.replace('/nas/', '/nas2/') if 'gpt' not in model_name else cache_dir\n",
    "        kwargs = dict(cache_dir=cache_dir, proxies=proxies, low_cpu_mem_usage=True)\n",
    "        if '/cpu' in model_name or 'gpt-j' not in model_name and 'gpt-neox' not in model_name:\n",
    "            model = model_cls.from_pretrained(model_name.replace('/cpu', ''), cache_dir=cache_dir, proxies=proxies)\n",
    "        elif 'gpt-j' in model_name:\n",
    "            device = 6\n",
    "            model = model_cls.from_pretrained(model_name, revision=\"float16\", torch_dtype=torch.float16, **kwargs).to(device)\n",
    "        elif 'gpt-neox' in model_name:\n",
    "            device = 8; device_map = {'gpt_neox': device, 'embed_out': device}\n",
    "            model = model_cls.from_pretrained(model_name, device_map=device_map, load_in_8bit=True, **kwargs)\n",
    "        if hasattr(model.config, 'use_cache'): model.config.use_cache = False  # save GPU mem\n",
    "        # if model_name in ['EleutherAI/gpt-neox-20b']: model = model.half()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name.replace('/cpu', ''), cache_dir=cache_dir)\n",
    "        unify(model)\n",
    "        models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc28755",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = gpt_model_names[0]  # engines[4]\n",
    "model, tokenizer = models[model_name]\n",
    "model_name_gpu = model_name.replace('/cpu', '')\n",
    "model_gpu = models[model_name_gpu][0] if model_name_gpu in models else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b77b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = model.transformer.h\n",
    "for i, b in enumerate(blocks): b.layer = i\n",
    "ln_f = model.transformer.ln_f\n",
    "L, H, embed_dim = len(blocks), blocks[0].attn.num_heads, blocks[0].attn.embed_dim\n",
    "\n",
    "# we = model.transformer.wte.weight.data\n",
    "# wu = model.lm_head.weight.data\n",
    "\n",
    "# es = [we]\n",
    "# for b in blocks[:1]: es.append(es[-1] + mlp_forward(b, es[-1]))\n",
    "# model.es = es\n",
    "# weBTAs = [es[i].T @ es[i] for i in range(2)]\n",
    "# model.weBTAs = weBTAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175ae7f8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloning GPTJBlock0.ln_1 to cuda:5 ... done 0:00:04.295055\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.033668\n",
      "cloning GPTJBlock1.ln_1 to cuda:5 ... done 0:00:00.001258\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027327\n",
      "cloning GPTJBlock2.ln_1 to cuda:5 ... done 0:00:00.000871\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027232\n",
      "cloning GPTJBlock3.ln_1 to cuda:5 ... done 0:00:00.000920\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.027115\n",
      "cloning GPTJBlock4.ln_1 to cuda:5 ... done 0:00:00.000869\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.026932\n",
      "cloning GPTJBlock5.ln_1 to cuda:5 ... done 0:00:00.001192\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.026631\n",
      "cloning GPTJBlock6.ln_1 to cuda:5 ... done 0:00:00.001220\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.026462\n",
      "cloning GPTJBlock7.ln_1 to cuda:5 ... done 0:00:00.000791\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.026036\n",
      "cloning GPTJBlock8.ln_1 to cuda:5 ... done 0:00:00.000841\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.024895\n",
      "cloning GPTJBlock9.ln_1 to cuda:5 ... done 0:00:00.001201\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025181\n",
      "cloning GPTJBlock10.ln_1 to cuda:5 ... done 0:00:00.001016\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025624\n",
      "cloning GPTJBlock11.ln_1 to cuda:5 ... done 0:00:00.001373\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025392\n",
      "cloning GPTJBlock12.ln_1 to cuda:5 ... done 0:00:00.000901\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025493\n",
      "cloning GPTJBlock13.ln_1 to cuda:5 ... done 0:00:00.000820\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025776\n",
      "cloning GPTJBlock14.ln_1 to cuda:5 ... done 0:00:00.000813\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025555\n",
      "cloning GPTJBlock15.ln_1 to cuda:5 ... done 0:00:00.000935\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.072964\n",
      "cloning GPTJBlock16.ln_1 to cuda:5 ... done 0:00:00.001380\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025150\n",
      "cloning GPTJBlock17.ln_1 to cuda:5 ... done 0:00:00.001075\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025177\n",
      "cloning GPTJBlock18.ln_1 to cuda:5 ... done 0:00:00.001043\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025065\n",
      "cloning GPTJBlock19.ln_1 to cuda:5 ... done 0:00:00.000885\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025354\n",
      "cloning GPTJBlock20.ln_1 to cuda:5 ... done 0:00:00.000785\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025927\n",
      "cloning GPTJBlock21.ln_1 to cuda:5 ... done 0:00:00.000662\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.026802\n",
      "cloning GPTJBlock22.ln_1 to cuda:5 ... done 0:00:00.000784\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025759\n",
      "cloning GPTJBlock23.ln_1 to cuda:5 ... done 0:00:00.000962\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025855\n",
      "cloning GPTJBlock24.ln_1 to cuda:5 ... done 0:00:00.000970\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025736\n",
      "cloning GPTJBlock25.ln_1 to cuda:5 ... done 0:00:00.000932\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.025603\n",
      "cloning GPTJBlock26.ln_1 to cuda:5 ... done 0:00:00.000861\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.033622\n",
      "cloning GPTJBlock27.ln_1 to cuda:5 ... done 0:00:00.000893\n",
      "cloning GPTJAttention.out_proj to cuda:5 ... done 0:00:00.032423\n",
      "cloning GPTJModel.ln_f to cuda:5 ... done 0:00:00.001531\n",
      "cloning GPTJForCausalLM.lm_head to cuda:5 ... done 0:00:00.405423\n",
      "mem_usage before / after clone_model_to: [0, 0, 654] / [1290, 1292, 1946]\n"
     ]
    }
   ],
   "source": [
    "# clone model \n",
    "device = torch.device('cuda:5')\n",
    "_ = clone_model_to(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706055f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "#     (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(genders_of_persons).use(['equal', 'child', 'sibling'])], MlM_gen, None, None,\n",
    "#     ), \n",
    "#      (lambda: [TreeSet(types_of_things).use(['equal', 'child', 'sibling']), TreeSet(types_of_things).use(['equal', 'child', 'sibling'])], MlM_gen, None, None,\n",
    "#      ), \n",
    "#     (lambda: [TreeSet(countries_of_cities).use(['equal', 'child', 'sibling']), TreeSet(countries_of_cities).use(['equal', 'child', 'sibling'])], MlM_gen, None, None,\n",
    "#     ),\n",
    "#     (lambda: fork_vocab(TreeSet(capabilities_of_things), [['equal', 'child', 'sibling']] * 2), MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: [SymSet(person_adjs).use(['equal']), SymSet(person_adjs).use(['opposite'])], MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: fork_vocab(PoSet(temporal_posets), [['equal'], ['prev']]), MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: fork_vocab(PoSet(temporal_posets), [['equal'], ['next']]), MlM_gen, None, None,\n",
    "#     ), \n",
    "#     (lambda: [TreeSet(types_of_characters).use('child'), TreeSet(types_of_things).use('child')], partial(MlM_gen, cxt_sample_fn=enumerate_sample, query=1),\n",
    "#      partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {i[1]}\", f\"{the_(i[1])} is {i[0]}'s\"]), lambda q, _: f\"{q}\", \" likes\"\n",
    "#     ),\n",
    "   (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(types_of_things).use(['equal', 'child', 'sibling'])], MlM_gen,\n",
    "    partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {a_(i[1])}\", f\"{_be(the_(i[1]))} {i[0]}'s\"]), lambda q, _: f\"{the_(q)} likes\",\n",
    "#      partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {a_(i[1])}\", f\"{_be(the_(i[1]))} {i[0]}'s\"]), lambda q, _: f\"What {the_(q)} has is a kind of\",\n",
    "   ), # t: 21-5, 15-8, 19. p: 16-7, 18-5, [3-12, 13-7]. p+: 16-7, 16-0. 13-7:induction head qk, thing->type ov\n",
    "#    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(countries_of_cities).use(['child', 'equal', 'sibling'])], MlM_gen,\n",
    "#     partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} likes {i[1]}\", f\"{i[1]} attracts {the_(i[0], uppercase=False)}\"]), lambda q, _: f'{the_(q)} wanna go to',\n",
    "#    ), # t: 19-12 >> 16-10 = 12-7\n",
    "#    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), TreeSet(capabilities_of_things).use(['child'])], MlM_gen,\n",
    "#     partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} has {a_(i[1])}\", f\"{_be(the_(i[1]))} {i[0]}'s\"]), lambda q, _: f'{q} can'\n",
    "#    ),\n",
    "     (lambda: [TreeSet(genders_of_persons).use('equal'), SymSet(person_adjs).use('equal')], MlM_gen,\n",
    "      partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} is {i[1]}\", f\"{i[1].capitalize()} is {i[0]}\"]), lambda q, _: f\"Yes, {q} looks\", \" like\"\n",
    "     ),\n",
    "    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), SymSet(person_adjs).use(['opposite'])], MlM_gen,\n",
    "     partial(_cxt2str, item2str=lambda i, _: [f\"{i[0]} is {i[1]}\", f\"{capitalize(i[1])} is {i[0]}\"]), lambda q, _: f\"{the_(q)} is not\",\n",
    "    ), # t: 16-14, somewhat 14-7 # verbose acc: gpj-j > curie-001 > davinci-001 > gpt-neox!? abstract acc: gpt-neox > gpt-j. all poor (inc. davinci-002!)\n",
    "#     (lambda: [TreeSet(genders_of_persons).use('equal'), PoSet(temporal_posets).use('equal')], MlM_gen,\n",
    "#      partial(_cxt2str, item2str=lambda i, _: [f'{i[0]} arrived {wrap_noun2(i[1])}', f'{wrap_noun2(i[1]).capitalize()} arrived {i[0]}']), lambda q, _: f\"So {q}'s arrival time\", ' is'\n",
    "#     ),\n",
    "    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), PoSet(temporal_posets).use(['prev'])], MlM_gen,\n",
    "     partial(_cxt2str, item2str=lambda i, _: [f'{i[0]} arrived {prep_(i[1])}', f'{capitalize(prep_(i[1]))} arrived {i[0]}']), lambda q, _: f'{the_(q)} arrived just before'\n",
    "    ),\n",
    "    (lambda: [TreeSet(genders_of_persons).use(['equal', 'child', 'sibling']), PoSet(temporal_posets).use(['next'])], MlM_gen,\n",
    "     partial(_cxt2str, item2str=lambda i, _: [f'{i[0]} arrived {prep_(i[1])}', f'{capitalize(prep_(i[1]))} arrived {i[0]}']), lambda q, _: f'{the_(q)} arrived just after'\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f62b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(key); print_tree(r.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b113734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:6\n",
      "cloning GPTJBlock0.ln_1 to 6 ... done 0:00:00.000030\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'requires_grad_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-d3819082e518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#model_gpu.lm_head.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#switch_model_to(model, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print('lm_head', model.lm_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/transformers/notebooks/model_utils.py\u001b[0m in \u001b[0;36mclone_model_to\u001b[0;34m(model, device, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mclone_module_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ln_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mclone_module_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out_proj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mclone_module_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ln_f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/transformers/notebooks/model_utils.py\u001b[0m in \u001b[0;36mclone_module_to\u001b[0;34m(module, name, device, dtype, remove_on_redo, switch, gpu_module)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cloning {module.__class__.__name__}{getattr(module, 'layer', '')}.{name} to {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# don't know if useful or not for saving mem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             setattr(module, name_with_device(name, device), deepcopy(getattr(module, name)).to(device, dtype=dtype)\n\u001b[1;32m    222\u001b[0m                 if gpu_module is None else getattr(gpu_module, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'requires_grad_'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "results = {}; key = None\n",
    "#print(device)\n",
    "#model.lm_head = copy.deepcopy(model_gpu.lm_head)\n",
    "#model.lm_head.to('cpu')\n",
    "print(model.lm_head.weight.device)\n",
    "#models.keys()\n",
    "#print(save_results)\n",
    "#model_gpu.lm_head.to(device)\n",
    "print(model_gpu.lm_head.weight.device)\n",
    "#switch_model_to(model, device)\n",
    "#print('lm_head', model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f6acd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}; key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f13e0c0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-->task: (<function <lambda> at 0x7f60ac5d93b0>, <function MlM_gen at 0x7f5f89c433b0>, functools.partial(<function _cxt2str at 0x7f5f89c43710>, item2str=<function <lambda> at 0x7f60ac5d9440>), <function <lambda> at 0x7f60ac5d94d0>) rel0_i:0, rel1_i:0, do_swap_qa:False, do_negate:False, do_rm_query:False, rev_item2str:False, do_g2c:False\n",
      "\n",
      "== MlM_gen[genders_of_persons.TreeSet.equal,types_of_things.TreeSet.equal][cxt_len=3] == \n",
      "David has a laptop. Sharon has a rabbit. Barbara has a plum. Barbara likes plum\n",
      "Donna has spaghetti. Michael has a plum. John has a jersey. John likes jersey\n",
      "Steven has vodka. Anthony has a rabbit. Thomas has a pig. Thomas likes pig\n",
      "John has a goat. Laura has pink. Jeff has a jersey. Laura likes pink\n",
      "attribute_tree ... In attribute_tree: attribute_step , topk=10 ... done 0:00:01.053217\n",
      "In attribute_tree: attribute_step stage2  ... done 0:00:00.202408\n",
      "In _add_node: add @0 16-7 100 B->A0 94/35/97/-3.3 attn/ans0s\n",
      "In _add_node: add @0 16-7 100 B->A0 94/35/97/-3.3 attn:B->~<s>\n",
      "In _add_node: add @1 16-0 95 B->A0 96/38/81/-2.9 attn/ans0s\n",
      "In _add_node: add @1 16-0 95 B->A0 96/38/81/-2.9 attn:B->~<s>\n",
      "In _add_node: add @2 13-7 58 B->A0 92/46/79/-3.1 attn/ans0s\n",
      "In _add_node: add @2 13-7 58 B->A0 92/46/79/-3.1 attn:B->~<s>\n",
      "In _add_node: add @3 20-8 41 B->A0 86/18/49/-4.2 attn/ans0s\n",
      "In _add_node: add @5 17-9 36 B->A0 88/20/80/-3.4 attn/ans0s\n",
      "In _add_node: add @5 17-9 36 B->A0 88/20/80/-3.4 attn:B->~<s>\n",
      "In _add_node: add @4 24-10 37 B->B 40/81/90/-5.3\n",
      "In _add_node: add @33 14-6 7 B->A0/49/66 attn/ans0s\n",
      "In _add_node: add @33 14-6 7 B->A0/49/66 attn:B->~<s>\n",
      "In _add_node: add @475 20-0 -55 B->A0/44/68 attn/ans0s\n",
      "In _add_node: add @475 20-0 -55 B->A0/44/68 attn:B->~<s>\n",
      "In _add_node: add @[0,1,2,3,5,6,7,8,9] 16-7,16-0,13-7,20-8,17-9,23-14 29,18-13 23,19-14 23,21-14 20 B->A0 81\n",
      "In _add_node: add @[4] 24-10 unk 0\n",
      "In _add_node: add 16-7,16-0,13-7 B->A0 attn/ans0s\n",
      "In _add_node: add 16-7,16-0,13-7 B->A0 attn:B->~<s>\n",
      "In attribute_tree: attribute_step 16-7,16-0,13-7 B->A0 attn/ans0s, topk=6 ... done 0:00:00.907340\n",
      "In attribute_tree: attribute_step stage2 16-7,16-0,13-7 B->A0 attn/ans0s ... done 0:00:00.199060\n",
      "In _add_node: add @0 11-12 100 B->Q 84/68/45/-4.1\n",
      "In _add_node: add @1 9-5 69 B->Q 94/82/-1/-5.4\n",
      "In _add_node: add @[0,1,2,5] 11-12,9-5,12-14 30,7-14 15 B->Q 55\n",
      "In _add_node: add @[3] 13-7 26 B->A0 17\n",
      "In _add_node: add @[4] 10-7 22 unk 0\n",
      "In attribute_tree: attribute_step 16-7,16-0,13-7 B->A0 attn:B->~<s>, topk=6 ... done 0:00:00.584978\n",
      "In attribute_tree: attribute_step stage2 16-7,16-0,13-7 B->A0 attn:B->~<s> ... done 0:00:00.167378\n",
      "In _add_node: add @1 9-14 97 B->A] 90/70\n",
      "In _add_node: add @5 8-1 67 B->A] 97/95\n",
      "In _add_node: add @2 11-12 85 B->Q 86/68/45/-4.8\n",
      "In _add_node: add @3 11-4 79 B->Q- 25/20/30/-3.6\n",
      "In _add_node: add @4 10-11 77 B->Q- 52/30/51/-4.1\n",
      "In _add_node: add @4 10-11 77 B->Q- 52/30/51/-4.1 attn:B->~<s>\n",
      "In _add_node: add @4 10-11 77 B->Q- 52/30/51/-4.1 attn/example\n",
      "In _add_node: add @0 6-2 100 B->B 1/1/-314/-9.3\n",
      "In _add_node: add @[1,5] 9-14,8-1 B->A] 31\n",
      "In _add_node: add @[2] 11-12 B->Q 20\n",
      "In _add_node: add @[3,4] 11-4,10-11 B->Q- 13\n",
      "In _add_node: add @[0] 6-2 unk 0\n",
      "done 0:00:04.664610\n"
     ]
    }
   ],
   "source": [
    "nrows, k_shot = 12, 3; cxt_len = 3; save_results = True\n",
    "batch_size = 8; verbose = False #not save_results or batch_size <= 8\n",
    "rel1_kwargs = {'x_f': None}  # {'x_f': _s, 'y_f': a_, 'skip_inv_f':False}\n",
    "for task,        rel0_i, rel1_i, do_swap_qa, do_negate, do_rm_query, rev_item2str, do_g2c in product(\n",
    "    #tasks[0:1], [0],[1,],[False],  [True],[True,],[False,],[False,]):\n",
    "    tasks[0:1], [0],[0,],[False],  [False],[False,],[False,],[False,]):\n",
    "    print('\\n-->task:',task, f'rel0_i:{rel0_i}, rel1_i:{rel1_i}, do_swap_qa:{do_swap_qa}, do_negate:{do_negate}, do_rm_query:{do_rm_query}, rev_item2str:{rev_item2str}, do_g2c:{do_g2c}')\n",
    "#     tasks[0:1],range(1,2),range(1),[True],[False,True],[True],[False,],[False,],[False,]):\n",
    "    seed(42)\n",
    "    args = dict(cxt_len=cxt_len, rev_item2str=rev_item2str, abstract=False)\n",
    "    trans_args = dict(rel0_i=rel0_i, rel1_i=rel1_i, rel1_kwargs=rel1_kwargs, do_swap_qa=do_swap_qa, do_negate=do_negate,\n",
    "                      do_rm_query=do_rm_query, do_g2c=do_g2c)\n",
    "    task = transform_task(task, **trans_args)\n",
    "    if task is None: print('task is None! skip.'); continue\n",
    "    res_key = f'{task2str(task)}[{args2str(args)}]'# + composed_heads2str(model)\n",
    "    if key is not None and res_key != key: continue\n",
    "    if not validate_args(task, args, trans_args): print(f'invalid args {res_key}! skip.'); continue\n",
    "    print(f'\\n== {res_key} == {args2str(trans_args)}')\n",
    "    r = results[res_key] if save_results and res_key in results else None\n",
    "#     if r is not None: print('duplicate task!'); continue \n",
    "    r = generate_and_predict_batch(model if save_results else model_gpu, tokenizer, task, nrows, k_shot, batch_size,\n",
    "            custom_forward=save_results, result=r, verbose=verbose, **args)\n",
    "    if save_results: results[res_key] = r\n",
    "#     if True or not save_results or getattr(r, 'mean_acc', 0) < 0.45: continue\n",
    "    #switch_model_to(model_gpu, device)\n",
    "    #print(model.lm_head)\n",
    "    if True or r.root is None: r.root = add_node(None, layer=L, label_type='labels')\n",
    "    r.root = attribute_tree_on(r.data_tuples, model, r.root, 0, topk=10, k_shot=k_shot, mix=True, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29a1f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ┌@[0,1,2,3,5,6,7,8,9] 16-7,16-0,13-7,20-8,17-9,23-14 29,18-13 23,19-14 23,21-14 20 B->A0 81\n",
      " ├@[4] 24-10 unk 0\n",
      " ├@0 16-7 100 B->A0 94/35/97/-3.3 attn/ans0s\n",
      " ├@0 16-7 100 B->A0 94/35/97/-3.3 attn:B->~<s>\n",
      " ├@1 16-0 95 B->A0 96/38/81/-2.9 attn/ans0s\n",
      " ├@1 16-0 95 B->A0 96/38/81/-2.9 attn:B->~<s>\n",
      " ├@2 13-7 58 B->A0 92/46/79/-3.1 attn/ans0s\n",
      " ├@2 13-7 58 B->A0 92/46/79/-3.1 attn:B->~<s>\n",
      " ├@3 20-8 41 B->A0 86/18/49/-4.2 attn/ans0s\n",
      " ├@5 17-9 36 B->A0 88/20/80/-3.4 attn/ans0s\n",
      " ├@5 17-9 36 B->A0 88/20/80/-3.4 attn:B->~<s>\n",
      " ├@33 14-6 7 B->A0/49/66 attn/ans0s\n",
      " ├@33 14-6 7 B->A0/49/66 attn:B->~<s>\n",
      " ├@475 20-0 -55 B->A0/44/68 attn/ans0s\n",
      " ├@475 20-0 -55 B->A0/44/68 attn:B->~<s>\n",
      " ├@4 24-10 37 B->B 40/81/90/-5.3\n",
      " ┤\n",
      " │                               ┌@[0,1,2,5] 11-12,9-5,12-14 30,7-14 15 B->Q 55\n",
      " │                               ├@[3] 13-7 26 B->A0 17\n",
      " ├16-7,16-0,13-7 B->A0 attn/ans0s┤\n",
      " │                               ├@[4] 10-7 22 unk 0\n",
      " │                               ├@0 11-12 100 B->Q 84/68/45/-4.1\n",
      " │                               └@1 9-5 69 B->Q 94/82/-1/-5.4\n",
      " │                                 ┌@[1,5] 9-14,8-1 B->A] 31\n",
      " │                                 ├@[2] 11-12 B->Q 20\n",
      " │                                 ├@[3,4] 11-4,10-11 B->Q- 13\n",
      " │                                 ├@[0] 6-2 unk 0\n",
      " │                                 ├@1 9-14 97 B->A] 90/70\n",
      " │                                 ├@5 8-1 67 B->A] 97/95\n",
      " └16-7,16-0,13-7 B->A0 attn:B->~<s>┤\n",
      "                                   ├@2 11-12 85 B->Q 86/68/45/-4.8\n",
      "                                   ├@3 11-4 79 B->Q- 25/20/30/-3.6\n",
      "                                   ├@4 10-11 77 B->Q- 52/30/51/-4.1\n",
      "                                   ├@4 10-11 77 B->Q- 52/30/51/-4.1 attn:B->~<s>\n",
      "                                   ├@4 10-11 77 B->Q- 52/30/51/-4.1 attn/example\n",
      "                                   └@0 6-2 100 B->B 1/1/-314/-9.3\n"
     ]
    }
   ],
   "source": [
    "print_tree(r.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b3cabd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nThere are a laptop, a laptop, gray. Which is different? A kind of color\\nThere are a peach, a pig, a pig. Which is different? A kind of fruit\\nThere are a handgun, a beetle, a beetle. Which is different? A kind of weapon\\nThere are an elephant, a gun, an elephant. Which is different? A kind of weapon\\nThere are a grenade, a grenade, a rabbit. Which is different? A kind of animal\\nThere are a beetle, a beetle, tea. Which is different? A kind of drink\\nThere are a mango, a mango, underwear. Which is different? A kind of clothing\\nThere are an apple, blue, blue. Which is different? A kind of fruit\\nThere are a burger, a burger, a costume. Which is different? A kind of clothing\\nThere are a goat, a goat, a grenade. Which is different? A kind of weapon\\nThere are a goat, a lemon, a lemon. Which is different? A kind of animal\\nThere are a peach, shoes, a peach. Which is different? A kind of clothing\\n', tensor([[  198,  1858,   389,   257, 13224,    11,   257, 13224,    11, 12768,\n",
      "            13,  9022,   318,  1180,    30,   317,  1611,   286,  3124,   198,\n",
      "          1858,   389,   257, 47565,    11,   257, 12967,    11,   257, 12967,\n",
      "            13,  9022,   318,  1180,    30,   317,  1611,   286,  8234,   198,\n",
      "          1858,   389,   257, 17849,    11,   257, 45489,    11,   257, 45489,\n",
      "            13,  9022,   318,  1180,    30,   317,  1611,   286,  4282,   198,\n",
      "          1858,   389,   281, 20950,    11,   257,  2485,    11,   281, 20950,\n",
      "            13,  9022,   318,  1180,    30,   317,  1611,   286,  4282,   198,\n",
      "          1858,   389,   257, 26168,    11,   257, 26168,    11,   257, 22746,\n",
      "            13,  9022,   318,  1180,    30,   317,  1611,   286,  5044,   198,\n",
      "          1858,   389,   257, 45489,    11,   257, 45489,    11,  8887,    13,\n",
      "          9022,   318,  1180,    30,   317,  1611,   286,  4144,   198,  1858,\n",
      "           389,   257, 49364,    11,   257, 49364,    11, 26170,    13,  9022,\n",
      "           318,  1180,    30,   317,  1611,   286,  9528,   198,  1858,   389,\n",
      "           281, 17180,    11,  4171,    11,  4171,    13,  9022,   318,  1180,\n",
      "            30,   317,  1611,   286,  8234,   198,  1858,   389,   257, 26593,\n",
      "            11,   257, 26593,    11,   257, 16569,    13,  9022,   318,  1180,\n",
      "            30,   317,  1611,   286,  9528,   198,  1858,   389,   257, 26917,\n",
      "            11,   257, 26917,    11,   257, 26168,    13,  9022,   318,  1180,\n",
      "            30,   317,  1611,   286,  4282,   198,  1858,   389,   257, 26917,\n",
      "            11,   257, 18873,    11,   257, 18873,    13,  9022,   318,  1180,\n",
      "            30,   317,  1611,   286,  5044,   198,  1858,   389,   257, 47565,\n",
      "            11, 10012,    11,   257, 47565,    13,  9022,   318,  1180,    30,\n",
      "           317,  1611,   286,  9528,   198]]), tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, 4282, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, 5044, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, 4144, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, 9528, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, 8234, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, 9528, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, 4282, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, 5044, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, 9528, -100, -100]]), [Ranges(bos=(17, 18), ans=(18, 19), ans0=(9, 10), query=None, tgt=(9, 10), sep=(10, 11), ans0s=(array([9, 4, 7]), array([10,  5,  8])), example=(1, 19)), Ranges(bos=(37, 38), ans=(38, 39), ans0=(23, 24), query=None, tgt=(23, 24), sep=(30, 31), ans0s=(array([29, 23, 26]), array([30, 24, 27])), example=(20, 39)), Ranges(bos=(57, 58), ans=(58, 59), ans0=(43, 44), query=None, tgt=(43, 44), sep=(50, 51), ans0s=(array([49, 43, 46]), array([50, 44, 47])), example=(40, 59)), Ranges(bos=(77, 78), ans=(78, 79), ans0=(66, 67), query=None, tgt=(66, 67), sep=(70, 71), ans0s=(array([69, 63, 66]), array([70, 64, 67])), example=(60, 79)), Ranges(bos=(97, 98), ans=(98, 99), ans0=(89, 90), query=None, tgt=(89, 90), sep=(90, 91), ans0s=(array([89, 83, 86]), array([90, 84, 87])), example=(80, 99)), Ranges(bos=(116, 117), ans=(117, 118), ans0=(108, 109), query=None, tgt=(108, 109), sep=(109, 110), ans0s=(array([108, 103, 106]), array([109, 104, 107])), example=(100, 118)), Ranges(bos=(135, 136), ans=(136, 137), ans0=(127, 128), query=None, tgt=(127, 128), sep=(128, 129), ans0s=(array([127, 122, 125]), array([128, 123, 126])), example=(119, 137)), Ranges(bos=(153, 154), ans=(154, 155), ans0=(141, 142), query=None, tgt=(141, 142), sep=(146, 147), ans0s=(array([143, 141, 145]), array([144, 142, 146])), example=(138, 155)), Ranges(bos=(173, 174), ans=(174, 175), ans0=(165, 166), query=None, tgt=(165, 166), sep=(166, 167), ans0s=(array([165, 159, 162]), array([166, 160, 163])), example=(156, 175)), Ranges(bos=(193, 194), ans=(194, 195), ans0=(185, 186), query=None, tgt=(185, 186), sep=(186, 187), ans0s=(array([185, 179, 182]), array([186, 180, 183])), example=(176, 195)), Ranges(bos=(213, 214), ans=(214, 215), ans0=(199, 200), query=None, tgt=(199, 200), sep=(206, 207), ans0s=(array([205, 199, 202]), array([206, 200, 203])), example=(196, 215)), Ranges(bos=(232, 233), ans=(233, 234), ans0=(221, 222), query=None, tgt=(221, 222), sep=(225, 226), ans0s=(array([224, 219, 221]), array([225, 220, 222])), example=(216, 234))], ['There are a laptop, a laptop, gray. Which is different? A kind of color', 'There are a peach, a pig, a pig. Which is different? A kind of fruit', 'There are a handgun, a beetle, a beetle. Which is different? A kind of weapon', 'There are an elephant, a gun, an elephant. Which is different? A kind of weapon', 'There are a grenade, a grenade, a rabbit. Which is different? A kind of animal', 'There are a beetle, a beetle, tea. Which is different? A kind of drink', 'There are a mango, a mango, underwear. Which is different? A kind of clothing', 'There are an apple, blue, blue. Which is different? A kind of fruit', 'There are a burger, a burger, a costume. Which is different? A kind of clothing', 'There are a goat, a goat, a grenade. Which is different? A kind of weapon', 'There are a goat, a lemon, a lemon. Which is different? A kind of animal', 'There are a peach, shoes, a peach. Which is different? A kind of clothing'], [17, 37, 57, 77, 97, 116, 135, 153, 173, 193, 213, 232], [19, 39, 59, 79, 99, 118, 137, 155, 175, 195, 215, 234], [array([3124]), array([8234]), array([4282]), array([4282]), array([5044]), array([4144]), array([9528]), array([8234]), array([9528]), array([4282]), array([5044]), array([9528])], [[17075, 17075, 3124], [8234, 5044, 5044], [4282, 12833, 12833], [5044, 4282, 5044], [4282, 4282, 5044], [12833, 12833, 4144], [8234, 8234, 9528], [8234, 3124, 3124], [2057, 2057, 9528], [5044, 5044, 4282], [5044, 8234, 8234], [8234, 9528, 8234]], [2, 0, 0, 1, 2, 2, 2, 0, 2, 2, 0, 1], CausalLMOutputWithPast(loss=None, logits=tensor([[[10.0234, 11.8203, 11.3438,  ..., -5.8125, -5.8320, -5.8203],\n",
      "         [ 7.9688,  6.7656,  3.5039,  ..., -7.7070, -7.8438, -7.7617],\n",
      "         [ 4.2852,  2.6660, -0.0952,  ..., -8.4141, -8.4766, -8.2734],\n",
      "         ...,\n",
      "         [ 5.6562,  6.0469,  2.7012,  ..., -7.4688, -7.4883, -7.6250],\n",
      "         [ 9.5312,  9.7578,  7.2812,  ..., -7.7500, -7.7383, -7.7617],\n",
      "         [ 9.1094, 11.2266, 11.8672,  ..., -6.8984, -6.7969, -6.7188]]]), past_key_values=None, hidden_states=None, attentions=None)] 3\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "#tokenizer\n",
    "print(r.data_tuples[-1], k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b563cd15",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e331e51a4a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_predictions_by_data_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nas/xd/projects/transformers/notebooks/model_utils.py\u001b[0m in \u001b[0;36mshow_predictions_by_data_tuples\u001b[0;34m(model, tokenizer, data_tuples, k_shot, to_layer, verbose)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_predictions_by_data_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_tuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mto_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "show_predictions_by_data_tuples(model, tokenizer, r.data_tuples, k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7977414910208334 0.7430555555555556  # davinci\n",
    "0.6401057616613541 0.7638888888888888  # 001 slowest\n",
    "0.11072069162819444 0.9722222222222222  # 002\n",
    "0.012047627688270834 1.0                # 003 fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(3+9)*16 equal\n",
    "1.369818564504385 0.4861111111111111 # cpu\n",
    "1.1842011790722609 0.5555555555555556  # gpu\n",
    "\n",
    "(7+9)*8 equal\n",
    "0.8679518327116966 0.6388888888888888 # cpu\n",
    "0.7969751581549644 0.6805555555555556 # gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "(7+9)*8 child+skip_inv_f\n",
    "In generate_and_predict_batch: predict ... done 0:01:31.341952  # cpu\n",
    "0.7979045361280441 0.7361111111111112\n",
    "In generate_and_predict_batch: predict ... done 0:02:14.901858  # cpu custom\n",
    "0.7978888042271137 0.7361111111111112\n",
    "In generate_and_predict_batch: predict ... done 0:00:03.989033  # gpu\n",
    "0.626735083758831 0.7777777777777778\n",
    "In generate_and_predict_batch: predict ... done 0:00:26.205185  # gpu  custom\n",
    "0.6506555993109941 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e835a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(genders_of_persons.TreeSet.child(skip_inv_f), types_of_things.TreeSet.child(skip_inv_f)) (cxt_len=3)'\n",
    "result = results[key]; print_tree(result.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36787b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(types_of_things.TreeSet.child, genders_of_persons.TreeSet.child) (cxt_len=3)'\n",
    "result = results[key]; show_predictions_by_result(tokenizer, result, k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c1baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrows, k_shot = 16, 7; cxt_len=3; save_results = True\n",
    "batch_size = 8; verbose = not save_results or batch_size <= 8\n",
    "for task, replace_rel0, replace_rel1, do_swap_qa, do_negate, do_rm_local_hop, do_rm_query, rev_item2str in product(\n",
    "    tasks[2:3], [0,  ], [0,     ],   [False,],[False,],[False,],[False,],[False,]):\n",
    "    seed(42)\n",
    "    args = dict(cxt_len=cxt_len, rev_item2str=rev_item2str, abstract=False)\n",
    "    trans_args = dict(replace_rel0=replace_rel0, replace_rel1=replace_rel1, do_swap_qa=do_swap_qa, do_negate=do_negate,\n",
    "                      do_rm_local_hop=do_rm_local_hop, do_rm_query=do_rm_query)\n",
    "    task = transform_task(task, **trans_args)\n",
    "    if task is None: continue\n",
    "    res_key = f\"{task2str(task)} ({args2str(args)})\" + composed_heads2str(model)\n",
    "#     if not validate_args(task, args, trans_args): print('invalid args! skip.'); continue\n",
    "#     if save_results and key is not None and res_key != key: continue\n",
    "    if verbose: print(f'\\n== {res_key} == {args2str(trans_args)}')\n",
    "    if save_results and res_key in results:\n",
    "        assert results[res_key].trans_args == trans_args, f'{res_key} {args2str(results[res_key].trans_args)} != {args2str(trans_args)}'\n",
    "        result = results[res_key]; data_tuples = result.data_tuples\n",
    "    else:\n",
    "        all_examples, texts, all_bos_tokens = zip(*[generate(task, verbose=False, plot=False, nrows=nrows, **args)\n",
    "                                                for i in range(batch_size)])\n",
    "        result = Result(task, trans_args, args, all_examples, texts)\n",
    "        for text in texts: print('\\n'.join(text.split('\\n')[:3]))\n",
    "\n",
    "        data_tuples, eval_results = zip(*[predict(model, tokenizer, text, examples,\n",
    "            k_shot=k_shot, bos_token=bos_tokens, verbose=verbose)\n",
    "            for text, examples, bos_tokens in zip(texts, all_examples, all_bos_tokens)\n",
    "            if True or any(s in text[24:] for s in ['dangerous'])])\n",
    "        result.data_tuples = data_tuples\n",
    "        loss, acc, *_ = zip(*eval_results)\n",
    "        result.mean_loss, result.mean_acc = np.array(loss).mean(), np.array(join_lists(acc)).mean()\n",
    "        if verbose: print(result.mean_loss, result.mean_acc)\n",
    "        if save_results: results[res_key] = result\n",
    "    if not save_results: continue\n",
    "\n",
    "#     for node_name in ['node']:\n",
    "#         node = getattr(result, node_name, None)\n",
    "#         if node is None: node = result.node = result.root = add_node(node, label_type=node_name.replace('node', 'labels'))\n",
    "#         node.data.attr = mr(attribute_step)(data_tuples[:], model, node)\n",
    "#     node.data.scores = {ap: mr(get_head_matching_scores)(data_tuples, ap, k_shot=k_shot)\n",
    "#         for ap in attn_patterns_by_step.get(node.data.step, [])} if 'g2c' not in res_key else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\") # codex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "_ = clone_model_to(model, device)\n",
    "data_tuples_gpu = data_tuples_to(data_tuples, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba71533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(root)  # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(genders_of_persons.TreeSet.child, types_of_things.TreeSet.equal) (cxt_len=3)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7f37f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:3], model, tokenizer, node, topi=[0,1,2], k_shot=k_shot, mix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f910b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples[:3]:\n",
    "    plot_attn_attr(data_tuple, model, tokenizer, node, 13, 7, attn_patterns=['bos->ans0]'], k_shot=0, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835266e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce14dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); print('\\n'.join(result.texts[-1].split('\\n')[:1]))\n",
    "node = result.node = add_node(node, topi=[0,1,2,3,4])#layer=11, head=12, attn_pattern='bos->query]', label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(genders_of_persons.TreeSet.equal, types_of_things.TreeSet.child) (cxt_len=3)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aebe41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples[:3]:\n",
    "    plot_attn_attr(data_tuple, model, tokenizer, node, 13, 7, attn_patterns=['bos->ans0]'], k_shot=0, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0701ae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:3], model, tokenizer, node, topi=[0,1,2], k_shot=k_shot, mix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = result.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d23ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); print('\\n'.join(result.texts[-1].split('\\n')[:1]))\n",
    "node = result.node = add_node(node, topi=[0,1,2])#, attn_pattern='bos->query]', label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e179ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1,2,3,4,5])#attn_pattern='bos->ans0]', label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcf5b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key = 'MlM_gen(genders_of_persons.TreeSet.equal, types_of_things.TreeSet.equal) (cxt_len=3)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fbf25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples[:]:\n",
    "    plot_attn_attr(data_tuple, model, tokenizer, node, 11, 12, attn_patterns=['bos->query]'], k_shot=0, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db824e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:3], model, tokenizer, node, topi=[0,1], k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f897ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = result.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07867280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); print('\\n'.join(result.texts[-1].split('\\n')[:1]))\n",
    "node = result.node = add_node(node, topi=[0,1,2,3,4,5])#, label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20113f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0])#, attn_pattern='bos->query]', label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for head_chain in product([(8, 7), (6, 2)], [(13, 13), (9, 14), (12, 10)], [(16, 7)]):\n",
    "    print(head_chain, plot_eigv(weightprod(model, list(head_chain), 'e vo vo qk e', weBTA=model.weBTAs[0]), plot=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0b598",
   "metadata": {},
   "source": [
    "### person_adjs.opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cbe1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=3, abstract=0)_4-6/6-10->4-8_1-7/6-2/8-7->7-9'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd48aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:3], model, tokenizer, node, topi=[0,1,2], k_shot=k_shot, mix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = result.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1])#, label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485bccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=3, abstract=0)_4-6qk->4-8_6-2qk->7-9'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,1,2,3], head_attr_fn=get_head_mlp_attr, k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1,2,3]) #label_type='argmax_attn_labels', attn_pattern='bos->query]') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa60022",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=3, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f031a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0], head_attr_fn=get_head_mlp_attr, k_shot=k_shot)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f131da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1], head_attr_fn=get_head_mlp_attr)#label_type='argmax_attn_labels', attn_pattern='bos->query]') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.equal) (cxt_len=3, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ba16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,1,4,5], k_shot=k_shot)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forked_node_k.k_node = q_node; forked_node_k.model = model\n",
    "del forked_node_k.k_node; del forked_node_k.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forked_node.k_node = k_node; forked_node.model = model\n",
    "del forked_node.k_node; del forked_node.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75612c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30875b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1,2,3]) #label_type='attn_labels', attn_pattern='bos->query]', step=0, attribute_k=True) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,1,2], head_attr_fn=get_head_mlp_attr, k_shot=k_shot)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ed26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0], k_shot=k_shot)  # head_attr_fn=get_head_mlp_attr, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e03775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,2], head_attr_fn=get_head_mlp_attr, k_shot=k_shot, mix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86219f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,1,2,3], head_attr_fn=get_head_mlp_attr, k_shot=k_shot, mix=True)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baa7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples[:4]: plot_attn_attr(data_tuple, model, tokenizer, node, 7, 9, attn_patterns=['bos->ans0]'], k_shot=k_shot, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.children[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17929199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1,2], head_attr_fn=get_head_mlp_attr)# label_type='attn_labels')  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079866a0",
   "metadata": {},
   "source": [
    "### thing->type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691139e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,1], head_attr_fn=get_head_mlp_attr, k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,2,4], head_attr_fn=get_head_mlp_attr)#, label_type='argmax_attn_labels')  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ffba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95222719",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples[:4]: plot_attn_attr(data_tuple, model, tokenizer, node, 16, 7, attn_patterns=['bos->ans0]'], k_shot=k_shot, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014caa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0,1,2,3], head_attr_fn=get_head_mlp_attr, k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9476640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, layer=16, head=7, topi=[0], head_attr_fn=get_head_mlp_attr)#, label_type=f'argmax_attn_labels')  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f07fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0], head_attr_fn=get_head_mlp_attr)#, label_type=f'argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4e419",
   "metadata": {},
   "source": [
    "### fr->en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0101f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, en2fr.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c870a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = result.node = add_node(node, topi=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ef9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(node));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, en2fr.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples, model, tokenizer, node, topi=[0,1,2], k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697be2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2], label_type='argmax_attn_labels')  # head_attr_fn=get_head_mlp_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(result.root.children[1].children[0].children[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68713987",
   "metadata": {},
   "source": [
    "### did->does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83926d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, does2did.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b511c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node.parent.parent.parent, topi=[0,1], head_attr_fn=get_head_mlp_attr, mix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, input_ids, labels, ranges, *args, o in data_tuples:\n",
    "    show_predictions(tokenizer, *args, logits=o.logits, labels=labels, k_shot=k_shot, topk=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.node = result.node.parent.parent.parent\n",
    "result.node = result.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657592c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[1,0,2,7], head_attr_fn=get_head_mlp_attr, label_type='attn_labels')  # head_attr_fn=get_head_mlp_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb091b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_tuple in data_tuples:\n",
    "    plot_attn_attr(data_tuple, model, tokenizer, node, 17, 16, attn_patterns=None, k_shot=0, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node.parent, topi=[0,1,6,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27296b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0, 1, 2], k_shot=k_shot)  # head_attr_fn=get_head_mlp_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0185b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, does2did.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0, 7], head_attr_fn=get_head_mlp_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6460cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(node));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01df267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, does2did.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result, topk=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2,3], label_type='attn_labels')  # head_attr_fn=get_head_mlp_attr, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a102409",
   "metadata": {},
   "source": [
    "### thing->capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb25f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d207a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, capabilities_of_things.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node, topi=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node, topi=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0541f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, capabilities_of_things.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc85adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[2,1,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd405a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_head_chains(model, get_head2scores(node));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, capabilities_of_things.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, input_ids, labels, ranges, *args, o in data_tuples:\n",
    "    loss, top1_corrects, answer_probs, candidate_probs = show_predictions(\n",
    "        tokenizer, *args, logits=o.logits, labels=labels, loss_reduction='mean',\n",
    "        candidates=None, k_shot=k_shot, topk=3, verbose=True)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854622df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node.parent.parent.parent, topi=[0], k_shot=k_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_attrs(data_tuples[:4], model, tokenizer, node, topi=[0, 1, 2, 3], k_shot=k_shot, plot_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d60291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[1,2,0], label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c9c89",
   "metadata": {},
   "source": [
    "### capital->country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb37da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, country2capital.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_circuit(model, tokenizer, result.task, node, topi=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd05881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28882636",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, country2capital.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, country2capital.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2])#, label_type='argmax_attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261a178",
   "metadata": {},
   "source": [
    "### person_adjs.opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b3633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebe210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, person_adjs.SymSet.opposite) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a226df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,])#, label_type='attn_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8dd04",
   "metadata": {},
   "source": [
    "### thing->type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d50e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, result in results.items(): print(f\"{key}: {result.mean_loss:.3f}, {result.mean_acc}\")  # old full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.parent) (cxt_len=1, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe516f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94799e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.equal) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b745c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MlM_gen(persons.EqSet.equal, types_of_things.TreeSet.parent) (cxt_len=2, abstract=0)'; result = results[key]\n",
    "node, data_tuples = show_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key); node = add_node_to_result(result, topi=[0,1,2])#, label_type='attn_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(1234); torch.cuda.empty_cache()\n",
    "model_names = ['EleutherAI/gpt-j-6B/cpu', 'EleutherAI/gpt-neox-20b', #'EleutherAI/gpt-neox-20b/cpu', \n",
    "               'text-curie-001', 'text-davinci-001', 'text-davinci-002'][:1]\n",
    "metrics = dict(losses=defaultdict(list), accuracies=defaultdict(list))\n",
    "\n",
    "def batch_predict(model, tokenizer):\n",
    "    return [predict(model, tokenizer, text, examples, k_shot=k_shot, custom_forward=False, # avoid computing head_inputs\n",
    "                    bos_token=bos_token, eos_token=eos_token, verbose=len(model_names) == 1)[1]\n",
    "            for text, examples in zip(texts, all_examples)]\n",
    "    \n",
    "with Timer('pmapped batch_predict'):\n",
    "    parallel = len(model_names) > 1\n",
    "    pool = Pool(len(model_names)) if parallel else itertools  # with Pool(len(model_names)) as pool:\n",
    "    results = pool.starmap(batch_predict, [models[model_name] for model_name in model_names])\n",
    "    if parallel: pool.close(); pool.join()\n",
    "            \n",
    "# query2acc, query2loss = defaultdict(list), defaultdict(list)\n",
    "for model_name, r in zip(model_names, results):\n",
    "    _, tokenizer = models[model_name]\n",
    "    for i, (loss, top1_corrects, answer_indices, answer_probs, candidate_probs) in enumerate(r):#.get()\n",
    "        acc = top1_corrects[k_shot:] # np.array(top1_corrects[k_shot:]).mean()\n",
    "        metrics['losses'][model_name].append(loss); metrics['accuracies'][model_name].append(acc)\n",
    "        if batch_size == 1: print(model_name, loss, acc)\n",
    "#         queries = [e[1] for e in _examples_list[i]][k_shot:]\n",
    "#         for q, a, l in zip(queries, acc, loss): query2acc[q].append(float(a)); query2loss[q].append(l)\n",
    "# print(sorted([(q, np.array(v).mean()) for q, v in query2acc.items()], key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89581697",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    for model_name in model_names[:]:\n",
    "        print(metric, model_name, np.array(metrics[metric][model_name]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(a, b):\n",
    "    print(a.dtype, a.size(), b.dtype, b.size())\n",
    "    print('allclose:', torch.allclose(a, b), 'equal:', torch.equal(a, b))\n",
    "    print((a == b).float().mean())\n",
    "    print((a - b).float().abs().mean(), a.float().abs().mean(), b.float().abs().mean())\n",
    "#     print((a - b).max(), (a - b).min())\n",
    "#     print(a[a - b == (a - b).max()])\n",
    "#     print(a[a - b == (a - b).min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc4497",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# text, _examples = texts[0], _examples_list[0]\n",
    "torch.cuda.empty_cache()\n",
    "if True: #def predict2(model, tokenizer, text, _examples):\n",
    "    examples, input_ids, tokens, bos_indices, eos_indices, answers, labels = make_data_tuple(\n",
    "        text, tokenizer, k_shot=k_shot, bos_token=bos_token, eos_token=eos_token)\n",
    "    candidates = [[tokenizer.encode(' ' + token)[0] for token in cands[0]] for _, _, cands, _ in _examples]\n",
    "    with torch.no_grad():\n",
    "        with Timer(): o0 = model(input_ids.to(model.device), output_attentions=True, output_hidden_states=True)\n",
    "        with Timer(): o1 = forward0(model, input_ids.to(model.device), labels=labels.to(model.device),\n",
    "                by_head=['head_input0', 'head_output0'], attn_weights=None, output_hidden_states=True)\n",
    "        for o in [o0, o1]:\n",
    "            logits = o.logits\n",
    "            if isinstance(logits, torch.Tensor): logits = logits.to('cpu').float()# softmax on cpu needs float32\n",
    "            loss, top1_corrects, answer_probs, candidate_probs = show_predictions(\n",
    "                examples, tokenizer, logits, bos_indices, eos_indices, answers, labels, loss_reduction='none',\n",
    "                candidates=candidates, k_shot=k_shot, topk=3, verbose=True)\n",
    "            print('\\n')\n",
    "#     return loss, top1_corrects, answer_probs, candidate_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74c135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a661b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    for model_name in model_names[:]:\n",
    "        print(metric, model_name, np.array(metrics[metric][model_name]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaa1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    for model_name in model_names[:]:\n",
    "        print(metric, model_name, np.array(metrics[metric][model_name])[:,:27].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracies', 'losses']:\n",
    "    _ = plt.figure(figsize=(10, 3));\n",
    "    for model_name in model_names[:2]:\n",
    "        plt.plot(np.array(metrics[metric][model_name])[:].mean(0), label=f'{model_name}');\n",
    "    _ = plt.legend();  _ = plt.title(metric); _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60515185",
   "metadata": {},
   "outputs": [],
   "source": [
    "time2prep = {tuple(clock_of_day): 'at', tuple(days_of_week): 'on', tuple(months): 'in'}\n",
    "def lookup_item2str(item, vocab=None):\n",
    "    if vocab[0] in [clock_of_day, days_of_week, months]:\n",
    "        prep = time2prep[tuple(vocab[0])]\n",
    "        return f'{item[1]} came {prep} {item[0]}'\n",
    "    elif vocab[0] == digits:\n",
    "        return f'{item[1]} is {item[0]}'\n",
    "def lookup_query2str(query, vocab=None, rel_name=None):\n",
    "    if vocab[0] in [clock_of_day, days_of_week, months]:\n",
    "        prep = time2prep[tuple(vocab[0])]\n",
    "        prep = {'prev': 'just before', 'next': 'just after', 'same': prep}[rel_name]\n",
    "        return f'Who came {prep} {query}?'\n",
    "    elif vocab[0] == digits:\n",
    "        prep = {'prev': 'a year younger than', 'next': 'a year younger than', 'same': ''}[rel_name]\n",
    "        return f'Who is {prep} {query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Aaren is a boy. Harlow is a girl.\n",
    "Harlow called Aaren.\n",
    "Harlow: \"Are you a girl?\"\n",
    "Aaren: \"'''\n",
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "model, tokenizer = models[model_name]\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "logits = model(input_ids.to(getattr(model, 'device', 'cpu'))).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topk(*logits[0][-1].softmax(-1).topk(5), indices_fn=tokenizer.convert_ids_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob_dist(logits.top_logprobs[-1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5780be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The capital of Canada is'\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids\n",
    "list(zip(tokenizer.convert_ids_to_tokens(input_ids[0]), input_ids[0].numpy()))\n",
    "outputs = model.generate(input_ids, max_length=10)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = 5; k_shot = nrows // 2 + 1\n",
    "# for pairs in [drop_first_and_last, ]:\n",
    "nrows = 6;  k_shot = 3\n",
    "for pairs in reversible_transformations + irreversible_transformations:\n",
    "    seps = [' -> ', '->'] if random.random() < 0.5 else ['->', ' -> ']\n",
    "    # seps = [' -> ', ' -> ']\n",
    "    samples = ['\\n' + '\\n'.join(a + seps[0] + b for a, b in sample(pairs, nrows)) + '\\n']\n",
    "    for s in samples: data_tuples.append(list(make_data_tuple(s, tokenizer, k_shot=k_shot, bos_token=tokenizer.tokenize(seps[0])[0])))\n",
    "    samples = ['\\n' + '\\n'.join(b + seps[1] + a for a, b in sample(pairs, nrows)) + '\\n' if pairs in reversible_transformations else \n",
    "                '\\n' + '\\n'.join(a + seps[1] + b for a, b in sample(pairs, nrows)) + '\\n']\n",
    "    for s in samples: data_tuples.append(list(make_data_tuple(s, tokenizer, k_shot=k_shot, bos_token=tokenizer.tokenize(seps[1])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sys.path.insert(0, '/nas/xd/projects/ec')\n",
    "# from child_utils import loadPBETasks, retrieveJSONTasks\n",
    "# challenge, challengeCheating = loadPBETasks('/nas/xd/projects/ec/PBE_Strings_Track')\n",
    "# challenge2, challengeCheating2 = loadPBETasks('/nas/xd/projects/ec/data/sygus')\n",
    "# tasks = retrieveJSONTasks(\"/nas/xd/projects/ec/data/list_tasks.json\")\n",
    "# tasks2 = retrieveJSONTasks(\"/nas/xd/projects/ec/data/list_tasks2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxy_utils import get_examples_behind, get_examples_before, get_examples_query_before, \\\n",
    "    get_examples_query_behid, get_examples_query_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432fcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversible_transformations = [list(digit2cardinal.items()), noun2adj, lxy, verb_form, country2capital, en2fr, antonyms]\n",
    "irreversible_transformations = [capabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cbab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc67151",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for model_name, (model, tokenizer) in models.items():\n",
    "    if any(model_name.startswith(s) for s in ['gpt2-', 'KoboldAI/fairseq-dense', 'text-davinci-001', ]): continue\n",
    "    if not model_name == 'EleutherAI/gpt-j-6B': continue\n",
    "    if not isinstance(model, types.FunctionType): _ = model.eval()\n",
    "    with Timer(model_name): outputs = model(**inputs)\n",
    "    options_ids_list = [[tokenizer.encode(' ' + option)[0] for option in options] for cxt, query, options, ans in _examples]\n",
    "    mask_logits_fn = partial(mask_logits, indices=bos_indices, kept_ids=options_ids_list)\n",
    "    loss, all_top1_correct = show_predictions(text, examples, tokenizer, outputs.logits, bos_indices, eos_indices, answers, labels,\n",
    "                    mask_logits_fn=None, topk=3, loss_reduction='mean', show_range=range(k_shot, len(examples)), sep='\\t')\n",
    "    print(loss, all_top1_correct, '\\n')\n",
    "    losses.append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "    if model_name == 'EleutherAI/gpt-j-6B': break\n",
    "print(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_functions = [prev(), next()]\n",
    "rel_fns = [prevs, nexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834550",
   "metadata": {},
   "source": [
    "**TODO: read children books for more posets**  \n",
    "**TODO: Prompt gpt3 to elicit the posets it knows**  \n",
    "$x \\to f(x)$ where $f \\in \\{\\text{prev/next in posets of numbers/letters/months/days, antonym, hypernym, hyponym, ...}\\}$  \n",
    "$x \\to f^2(x)$  \n",
    "one poset or mixed posets  \n",
    "$x, f(x).~y \\to Ff^{[-1]}(y)$ one poset or mixed posets  \n",
    "$x, f^k(x).~y \\to Ff^{[-1]}(y)~/Ff^{[-]k}(y)$  \n",
    "$x, f(f(x))~/f(f(x)), x \\to f(x)$ in between, the simplest form of sequence completion  \n",
    "$x, f(x) \\to Gf$ where $Gf \\in \\{<, >\\}$  \n",
    "$x, f(x); y, g(y) \\to Ff \\stackrel{?}{=} g^{[-1]}$ where $\\text{output} \\in \\{\\text{True}, \\text{False}\\}$  \n",
    "sort\n",
    "\n",
    "There is a *natural* monotone map/functor $F$ between posets/sets $A$ and $B$.  Compose the computation (set operations, sorting etc.) between $A$ and $B$ with $F$ to make harder tasks.  \n",
    "$P(A) ,P(B) \\to F(P(A)) \\setminus ~/ \\cap ~/ \\triangle P(B)$. Harder form of set difference/intersection.  \n",
    "$P(A) \\to F(\\text{sorted}(P(A)))$. Harder form of sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17373019",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_valid = 192, 64\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(s.count('Yes') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,\n",
    "    lr_scheduler_type='constant', learning_rate=5e-3, num_train_epochs=4,\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,\n",
    "    no_cuda=True, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
