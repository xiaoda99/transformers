{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '/nas/xd/projects/transformers/src/transformers')\n",
    "import os\n",
    "device_mappings = {0: 1, 1: 5, 2: 6, 3: 7, 4: 2, 5: 3, 6: 0, 1: 4}\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_mappings[2])\n",
    "\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import product, chain\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "import types\n",
    "\n",
    "from pattern.en import comparative\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "# from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In file_utils.py: default_cache_path = /home/xd/.cache/huggingface/transformers\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'4.18.0.dev0'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data.dataset import Dataset, IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Union\n",
    "# from transformers import RobertaForMaskedLM\n",
    "\n",
    "\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "from transformers import AutoConfig, PreTrainedModel, RobertaForMaskedLM, RobertaTokenizer\n",
    "# from transformers.modeling_roberta import RobertaForProbing, RobertaDoubleHeadsModel, \\\n",
    "#     RobertaDoubleHeadsModel2, RobertaDoubleHeadsModel3, RobertaForSequenceClassification  # XD\n",
    "# from transformers.trainer import get_mean_pred_prob  # XD\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, GPTJForCausalLM, XGLMForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "from common_utils import *\n",
    "from utils import *\n",
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "model_name = \"gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/vocab.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/merges.txt\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/added_tokens.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/special_tokens_map.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2-large/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/gpt2-large-config.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2-large/resolve/main/pytorch_model.bin\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/234578a5793e64713ba846b4c5e181e043f48b33140622e2c1dd623b665de3f9.4780ef91b17260f8dac8a3c2183aa338b27365326fb706e74db40b03749f8aba\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "_=model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "source": [
    " import random\n",
    " input_ids = random.sample(range(200,20000),25)\n",
    " input_ids = torch.tensor(input_ids).view(1,25)\n",
    " input_ids.shape\n",
    " input_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "metadata": {},
     "execution_count": 311
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 4346,  9512,  6004, 16602,  7755, 15780, 10244, 17558, 17792, 11566,\n",
       "         14119,  8155, 13697, 11078, 15441,  1918, 13074,  1462, 19531,  7603,\n",
       "           547, 11346,  9095, 19983,  5320]])"
      ]
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "source": [
    "# input_ids = inputs.input_ids\n",
    "# input_shape = input_ids.size()\n",
    "# device = input_ids.device\n",
    "# position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=device)\n",
    "# position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "self = model.transformer\n",
    "inputs_embeds = self.wte(input_ids) #位置编码要不要加？\n",
    "# position_embeds = self.wpe(position_ids)\n",
    "# hidden_states = inputs_embeds + position_embeds\n",
    "# hidden_states = self.drop(hidden_states)\n",
    "# hidden_states=hidden_states.to(torch.float32)\n",
    "# inputs_embeds.shape\n",
    "hidden_states =inputs_embeds\n",
    "attn_output, _,attention ,aaa = self.h[0].attn(self.h[0].ln_1(hidden_states),output_attentions=True) #1, 25, 1280]\n",
    "W_O = self.h[0].attn.c_proj.weight.split(64,0)\n",
    "bias= self.h[0].attn.c_proj.bias #最终要不要加bias\n",
    "aaa.shape\n",
    "attention.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 25, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 313
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 25, 25])"
      ]
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "source": [
    "ans= torch.zeros(1,25,1280)\n",
    "ans+=aaa[:,0,:,:].matmul(W_O[0]) #要不要加bias\n",
    "# ans.shape\n",
    "logits = model.lm_head(ans) #1,25,50527\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "source": [
    "input_ids.shape\n",
    "logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "metadata": {},
     "execution_count": 315
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 50257])"
      ]
     },
     "metadata": {},
     "execution_count": 315
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "source": [
    "bias = torch.zeros((25, 50257), dtype=torch.uint8)#设置mask机制\n",
    "\n",
    "for i in range(25):\n",
    "    bias[i:,input_ids[0][i]] = 1 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "source": [
    "causal_mask = bias.bool().unsqueeze(0)\n",
    "# causal_mask.shape\n",
    "# logits.shape\n",
    "logits=torch.where(causal_mask, logits, torch.tensor(-1e9))\n",
    "loggailv=nn.functional.softmax(logits, dim=-1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "source": [
    "attention[0,0,:,:].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "metadata": {},
     "execution_count": 318
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "source": [
    "ssss=torch.max(attention[0,0,:,:],dim=-1)\n",
    "ssss ##自身关注自身算不算copy head ，需要把这种情况去掉吗"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0000, 0.9861, 0.9927, 0.9968, 0.9920, 1.0000, 0.9982, 0.9992, 0.9996,\n",
       "        0.9993, 0.9988, 0.9737, 0.9984, 0.9881, 0.9823, 0.8987, 0.9988, 0.5715,\n",
       "        1.0000, 0.9947, 0.8247, 0.9988, 0.9971, 0.9987, 0.9870],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24]))"
      ]
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "source": [
    "input_ids.shape\n",
    "tmp=input_ids[0,ssss.indices]\n",
    "tmp\n",
    "# loggailv[0,:,input_ids[0,ssss.indices]]\n",
    "# # loggailv.sum(dim=-1).shape\n",
    "\n",
    "loggailv[0,0,tmp[i]]\n",
    "\n",
    "# jieguo= torch.zeros(25,1)\n",
    "# for i in range(25):\n",
    "#     jieguo[i][0]=loggailv[0,i,input_ids[0,ssss.indices][i]]\n",
    "\n",
    "# jieguo\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 304
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for dimension 0 with size 16",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-c48ee79f852e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mssss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loggailv[0,:,input_ids[0,ssss.indices]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # loggailv.sum(dim=-1).shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for dimension 0 with size 16"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}