{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e98a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93143a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a404d80a-5b2f-4657-ab88-47db382a04a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2748,  0.5467,  1.9439,  0.4705,  1.2793, -0.2457,  0.7065, -0.4898,\n",
      "          1.9260,  1.5952],\n",
      "        [ 0.1441,  0.8569, -1.2726,  0.3970,  1.1867,  0.1359,  0.3979,  1.0589,\n",
      "         -1.7756,  0.8207],\n",
      "        [-0.0428,  0.9587,  1.7211, -0.3067, -0.3189,  0.8541,  0.0526,  1.9741,\n",
      "          0.5461,  1.0095],\n",
      "        [ 1.7107,  0.5056, -0.0739, -0.4487, -0.8810,  0.2943,  0.3522,  1.7507,\n",
      "         -1.0195,  0.5647],\n",
      "        [ 0.2748,  1.0458,  1.5435,  0.8252,  1.1416,  1.4460,  1.6930, -0.0650,\n",
      "         -0.4000, -0.0466],\n",
      "        [ 1.8172,  0.0346,  0.1227,  0.5867,  0.2079,  1.4106,  0.4992, -0.1673,\n",
      "          1.0171,  1.6174],\n",
      "        [ 1.2070,  1.1640,  0.6587,  0.8267, -0.2508, -0.1096,  0.4644, -0.0699,\n",
      "          1.7652,  0.8288],\n",
      "        [ 0.7397, -0.4255,  1.8645, -0.0685,  0.1234,  0.4751,  0.2722,  1.3570,\n",
      "          0.4018, -0.2655],\n",
      "        [ 1.5860, -0.0666,  1.1341,  1.0162, -0.0681,  1.9191,  0.2502,  0.5994,\n",
      "          1.7928,  1.7523],\n",
      "        [ 0.7424,  1.0660, -0.3392, -1.1846, -0.2885, -1.0058, -0.0985,  1.4259,\n",
      "          1.4496,  0.3910]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "opt_foo1 = torch.compile(foo)\n",
    "a, b = torch.randn(10, 10), torch.randn(10, 10)\n",
    "print(opt_foo1(a.to(0), b.to(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6158232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm(inputs):\n",
    "    var = np.mean(np.square(inputs), axis=-2, keepdims=True)\n",
    "    return inputs / np.sqrt(var + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e701ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HD = 64\n",
      "hidden: (4, 16, 128) 1.368727324844607\n",
      "q_hidden: (4, 16, 64) 1.3501020927830765\n",
      "qw: (64, 16, 4) 0.00013758410245601785\n",
      "qw1/2: (4, 16, 16, 2) 0.0015094095864997892 0.0014835413300252301\n",
      "qw1: (4, 16, 16, 2) 0.8199719259850679\n"
     ]
    }
   ],
   "source": [
    "# B, T, D, N, I = 4, 16, 4096, 32, 2; S = T\n",
    "B, T, D, N, I = 4, 16, 2048, 16, 2; S = T\n",
    "HD = N * I * 2; print('HD =', HD)\n",
    "x = np.random.randn(B, T, D)\n",
    "dw1 = np.random.randn(D, HD * 2) * np.sqrt(2/(D + HD * 1))  # + HD * 2\n",
    "hidden = x @ dw1; print('hidden:', hidden.shape, hidden.std())\n",
    "\n",
    "q_hidden, k_hidden = np.split(hidden, 2, axis=-1)\n",
    "print('q_hidden:', q_hidden.shape, q_hidden.std())\n",
    "\n",
    "qw = np.random.randn(HD, N, I * 2) * np.sqrt(1 / HD) * 2 / (N + I) * 0.01; print('qw:', qw.shape, qw.std())\n",
    "qw1, qw2 = np.split(np.einsum('BTK,KNI->BTNI', q_hidden, qw), 2, axis=-1); print('qw1/2:', qw1.shape, qw1.std(), qw2.std())\n",
    "qw1 = rmsnorm(qw1); print('qw1:', qw1.shape, qw1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qw1 = np.random.randn(B, T, N, I) * np.sqrt(2 / (N + I))\n",
    "qw2 = np.random.randn(B, T, N, I) * np.sqrt(2 / (N + I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d021d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (4, 16, 16, 16) 0.9980158533354072\n",
      "hidden: (4, 2, 16, 16) 3.265709170132894\n",
      "qout: (4, 16, 16, 16) 0.006964823501883651\n",
      "out: (4, 16, 16, 16) 0.009851364194778629\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.randn(B, N, T, S); print('inputs:', inputs.shape, inputs.std())\n",
    "hidden = np.einsum('BNTS,BTNI->BITS', inputs, qw1); print('hidden:', hidden.shape, hidden.std())\n",
    "qout = np.einsum('BITS,BTNI->BNTS', hidden, qw2); print('qout:', qout.shape, qout.std())\n",
    "\n",
    "kout = np.random.randn(B, N, T, S) * qout.std()\n",
    "out = qout + kout; print('out:', out.shape, out.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450fbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd: (2048, 32) 0.0001560595921996125\n",
      "q/kdd: (4, 16, 16) 0.007099319258200077 0.007054335266385006\n",
      "qdout: (4, 16, 16, 16) 0.006977773284341642\n",
      "kdout: (4, 16, 16, 16) 0.006977773284341642\n",
      "out: (4, 16, 16, 16) 0.009884432699261078\n"
     ]
    }
   ],
   "source": [
    "dd = np.random.randn(D, N * 2) * np.sqrt(2 / (D + N * 1)) * 0.005; print('dd:', dd.shape, dd.std())\n",
    "qdd, kdd = np.split(np.einsum('BTD,DN->BTN', x, dd), 2, axis=-1); print('q/kdd:', qdd.shape, qdd.std(), kdd.std())\n",
    "\n",
    "qdout = np.einsum('BNTS,BTN->BNTS', inputs, qdd); print('qdout:', qdout.shape, qdout.std())\n",
    "\n",
    "kdout = np.random.randn(B, N, T, S) * qdout.std(); print('kdout:', kdout.shape,qdout.std())\n",
    "out = qdout + kdout; print('out:', out.shape, out.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9582414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb18efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def attn(x, wq, wk, wv, wo, sw, dw, K, V, KW, KDD):\n",
    "    B, T, S, N, D, I = 1, 1, 1024, 32, 128, 2; E = D * N\n",
    "    q, k, v = x @ wq, x @ wk, x @ wv\n",
    "    q, k, v = [a.view(B, T, N, D).permute((0, 2, 1, 3)) for a in [q, k, v]]  # BTND->BNTD\n",
    "    K = torch.cat([K, k], dim=2); V = torch.cat([V, v], dim=2)  # BNSD,BN1D->BN(S+1)D\n",
    "    logits = q @ K.transpose(-2, -1)  # BNTS\n",
    "    probs = logits.softmax(-1)\n",
    "    \n",
    "#     w1, w2, dd = (x @ dw).view(B, T, 2, N, -1).split([I, I, 1], dim=-1)  # BTE,E(2N(2*I+1))->BT(2N(2*I+1))->BT2N(2*I+1)->[BT2NI]*2,BT2N\n",
    "#     qw, kw = torch.einsum('BTKNI,BTKMI->BTKNM', w1, w2).unbind(dim=2)  # k=2, BT2NM->[BTNM]*2\n",
    "#     KW = torch.cat([KW, kw], dim=1)  # BSNM,B1NM->B(S+1)NM\n",
    "#     w = sw + qw + KW  # NM,B1NM,BSNM->BSNM\n",
    "#     qdd, kdd = dd.squeeze(-1).unbind(dim=2) # BT2N1->BT2N->[BTN]*2\n",
    "#     KDD = torch.cat([KDD, kdd], dim=1)  # BSN,B1N->B(S+1)N\n",
    "#     d = qdd + KDD  # B1N,BSN->BSN\n",
    "#     probs = torch.einsum('BNTS,BSNM->BMTS', probs, w) + torch.einsum('BNTS,BSN->BNTS', probs, d)\n",
    "    \n",
    "    w1, w2, dd = (x @ dw).view(B, T, 2, N, -1).split([I, I, 1], dim=-1)  # BTE,E(2N(2*I+1))->BT(2N(2*I+1))->BT2N(2*I+1)->[BT2NI]*2,BT2N1\n",
    "    w1 = w1.transpose(-2, -1)  # BT2NI->BT2IN\n",
    "#     w1 = w1 * torch.rsqrt(w1.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "    w1 = F.layer_norm(w1, (N,))\n",
    "    qw, kw = torch.einsum('BTKIN,BTKMI->BKNMT', w1, w2).unbind(dim=1)  # k=2, B2NMT->[BNMT]*2\n",
    "    KW = torch.cat([KW, kw], dim=-1)  # BNMS,BNM1->BNM(S+1)\n",
    "    w = sw.unsqueeze(-1) + qw + KW  # NM1,BNM1,BNMS->BNMS\n",
    "#     qdd, kdd = dd.squeeze(-1).unbind(dim=2) # BT2N1->BT2N->[BTN]*2\n",
    "#     KDD = torch.cat([KDD, kdd], dim=1)  # BSN,B1N->B(S+1)N\n",
    "    qdd, kdd = dd.squeeze(-1).permute((0, 2, 3, 1)).unbind(dim=1) # BT2N1->BT2N->B2NT->[BNT]*2\n",
    "    KDD = torch.cat([KDD, kdd], dim=-1)  # BNS,BN1->BN(S+1)\n",
    "    d = qdd + KDD  # B1N,BSN->BSN\n",
    "    probs = torch.einsum('BNTS,BNMS->BMTS', probs, w) + torch.einsum('BNTS,BNS->BNTS', probs, d)\n",
    "    \n",
    "    o = probs @ V  # BNTS,BNSD->BNTD\n",
    "    o = o.permute((0, 2, 1, 3)).view(B, T, E) @ wo  # BNTD->BTND->BT(ND)\n",
    "    return K, V, KW, KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def attn(x, wq, wk, wv, wo, sw, dw, K, V, KW, KDD):\n",
    "    B, T, S, N, D, I = 1, 1, 1024, 32, 128, 2; E = D * N\n",
    "    q, k, v = x @ wq, x @ wk, x @ wv\n",
    "    q, k, v = [a.view(B, T, N, D).permute((0, 2, 1, 3)) for a in [q, k, v]]  # BTND->BNTD\n",
    "    K = torch.cat([K, k], dim=2); V = torch.cat([V, v], dim=2)  # BNSD,BN1D->BN(S+1)D\n",
    "    logits = q @ K.transpose(-2, -1)  # BNTS\n",
    "    \n",
    "    project_logits = True\n",
    "    shape = (B,T,2,2,N,-1) # if project_logits else (B,T,2,N,-1)\n",
    "    w1, w2, dd = (x @ dw).view(shape).split([I, I, 1], -1) # BTE,E(2{2}N(2*I+1))->BT(2{2}N(2*I+1))->BT2{2}N(2*I+1)->[BT2{2}NI]*2,BT2{2}N\n",
    "    w1 = w1.transpose(-2, -1)  # BT2{2}NI->BT2{2}IN\n",
    "#     w1 = w1 * torch.rsqrt(w1.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "    w1 = F.layer_norm(w1, (N,))\n",
    "    J = 'J' # if project_logits else ''\n",
    "    qw, kw = torch.einsum(f'BTK{J}IN,BTK{J}MI->BK{J}NMT', w1, w2).unbind(dim=1)  # j=k=2, B2{2}NMT->[B{2}NMT]*2\n",
    "    KW = torch.cat([KW, kw], dim=-1)  # B{2}NMS,B{2}NM1->B{2}NM(S+1)\n",
    "    w = sw.unsqueeze(-1) + qw + KW  # {2}NM1,B{2}NM1,B{2}NMS->B{2}NMS\n",
    "    dims = (0, 2, 3, 4, 1) # if project_logits else (0, 2, 3, 1)\n",
    "    qdd, kdd = F.tanh(dd).squeeze(-1).permute(dims).unbind(dim=1) # BT2{2}N1->BT2{2}N->B2{2}NT->[B{2}NT]*2\n",
    "    KDD = torch.cat([KDD, kdd], dim=-1)  # B{2}NS,B{2}N1->B{2}N(S+1)\n",
    "    d = qdd + KDD  # B{2}N1,B{2}NS->B{2}NS\n",
    "    if project_logits:\n",
    "        wl, w = w.unbind(1); dl, d = d.unbind(1)\n",
    "        logits = torch.einsum('BNTS,BNMS->BMTS', logits, wl) + torch.einsum('BNTS,BNS->BNTS', logits, dl)\n",
    "    probs = logits.softmax(-1)\n",
    "    probs = torch.einsum('BNTS,BNMS->BMTS', probs, w) + torch.einsum('BNTS,BNS->BNTS', probs, d)\n",
    "    \n",
    "    o = probs @ V  # BNTS,BNSD->BNTD\n",
    "    o = o.permute((0, 2, 1, 3)).view(B, T, E) @ wo  # BNTD->BTND->BT(ND)\n",
    "    return K, V, KW, KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e54a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, S, N, D, I = 1, 1, 1024, 32, 128, 2; E = D * N\n",
    "# w3 = torch.randn(E, E * 3).to(0)\n",
    "wq = torch.randn(E, E).to(0); wk = torch.randn(E, E).to(0); wv = torch.randn(E, E).to(0); wo = torch.randn(E, E).to(0)  \n",
    "K = torch.randn(B, N, S, D).to(0); V = torch.randn(B, N, S, D).to(0)\n",
    "project_logits = False\n",
    "if project_logits:\n",
    "    sw = torch.randn(2, N, N).to(0)\n",
    "    dw = torch.randn(E, 2 * 2 * N * (2 * I + 1)).to(0)\n",
    "    KW = torch.randn(B, 2, N, N, S).to(0)  \n",
    "    KDD = torch.randn(B, 2, N, S).to(0)\n",
    "else:\n",
    "    sw = torch.randn(N, N).to(0)\n",
    "    dw = torch.randn(E, 2 * N * (2 * I + 1)).to(0)\n",
    "    # KW = torch.randn(B, S, N, N).to(0)\n",
    "    KW = torch.randn(B, N, N, S).to(0)  \n",
    "    # KDD = torch.randn(B, S, N).to(0)\n",
    "    KDD = torch.randn(B, N, S).to(0)\n",
    "\n",
    "n_iters = 50\n",
    "times = []\n",
    "for i in range(n_iters):\n",
    "    x = torch.randn(B, T, E).to(0)\n",
    "    with torch.no_grad(): (K, V, KW, KDD), t = timed(lambda: attn(x, wq, wk, wv, wo, sw, dw, K, V, KW, KDD))\n",
    "    times.append(t)\n",
    "plt.plot(times); # print(times)\n",
    "print(np.mean(np.array(times[5:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e94873",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline 254\n",
    "post 310\n",
    "post+rms 310~380\n",
    "post+F.rms 310\n",
    "pre+post+F.rms 314\n",
    "\n",
    "not jit\n",
    "baseline 333\n",
    "pre+post+F.rms 740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a302c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def model(x, wqs, wks, wvs, wos, sws, dws, qkws, Ks, Vs, KWs, KDDs, W1s, Wgs, W2s, We, Wu):\n",
    "    # B, T, S, N, D, I = 1, 1, 1024, 16, 128, 2; E = D * N; L=28 # 1.4B\n",
    "    B, T, S, N, D, I = 1, 1, 1024, 32, 128, 2; E = D * N; L=32 # 6.7B\n",
    "    x = We[x]\n",
    "    Ks_new, Vs_new, KWs_new, KDDs_new = [], [], [], [] \n",
    "    for lidx in range(L): \n",
    "        wq, wk, wv, wo, sw, dw, qkw, K, V, KW, KDD, W1, Wg, W2 = [w[lidx] for w in \n",
    "            [wqs, wks, wvs, wos, sws, dws, qkws, Ks, Vs, KWs, KDDs, W1s, Wgs, W2s]]\n",
    "        window_size = 256 if lidx % 2 == 0 else None\n",
    "        if window_size is not None:\n",
    "            K, V = K[:, :, -window_size+1:], V[:, :, -window_size+1:]  # BNSD->BNWD\n",
    "            KW = KW[:, -window_size+1:]  # BS2NN->BW2NN\n",
    "        residual = x\n",
    "        x = F.layer_norm(x, (E,)) # pre layernorm\n",
    "        #Attn\n",
    "        q, k, v = x @ wq, x @ wk, x @ wv\n",
    "        q, k, v = [a.view(B, T, N, D).permute((0, 2, 1, 3)) for a in [q, k, v]]  # BTND->BNTD\n",
    "        K = torch.cat([K, k], dim=2); V = torch.cat([V, v], dim=2)  # BNSD,BN1D->BN(S+1)D\n",
    "        logits = q @ K.transpose(-2, -1)  # BNTS\n",
    "        \n",
    "        project_logits = True # lidx % 4 == 1\n",
    "        \n",
    "        dw_hidden, dd = (x @ dw).split([2*2*N*(2*I), 2*2*N*1], -1) # BT(4K), BT4N         # K=2*N*I\n",
    "        dw_hidden = F.gelu(dw_hidden) \n",
    "        dw_hidden = dw_hidden.view(dw_hidden.shape[:2]+(4,-1)) #B T (4 K) -> B T 4 K  # reshape\n",
    "        dw = torch.einsum('B T C K, C K D -> B T C D', dw_hidden, qkw) # BT4K,4K(MI)->BT4(MI)\n",
    "#         shape = (B,T,2,2,N,-1) # if project_logits else (B,T,2,N,-1)\n",
    "#         w1, w2 = dw.view(shape).split(I,-1) # BT22N(2*I) -> 2*[BT22NI]:BT(pre/post)(q/k)NI\n",
    "#         w1 = w1.transpose(-2, -1)  # BT2{2}NI->BT2{2}IN\n",
    "        shape = (B,T,2,2,-1,N)\n",
    "        w1, w2 = dw.view(shape).split(I,-2)\n",
    "        \n",
    "    #     w1 = w1 * torch.rsqrt(w1.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "        w1 = F.layer_norm(w1, (N,))\n",
    "        J = 'J' # if project_logits else ''\n",
    "\n",
    "#         qkw = torch.einsum(f'BTK{J}IN,BTK{J}MI->BTK{J}NM', w1, w2)  # j=k=2, BT2{2}NM\n",
    "#         qkdd = F.tanh(dd).squeeze(-1).view(shape[:-1]) # BT2{2}N1->BT2{2}N\n",
    "        qkw = torch.einsum(f'BTK{J}IN,BTK{J}IM->BTK{J}NM', w1, w2)  # j=k=2, BT2{2}NM\n",
    "        qkdd = F.tanh(dd).squeeze(-1).view(shape[:-2] + (N,)) # BT2{2}N1->BT2{2}N\n",
    "        \n",
    "        qkw = qkw + torch.diag_embed(qkdd) # BT2{2}NN,->BT2{2}NN\n",
    "        qw, kw = qkw.unbind(2)  # B2{2}NNT-> [B{2}NMT]*2\n",
    "        KW = torch.cat([KW, kw], dim=1)  # BS{2}NM,B1{2}NM->B(S+1){2}NM\n",
    "        w = sw  + qw + KW  #\n",
    "        \n",
    "        # qw, kw = torch.einsum(f'BTK{J}IN,BTK{J}MI->BK{J}NMT', w1, w2).unbind(dim=1)  # j=k=2, B2{2}NMT->[B{2}NMT]*2\n",
    "        # KW = torch.cat([KW, kw], dim=-1)  # B{2}NMS,B{2}NM1->B{2}NM(S+1)\n",
    "        # # print('sw qw, kw shape', sw.shape, qw.shape, KW.shape)\n",
    "        # w = sw.unsqueeze(-1) + qw + KW  # {2}NM1,B{2}NM1,B{2}NMS->B{2}NMS\n",
    "        # dims = (0, 2, 3, 4, 1) # if project_logits else (0, 2, 3, 1)\n",
    "        # dd = F.tanh(dd); dd = dd.view(shape[:-1])\n",
    "        # qdd, kdd = dd.squeeze(-1).permute(dims).unbind(dim=1) # BT2{2}N1->BT2{2}N->B2{2}NT->[B{2}NT]*2\n",
    "        # KDD = torch.cat([KDD, kdd], dim=-1)  # B{2}NS,B{2}N1->B{2}N(S+1)\n",
    "        # d = qdd + KDD  # B{2}N1,B{2}NS->B{2}NS\n",
    "        \n",
    "        wl, w = w.unbind(2) # dl, d = d.unbind(1)\n",
    "        if project_logits:\n",
    "            logits = torch.einsum('BNTS,BSNM->BMTS', logits, wl) #+ torch.einsum('BNTS,BNS->BNTS', logits, dl)\n",
    "        probs = logits.softmax(-1)\n",
    "        probs = torch.einsum('BNTS,BSNM->BMTS', probs, w) #+ torch.einsum('BNTS,BNS->BNTS', probs, d)\n",
    "        \n",
    "        o = probs @ V  # BNTS,BNSD->BNTD\n",
    "        o = o.permute((0, 2, 1, 3)).view(B, T, E) @ wo  # BNTD->BTND->BT(ND)\n",
    "        Ks_new.append(K);Vs_new.append(V);KWs_new.append(KW)#; KDDs_new.append(KDD) \n",
    "        x = residual + o\n",
    "        #MLP\n",
    "        parallel_mlp = False\n",
    "        xn = F.layer_norm(residual if parallel_mlp else x, (E,))\n",
    "        mlp_out = ((xn @ W1) * F.silu(xn @ Wg)) @ W2\n",
    "        x = x + mlp_out\n",
    "    x = F.layer_norm(x, (E,))\n",
    "    x = (x @ Wu).softmax(dim=-1)\n",
    "    x = torch.argmax(x, dim=-1) # greedy generation\n",
    "    return x, Ks_new, Vs_new, KWs_new, KDDs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_causal_mask(shape, dtype):\n",
    "    bsz, tgt_len = shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min * 0.5)\n",
    "    mask_cond = torch.arange(mask.size(-1))\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    return mask\n",
    "        \n",
    "def _cross_head_proj(inputs, sw, qw1, qw2, kw1, kw2, qdd, kdd, loop_over_dynamic_hd=False):\n",
    "    out = inputs + torch.einsum('BNTS,NM->BMTS', inputs, sw)\n",
    "    if loop_over_dynamic_hd:\n",
    "        for i in range(qw1.shape[-2]):\n",
    "            qhidden = torch.einsum('BNTS,BTN->BTS', inputs, qw1[..., i, :])\n",
    "            qout = torch.einsum('BTS,BTN->BNTS', qhidden, qw2[..., i, :]); out = out + qout\n",
    "            khidden = torch.einsum('BNTS,BSN->BTS', inputs, kw1[..., i, :])\n",
    "            kout = torch.einsum('BTS,BSN->BNTS', khidden, kw2[..., i, :]); out = out + kout\n",
    "    else:\n",
    "        qhidden = torch.einsum('BNTS,BTIN->BITS', inputs, qw1)\n",
    "        qout = torch.einsum('BITS,BTIN->BNTS', qhidden, qw2); out = out + qout\n",
    "        khidden = torch.einsum('BNTS,BSIN->BITS', inputs, kw1)\n",
    "        kout = torch.einsum('BITS,BSIN->BNTS', khidden, kw2); out = out + kout\n",
    "    qdout = torch.einsum('BNTS,BTN->BNTS', inputs, qdd); out = out + qdout\n",
    "    kdout = torch.einsum('BNTS,BSN->BNTS', inputs, kdd); out = out + kdout\n",
    "    return out\n",
    "    \n",
    "def _atten_context(query, key, value, atten_mask, pre_proj_dw_args, post_proj_dw_args):\n",
    "    logits = query @ key.transpose(-2, -1)\n",
    "    if pre_proj_dw_args is not None: logits = _cross_head_proj(logits, *pre_proj_dw_args)\n",
    "    logits = logits + atten_mask\n",
    "    probs = logits.softmax(-1)\n",
    "    if post_proj_dw_args is not None: probs = _cross_head_proj(probs, *post_proj_dw_args)\n",
    "    o = probs @ value  # BNTS,BNSD->BNTD\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2377f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def model(x, labels, atten_mask, wqs, wks, wvs, wos, sws, dws, qkws, W1s, Wgs, W2s, We, Wu):  # Ks, Vs, KWs, KDDs, \n",
    "    # B, T, S, N, D, I = 1, 1, 1024, 16, 128, 2; E = D * N; L=28 # 1.4B\n",
    "    B, T, S, N, D, I = 1, 2048 // 2, 2048 // 2, 32, 128, 2; E = D * N; L=32 # 6.7B\n",
    "    q_chunk_size = T # 128\n",
    "    x = We[x]\n",
    "    for lidx in range(L): \n",
    "        wq, wk, wv, wo, sw, dw, qkw, W1, Wg, W2 = [w[lidx] for w in   # K, V, KW, KDD, \n",
    "            [wqs, wks, wvs, wos, sws, dws, qkws, W1s, Wgs, W2s]]  # Ks, Vs, KWs, KDDs, \n",
    "        window_size = 256 if lidx % 2 == 0 else S\n",
    "        residual = x\n",
    "        x = F.layer_norm(x, (E,)) # pre layernorm\n",
    "        #Attn\n",
    "        q, k, v = x @ wq, x @ wk, x @ wv\n",
    "        q, k, v = [a.view(B, T, N, D).permute((0, 2, 1, 3)) for a in [q, k, v]]  # BTND->BNTD\n",
    "        \n",
    "        project_logits = True # lidx % 4 == 1\n",
    "        project_probs = True\n",
    "        if project_probs:\n",
    "            dw_hidden, dd = (x @ dw).split([2*2*N*(2*I), 2*2*N*1], -1) # BT(4K), BT4N         # K=2*N*I\n",
    "            dw_hidden = F.gelu(dw_hidden) \n",
    "            dw_hidden = dw_hidden.view(dw_hidden.shape[:2]+(4,-1)) #B T (4 K) -> B T 4 K  # reshape\n",
    "            dw = torch.einsum('B T C K, C K D -> B T C D', dw_hidden, qkw) # BT4K,4K(MI)->BT4(MI)\n",
    "            shape = (B,T,2*2,-1,N)# if project_logits else (B,T,2,N,-1)  # BT(pre/post)(q/k)IN\n",
    "            w1, w2 = dw.view(shape).split(I,-2)\n",
    "            w1 = F.layer_norm(w1, (N,)) # w1 = w1 * torch.rsqrt(w1.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "            J = 'J' # if project_logits else ''\n",
    "    \n",
    "            pre_sw, post_sw = sw.unbind(0)\n",
    "            pre_qw1, pre_kw1, post_qw1, post_kw1 = w1.unbind(2)  # BT(2{*2})IN->[BTIN]*4\n",
    "            pre_qw2, pre_kw2, post_qw2, post_kw2 = w2.unbind(2)\n",
    "            qkdd = F.tanh(dd).squeeze(-1).view(shape[:-2] + (N,)) # BT(2{*2})N1->BT(2{*2})N\n",
    "            pre_qdd, pre_kdd, post_qdd, post_kdd = qkdd.unbind(2)  # BT(2{*2})N->[BTN]*4\n",
    "        \n",
    "        o = torch.zeros(B, N, T, D).to(q.device, dtype=q.dtype)\n",
    "        for i in range(T // q_chunk_size):\n",
    "            start, stop = i * q_chunk_size, (i + 1) * q_chunk_size\n",
    "            kv_start = max(0, stop - q_chunk_size - window_size)\n",
    "            _q = q[:, :, start : stop, :]\n",
    "            _k, _v = k[:, :, kv_start : stop, :], v[:, :, kv_start : stop, :]\n",
    "            _atten_mask = atten_mask[:, :, start : stop, kv_start : stop]\n",
    "            def slice_dw(sw, qw1, qw2, kw1, kw2, qdd, kdd):\n",
    "                return (sw,\n",
    "                        qw1[:, start : stop] if qw1 is not None else None,\n",
    "                        qw2[:, start : stop] if qw2 is not None else None,\n",
    "                        kw1[:, kv_start : stop] if kw1 is not None else None,\n",
    "                        kw2[:, kv_start : stop] if kw2 is not None else None,\n",
    "                        qdd[:, start : stop] if qdd is not None else None,\n",
    "                        kdd[:, kv_start : stop] if kdd is not None else None)\n",
    "            _pre_proj_dw_args = slice_dw(pre_sw, pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd) if project_logits else None\n",
    "            _post_proj_dw_args = slice_dw(post_sw, post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd) if project_probs else None\n",
    "            _o = _atten_context(_q, _k, _v, _atten_mask, _pre_proj_dw_args, _post_proj_dw_args)\n",
    "            o[:,:,start:stop] = _o\n",
    "            \n",
    "        o = o.permute((0, 2, 1, 3)).reshape(B, T, E) @ wo  # BNTD->BTND->BT(ND)\n",
    "        x = residual + o\n",
    "        #MLP\n",
    "        parallel_mlp = False\n",
    "        xn = F.layer_norm(residual if parallel_mlp else x, (E,))\n",
    "        mlp_out = ((xn @ W1) * F.silu(xn @ Wg)) @ W2\n",
    "        x = x + mlp_out\n",
    "    x = F.layer_norm(x, (E,))\n",
    "    logits = x.float() @ Wu\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    print('loss =', loss.item())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 1; dtype = torch.float16\n",
    "B, T, S, N, D, I = 1, 2048 // 2, 2048 // 2, 32, 128, 2; E = D * N; L=32 # 6.7B\n",
    "VS= 50257\n",
    "IE = 11008 # int(8/3 * E)  # XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3 = torch.randn(E, E * 3).to(0)\n",
    "wqs = (torch.randn(L, E, E) * 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "wks = (torch.randn(L, E, E)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "wvs = (torch.randn(L, E, E)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "wos = (torch.randn(L, E, E)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "# Ks = torch.randn(L, B, N, S, D).to(device, dtype)\n",
    "# Vs = torch.randn(L, B, N, S, D).to(device, dtype)\n",
    "\n",
    "sws = (torch.randn(L, 2, N, N) * 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "dws = (torch.randn(L, E, 2 * 2 * N * (2 * I + 1))* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "qkws = (torch.randn(L, 4, 2*N*I, N*I*2)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5 # new\n",
    "# KWs = torch.randn(L, B, S, 2, N, N).to(device, dtype)  # permutated\n",
    "# KDDs = torch.randn(L, B, S, 2, N).to(device, dtype) # permutated\n",
    "\n",
    "W1s = (torch.randn(L, E, IE)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "Wgs = (torch.randn(L, E, IE)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "W2s = (torch.randn(L, IE, E)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "\n",
    "We = (torch.randn(VS, E)* 1e-5).to(device, dtype).requires_grad_() #* 1e-5\n",
    "Wu = (torch.randn(E, VS)* 1e-5).to(device, torch.float32).requires_grad_() #* 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ec185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, labels, mask, wqs, wks, wvs, wos, sws, dws, qkws, W1s, Wgs, W2s, We, Wu):\n",
    "    loss = model(x, labels, mask, wqs, wks, wvs, wos, sws, dws, qkws, W1s, Wgs, W2s, We, Wu)  # Ks, Vs, KWs, KDDs, \n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10\n",
    "times = []\n",
    "mask = make_causal_mask((B, T), dtype=dtype)[None, None, :, :].to(device, dtype=dtype)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    x = torch.randint(VS, (B,T)).to(device)\n",
    "    labels = torch.randint(VS, (B,T)).to(device)\n",
    "    _, t = timed(lambda: train_step(x, labels, mask, wqs, wks, wvs, wos, sws, dws, qkws, W1s, Wgs, W2s, We, Wu))\n",
    "    # print(i, Ks[0].shape)\n",
    "    times.append(t)\n",
    "plt.plot(times); # print(times)\n",
    "print(np.mean(np.array(times[3:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47184190",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1024\n",
    "baseline\n",
    "0.169\n",
    "taling\n",
    "0.652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline\n",
    "nocompile 0.704693830217634\n",
    "compile nochunk 0.2715109645298549\n",
    "\n",
    "talking\n",
    "nocompile 1.5 / 3.560845179966518\n",
    "compile nochunk 0.271770769391741 compile 10x faster\n",
    "compile chunk128 0.2717097691127232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ddeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 50\n",
    "times = []\n",
    "x = torch.randint(VS, (B,T)).to(device)\n",
    "labels = torch.randint(VS, (B,T)).to(device)\n",
    "mask = make_causal_mask(x, dtype=dtype)\n",
    "for i in range(n_iters):\n",
    "    # x = torch.randn(B, T, E).to(0)\n",
    "    with torch.no_grad():\n",
    "        (x, Ks_new, Vs_new, KWs_new, KDDs_new), t = timed(lambda: model(\n",
    "            x, wqs, wks, wvs, wos, sws, dws, qkws, Ks, Vs, KWs, KDDs, W1s, Wgs, W2s,We,Wu))\n",
    "    # print(i, Ks[0].shape)\n",
    "    times.append(t)\n",
    "plt.plot(times); # print(times)\n",
    "print(np.mean(np.array(times[10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp16\n",
    "baseline 0.0133\n",
    "baseline win256 0.0124\n",
    "whole 0.0160\n",
    "1to4 0.0156\n",
    "onlyprobs 0.0156\n",
    "whole win256 0.0161?\n",
    "whole win256+1to4 0.0152\n",
    "\n",
    "# fp32\n",
    "baseline 0.0272\n",
    "baseline win256 0.0262\n",
    "baseline paramlp 0.0270\n",
    "whole 0.0306\n",
    "1to4 0.0303\n",
    "onlyprobs 0.0301\n",
    "whole win256 0.0292\n",
    "whole win256+1to4 0.0289\n",
    "whole paramlp 0.03034"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
