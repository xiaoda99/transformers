{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811399ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432f8b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid/xd/.cache/torch/hub\n",
      "Loading tokenizer ... done 0:00:08.810411\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src')\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, GPTJForCausalLM\n",
    "\n",
    "import unseal\n",
    "import string\n",
    "from itertools import product, chain\n",
    "import math\n",
    "import json\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import torch\n",
    "from unseal import transformers_util as tutil\n",
    "from unseal import hooks\n",
    "import unseal.visuals.utils as utils\n",
    "from unseal.hooks import HookedModel\n",
    "\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *\n",
    "from model_utils import *\n",
    "from weight_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00fe3b",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6101119",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# cache_dir = '/mnt/nvme1/xd/.cache/torch/transformers/'  # for gpt-j-6B on elderberry\n",
    "\n",
    "proxies = {'http': '192.168.50.1:1081'} \n",
    "\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = GPTJForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646dbac6",
   "metadata": {},
   "source": [
    "# 读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f785f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_date(name):\n",
    "    if(name == \"1\"):\n",
    "        sentences_Winograd = _read_Winograd(\"/nas/xd/data/circuits_datasets/winogrande_1.1/winogrande_1.1/train_l.jsonl\")\n",
    "        textsent = sentences_Winograd\n",
    "    elif(name == \"2\"):\n",
    "        sentences_commonsenseqa = _read_commonsenseqa(\"/nas/xd/data/circuits_datasets/commonsenseqa/train_rand_split.jsonl\")\n",
    "        textsent = sentences_commonsenseqa\n",
    "    elif(name == \"3\"):\n",
    "        sentences_CSQA2 = _read_CSQA2(\"/nas/xd/data/circuits_datasets/CSQA2/CSQA2_train.json\")   \n",
    "        textsent = sentences_CSQA2\n",
    "    elif(name == \"4\"):\n",
    "        sentences_anli = _read_anli(\"/nas/xd/data/circuits_datasets/anli_v1.0/anli_v1.0/R1/train.jsonl\")   \n",
    "        textsent = sentences_anli\n",
    "    else:\n",
    "        sentences_RACE = _read_RACE(\"/nas/xd/data/circuits_datasets/RACE/train/middle\")\n",
    "        textsent = sentences_RACE\n",
    "    return textsent\n",
    "\n",
    " \n",
    "# 获取file_path路径下的所有TXT文本内容和文件名\n",
    "def get_text_list(file_path):\n",
    "    files = os.listdir(file_path)\n",
    "    text_list = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(file_path, file), \"r\") as f:\n",
    "            text_list.append(f.read())\n",
    "    return text_list\n",
    "\n",
    "#经典Winograd进阶版，短   （原文+嵌入）\n",
    "def _read_Winograd(file_path: str):\n",
    "    label_dict = {'1':\"option1\", '2':\"option2\"}\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            question = line['sentence']\n",
    "            choices = line['answer']\n",
    "            replace = line[label_dict[choices]]\n",
    "            sentence = question.replace(\"_\",replace)\n",
    "            sentences.append(sentence)\n",
    "    \n",
    "#     random.shuffle(sentences)\n",
    "    return sentences[:5]\n",
    "\n",
    "# CSQA   （问题+答案）\n",
    "def _read_commonsenseqa(file_path: str):\n",
    "    label_dict = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4}\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            question = line['question']['stem']\n",
    "            choices = [c['text'] for c in line['question']['choices']]\n",
    "            label = label_dict[line['answerKey']] if 'answerKey' in line else None\n",
    "            sentence = _connect(question,choices[label])\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "#  CSQA2 常识问答，短     （问题+答案）\n",
    "def _read_CSQA2(file_path: str):\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            question = line['question']\n",
    "            ans = line['answer']\n",
    "            sentence = _connect(question,ans)\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "#NLI/MNLI进阶版，中    （前提+假设）\n",
    "def _read_anli(file_path: str):\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            context = line['context']\n",
    "            hypothesis = line['hypothesis']\n",
    "            sentence = _connect(context,hypothesis)\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "#RACE Dataset  中学英语阅读理解，长    （原文+问题+选项）\n",
    "def _read_RACE(file_path: str):\n",
    "    sentences = []\n",
    "    label_dict = {'A':0, 'B':1, 'C':2, 'D':3}\n",
    "    text_list = get_text_list(file_path)\n",
    "    for i in range(len(text_list)):\n",
    "        text_list[i] = eval(text_list[i])\n",
    "        context = text_list[i]['article']\n",
    "#         label = text_list[i]['id']\n",
    "        for j in range(len(text_list[i]['answers'])):\n",
    "            que = text_list[i]['questions'][j]\n",
    "            ans = text_list[i]['options'][j][label_dict[text_list[i]['answers'][j]]]\n",
    "            sentence = _connect(context,que)\n",
    "            sentence = _connect(sentence,ans)\n",
    "#             sentence = _connect(sentence,label)\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e02937",
   "metadata": {},
   "source": [
    "# 主要函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37893d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, tokenizer, text):\n",
    "    inputs1 = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs1, model.device)\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    return outputs.attentions\n",
    "\n",
    "def _create_attention(outputs, heads, head_dict): #任何一个开头的注意力分数<0.5，加入该head\n",
    "    head = []\n",
    "    for i in heads:\n",
    "        flag = False\n",
    "        for j in range(len(outputs[0][0][0])):\n",
    "            if(outputs[i[1]][0][i[0]][j][0]<0.5):\n",
    "                flag = True\n",
    "                break\n",
    "        if(flag == True):\n",
    "            head.append(i)\n",
    "    head_dict[\"heads\"].append(head)\n",
    "\n",
    "\n",
    "def choose(k,texts): #随机选取几个句子\n",
    "    text_choice = []\n",
    "    for i in range(k):\n",
    "        text = random.choice(texts)\n",
    "        text_choice.append(text)\n",
    "    return text_choice\n",
    "\n",
    "def _concat(attentions, heads):\n",
    "    output = attentions[heads[0][0]][0][heads[0][1]]\n",
    "    for i in heads[1:]:\n",
    "        try:\n",
    "            output= torch.stack([output, attentions[i[0]][0][i[1]]],0);\n",
    "        except:\n",
    "            output= torch.vstack((output,attentions[i[0]][0][i[1]][None]));\n",
    "#     output = einops.rearrange(attentions[:,:,:,:,:], 'l n h n1 n2 -> h n l n1 n2')\n",
    "    val= torch.tensor([item.cpu().detach().numpy() for item in output]).cuda()\n",
    "    return val\n",
    "    \n",
    "def draw(attentions, text, head):\n",
    "    val = _concat(attentions, head)\n",
    "    vall = einops.rearrange(val[:,:,:], 'h n1 n2 -> n1 n2 h ')\n",
    "    attention = vall\n",
    "    html_object = ps.AttentionLogits(tokens=text, attention=attention, pos_logits=attention, neg_logits=attention, head_labels=[f'{i}:{j}' for i,j in head])\n",
    "    html_object = html_object.update_meta(suppress_title=True)\n",
    "    html_str = html_object.html_page_str()\n",
    "    # save html string\n",
    "    html_storage[f'{text}'] = html_str\n",
    "\n",
    "    html_objects = {key: HTML(val) for (key, val) in html_storage.items()}\n",
    "    for i in head_dict[text]:\n",
    "        print(f'\\n')\n",
    "        display(html_objects[f'{i}'], display_id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a59b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(num):\n",
    "    texts = _choose_date(num)\n",
    "    return texts\n",
    "    \n",
    "def find_activations(model, tokenizer, texts, heads):  # long run\n",
    "    head_dict = {\"text\":[],\"heads\":[]}\n",
    "    for text in texts:\n",
    "        head_dict[\"text\"].append(text)\n",
    "        outputs = forward(model, tokenizer, text)#outputs attention矩阵\n",
    "        _create_attention(outputs, heads, head_dict) \n",
    "    return head_dict #[\"text\":[\"xxx\",\"xxxx\"],\"heads\":[[(5,12),(1,1)],[(3,6),(4,8)]]]\n",
    "\n",
    "#pickle dump head_dict #存json\n",
    "def dump_dict(head_dict):\n",
    "    with open('nrk/head_dict.json', 'w',encoding='utf-8') as fp:\n",
    "        json.dump(head_dict, fp)\n",
    "\n",
    "#pickle load head_dict #读json\n",
    "def load_dict():\n",
    "    with open('nrk/head_dict.json', 'r',encoding='utf-8') as fp:\n",
    "        head_dict = json.load(fp)\n",
    "    return head_dict\n",
    "\n",
    "def show_activations(model, tokenizer, head_dict, heads, k): \n",
    "    texts = choose(k,head_dict[\"text\"]) #重写函数，随机找k个文本输入\n",
    "    head_dict = {\"text\":[],\"heads\":[]}\n",
    "    for i in range(k):\n",
    "        head_dict[\"text\"].append(texts[i])\n",
    "        outputs = forward(model,tokenizer, texts[i])#outputs attention矩阵\n",
    "        _create_attention(outputs, heads, head_dict)\n",
    "        draw(outputs, head_dict[\"text\"][i], head_dict[\"heads\"][i]) #绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef710bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysvelte as ps \n",
    "import npm\n",
    "from contextlib import suppress\n",
    "from typing import Optional\n",
    "heads = [(5, 12), (7, 9), (3, 11)]\n",
    "\n",
    "\n",
    "texts = read_texts(\"1\")\n",
    "head_dict = find_activations(model, tokenizer, texts, heads)\n",
    "# dump_dict(head_dict)\n",
    "# head_dict = load_dict()\n",
    "# show_activations(model, tokenizer, head_dict, heads, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68af2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of model_utils failed: Traceback (most recent call last):\n",
      "  File \"/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/nas/xd/projects/transformers/notebooks/model_utils.py\", line 1854\n",
      "    p.step == 0 and p.top_score > 0 and c.attn_pattern in ['bos->query', 'bos->bos'] and\n",
      "                                                                                        ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21350bd4c6aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-3e5f1331ca9a>\u001b[0m in \u001b[0;36mshow_activations\u001b[0;34m(model, tokenizer, head_dict, heads, k)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhead_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#重写函数，随机找k个文本输入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mhead_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"heads\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4874dbd00293>\u001b[0m in \u001b[0;36mchoose\u001b[0;34m(k, texts)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtext_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtext_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext_choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'choice'"
     ]
    }
   ],
   "source": [
    "show_activations(model, tokenizer, head_dict, heads, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
