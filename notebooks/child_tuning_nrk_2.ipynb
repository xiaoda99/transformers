{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b6295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204f8c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid/xd/.cache/torch/hub\n",
      "Loading tokenizer ... done 0:00:08.903279\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src')\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, GPTJForCausalLM\n",
    "\n",
    "import unseal\n",
    "import string\n",
    "from itertools import product, chain\n",
    "import math\n",
    "import json\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import torch\n",
    "from unseal import transformers_util as tutil\n",
    "from unseal import hooks\n",
    "import unseal.visuals.utils as utils\n",
    "from unseal.hooks import HookedModel\n",
    "\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *\n",
    "from model_utils import *\n",
    "from weight_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f8936",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9703b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# cache_dir = '/mnt/nvme1/xd/.cache/torch/transformers/'  # for gpt-j-6B on elderberry\n",
    "\n",
    "proxies = {'http': '192.168.50.1:1081'} \n",
    "\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = GPTJForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbe99e",
   "metadata": {},
   "source": [
    "# 读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2a2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_date(name):\n",
    "    if(name == \"1\"):\n",
    "        sentences_Winograd = _read_Winograd(\"/nas/xd/data/circuits_datasets/winogrande_1.1/winogrande_1.1/train_l.jsonl\")\n",
    "        textsent = sentences_Winograd\n",
    "    elif(name == \"2\"):\n",
    "        sentences_commonsenseqa = _read_commonsenseqa(\"/nas/xd/data/circuits_datasets/commonsenseqa/train_rand_split.jsonl\")\n",
    "        textsent = sentences_commonsenseqa\n",
    "    elif(name == \"3\"):\n",
    "        sentences_CSQA2 = _read_CSQA2(\"/nas/xd/data/circuits_datasets/CSQA2/CSQA2_train.json\")   \n",
    "        textsent = sentences_CSQA2\n",
    "    elif(name == \"4\"):\n",
    "        sentences_anli = _read_anli(\"/nas/xd/data/circuits_datasets/anli_v1.0/anli_v1.0/R1/train.jsonl\")   \n",
    "        textsent = sentences_anli\n",
    "    else:\n",
    "        sentences_RACE = _read_RACE(\"/nas/xd/data/circuits_datasets/RACE/train/middle\")\n",
    "        textsent = sentences_RACE\n",
    "    return textsent\n",
    "\n",
    " \n",
    "# 获取file_path路径下的所有TXT文本内容和文件名\n",
    "def get_text_list(file_path):\n",
    "    files = os.listdir(file_path)\n",
    "    text_list = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(file_path, file), \"r\") as f:\n",
    "            text_list.append(f.read())\n",
    "    return text_list\n",
    "\n",
    "#经典Winograd进阶版，短   （原文+嵌入）\n",
    "def _read_Winograd(file_path: str):\n",
    "    label_dict = {'1':\"option1\", '2':\"option2\"}\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            question = line['sentence']\n",
    "            choices = line['answer']\n",
    "            replace = line[label_dict[choices]]\n",
    "            sentence = question.replace(\"_\",replace)\n",
    "            sentences.append(sentence)\n",
    "    \n",
    "#     random.shuffle(sentences)\n",
    "    return sentences[:5]\n",
    "\n",
    "# CSQA   （问题+答案）\n",
    "def _read_commonsenseqa(file_path: str):\n",
    "    label_dict = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4}\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            question = line['question']['stem']\n",
    "            choices = [c['text'] for c in line['question']['choices']]\n",
    "            label = label_dict[line['answerKey']] if 'answerKey' in line else None\n",
    "            sentence = _connect(question,choices[label])\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "#  CSQA2 常识问答，短     （问题+答案）\n",
    "def _read_CSQA2(file_path: str):\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            question = line['question']\n",
    "            ans = line['answer']\n",
    "            sentence = _connect(question,ans)\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "#NLI/MNLI进阶版，中    （前提+假设）\n",
    "def _read_anli(file_path: str):\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = json.loads(line)\n",
    "            if not line:\n",
    "                continue\n",
    "            context = line['context']\n",
    "            hypothesis = line['hypothesis']\n",
    "            sentence = _connect(context,hypothesis)\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "#RACE Dataset  中学英语阅读理解，长    （原文+问题+选项）\n",
    "def _read_RACE(file_path: str):\n",
    "    sentences = []\n",
    "    label_dict = {'A':0, 'B':1, 'C':2, 'D':3}\n",
    "    text_list = get_text_list(file_path)\n",
    "    for i in range(len(text_list)):\n",
    "        text_list[i] = eval(text_list[i])\n",
    "        context = text_list[i]['article']\n",
    "#         label = text_list[i]['id']\n",
    "        for j in range(len(text_list[i]['answers'])):\n",
    "            que = text_list[i]['questions'][j]\n",
    "            ans = text_list[i]['options'][j][label_dict[text_list[i]['answers'][j]]]\n",
    "            sentence = _connect(context,que)\n",
    "            sentence = _connect(sentence,ans)\n",
    "#             sentence = _connect(sentence,label)\n",
    "            sentences.append(sentence)\n",
    "#     random.shuffle(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9634f9e",
   "metadata": {},
   "source": [
    "# 主要函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc10ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, tokenizer, text):\n",
    "    inputs1 = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs1, model.device)\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    return outputs.attentions\n",
    "\n",
    "def _create_attention(outputs, heads, head_dict): #任何一个开头的注意力分数<0.5，加入该head\n",
    "    head = []\n",
    "    for i in heads:\n",
    "        flag = False\n",
    "        for j in range(len(outputs[0][0][0])):\n",
    "            if(outputs[i[1]][0][i[0]][j][0]<0.5):\n",
    "                flag = True\n",
    "                break\n",
    "        if(flag == True):\n",
    "            head.append(i)\n",
    "    head_dict[\"heads\"].append(head)\n",
    "\n",
    "\n",
    "def choose(k,texts): #随机选取几个句子\n",
    "    text_choice = random.choices(texts,k=k)\n",
    "    return text_choice\n",
    "\n",
    "def _concat(attentions, heads):\n",
    "    output = attentions[heads[0][0]][0][heads[0][1]]\n",
    "    for i in heads[1:]:\n",
    "        try:\n",
    "            output= torch.stack([output, attentions[i[0]][0][i[1]]],0);\n",
    "        except:\n",
    "            output= torch.vstack((output,attentions[i[0]][0][i[1]][None]));\n",
    "#     output = einops.rearrange(attentions[:,:,:,:,:], 'l n h n1 n2 -> h n l n1 n2')\n",
    "    val= torch.tensor([item.cpu().detach().numpy() for item in output]).cuda()\n",
    "    return val\n",
    "    \n",
    "def draw(attentions, text, heads):\n",
    "    val = _concat(attentions, heads)\n",
    "    vall = einops.rearrange(val[:,:,:], 'h n1 n2 -> n1 n2 h ')\n",
    "    html_object = ps.AttentionLogits(tokens=text, attention=vall, pos_logits=vall, neg_logits=vall, head_labels=[f'{i}:{j}' for i,j in heads])\n",
    "\n",
    "    html_object = html_object.update_meta(suppress_title=True)\n",
    "    html_str = html_object.html_page_str()\n",
    "    # save html string\n",
    "    html_storage[f'{text}'] = html_str\n",
    "\n",
    "    html_objects = {key: HTML(val) for (key, val) in html_storage.items()}\n",
    "    for i in head_dict[text]:\n",
    "        print(f'\\n')\n",
    "        display(html_objects[f'{i}'], display_id=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eea273",
   "metadata": {},
   "source": [
    "# 流程函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6120a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(num):\n",
    "    texts = _choose_date(num)\n",
    "    return texts\n",
    "    \n",
    "def find_activations(model, tokenizer, texts, heads):  # long run\n",
    "    head_dict = {\"text\":[],\"heads\":[]}\n",
    "    for text in texts:\n",
    "        head_dict[\"text\"].append(text)\n",
    "        outputs = forward(model, tokenizer, text)#outputs attention矩阵\n",
    "        _create_attention(outputs, heads, head_dict) \n",
    "    return head_dict #[\"text\":[\"xxx\",\"xxxx\"],\"heads\":[[(5,12),(1,1)],[(3,6),(4,8)]]]\n",
    "\n",
    "#pickle dump head_dict #存json\n",
    "def dump_dict(head_dict):\n",
    "    with open('nrk/head_dict.json', 'w',encoding='utf-8') as fp:\n",
    "        json.dump(head_dict, fp)\n",
    "\n",
    "#pickle load head_dict #读json\n",
    "def load_dict():\n",
    "    with open('nrk/head_dict.json', 'r',encoding='utf-8') as fp:\n",
    "        head_dict = json.load(fp)\n",
    "    return head_dict\n",
    "\n",
    "def show_activations(model, tokenizer, head_dict, heads, k): \n",
    "    texts = choose(k,head_dict[\"text\"]) #重写函数，随机找k个文本输入\n",
    "    head_dict = {\"text\":[],\"heads\":[]}\n",
    "    for i in range(k):\n",
    "        head_dict[\"text\"].append(texts[i])\n",
    "        outputs = forward(model,tokenizer, texts[i])#outputs attention矩阵\n",
    "        _create_attention(outputs, heads, head_dict)\n",
    "        draw(outputs, head_dict[\"text\"][i], head_dict[\"heads\"][i]) #绘图函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41782a0c",
   "metadata": {},
   "source": [
    "# 运行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01c1aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type of argument \"tokens\" must be a list; got str instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-517980c4fa80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# dump_dict(head_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# head_dict = load_dict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mshow_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3e5f1331ca9a>\u001b[0m in \u001b[0;36mshow_activations\u001b[0;34m(model, tokenizer, head_dict, heads, k)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#outputs attention矩阵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0m_create_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"heads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#绘图函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-888f767ba312>\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(attentions, text, heads)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mvall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h n1 n2 -> n1 n2 h '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mhtml_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttentionLogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{i}:{j}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mhtml_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuppress_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/PySvelte/pysvelte/SvelteComponent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mHtml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_and_process_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/PySvelte/pysvelte/SvelteComponent.py\u001b[0m in \u001b[0;36mvalidate_and_process_args\u001b[0;34m(**kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_and_process_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CallMemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_localns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mcheck_argument_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mcheck_argument_types\u001b[0;34m(memo)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# suppress unnecessarily long tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: type of argument \"tokens\" must be a list; got str instead"
     ]
    }
   ],
   "source": [
    "import npm\n",
    "from contextlib import suppress\n",
    "from typing import Optional\n",
    "import random\n",
    "import pysvelte as ps \n",
    "heads = [(5, 12), (7, 9), (3, 11)]\n",
    "\n",
    "texts = read_texts(\"1\")\n",
    "head_dict = find_activations(model, tokenizer, texts, heads)\n",
    "# dump_dict(head_dict)\n",
    "# head_dict = load_dict()\n",
    "show_activations(model, tokenizer, head_dict, heads, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fea0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show_activations(model, tokenizer, head_dict, heads, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbf7a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SvelteComponent AttentionLogits>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.AttentionLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23227a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
