{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20655be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easy_transformer\n",
    "from easy_transformer.EasyTransformer import (\n",
    "    EasyTransformer,\n",
    ")\n",
    "from easy_transformer.ioi_dataset import (\n",
    "    IOIDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src')\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid3/xd/.cache/torch'  # deliberately set this wrong path to avoid migrating cache\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8,7\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, fields, asdict\n",
    "import itertools\n",
    "from itertools import chain, product\n",
    "import math\n",
    "from functools import reduce, partial\n",
    "from collections.abc import Iterable\n",
    "from collections import namedtuple \n",
    "import traceback\n",
    "import pickle, gzip\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# from torch.multiprocessing import Pool\n",
    "# torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer#, pipeline\n",
    "# from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "# from transformers import T5Tokenizer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "# from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _ as __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457df7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import Timer\n",
    "with Timer('common_utils'): from common_utils import *\n",
    "with Timer('utils'): from utils import *\n",
    "with Timer('child_utils'): from child_utils import *\n",
    "from child_utils import _cxt2str, _item2str, _s\n",
    "from child_frames import *\n",
    "with Timer('tasks'): from tasks import *\n",
    "with Timer('model_utils_copy'): from model_utils_copy import *\n",
    "with Timer('weight_analysis'): from weight_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "#cache_dir='/nas/wab/.cache/huggingface/transformers'\n",
    "# cache_dir = '/mnt/nvme1/xd/.cache/torch/transformers/'  # for gpt-j-6B on elderberry\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e322a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -x http://192.168.50.1:1081 -L -O [-C -] https://huggingface.co/google/ul2/resolve/main/pytorch_model.bin  # -C for 断点续传\n",
    "s2s_model_names = ['','google/t5-xl-lm-adapt', 'google/t5-xxl-lm-adapt', 'bigscience/T0p', 'bigscience/T0_3B', \n",
    "    'allenai/tk-instruct-3b-pos', 'allenai/tk-instruct-3b-def-pos', 'google/ul2']#wab\n",
    "gpt_model_names = ['gpt2/cpu','gpt2-medium/cpu','gpt2-xl/cpu','EleutherAI/gpt-j-6B/cpu',]\n",
    "\n",
    "for model_name in gpt_model_names[0:1]:#, 'gpt2-xl', 'EleutherAI/gpt-neo-1.3B', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name in models: continue\n",
    "    with Timer(model_name):\n",
    "        model_cls = AutoModelForCausalLM if any(s in model_name for s in ['gpt', 'fairseq-dense']) else T5ForConditionalGeneration\n",
    "        # _cache_dir = cache_dir.replace('/nas/', '/nas2/') if 'gpt' not in model_name else cache_dir\n",
    "        kwargs = dict(cache_dir=cache_dir, proxies=proxies, low_cpu_mem_usage=True)\n",
    "        if '/cpu' in model_name or 'gpt-j' not in model_name and 'gpt-neox' not in model_name:\n",
    "            model = model_cls.from_pretrained(model_name.replace('/cpu', ''), cache_dir=cache_dir, proxies=proxies)\n",
    "        elif 'gpt-j' in model_name:\n",
    "            device = 0\n",
    "            model = model_cls.from_pretrained(model_name, revision=\"float16\", torch_dtype=torch.float16, **kwargs).to(device)\n",
    "        elif 'gpt-neox' in model_name:\n",
    "            device = 7; device_map = {'gpt_neox': device, 'embed_out': device}\n",
    "            model = model_cls.from_pretrained(model_name, device_map=device_map, load_in_8bit=True, **kwargs)\n",
    "        if hasattr(model.config, 'use_cache'): model.config.use_cache = False  # save GPU mem\n",
    "        # if model_name in ['EleutherAI/gpt-neox-20b']: model = model.half()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name.replace('/cpu', ''), cache_dir=cache_dir)\n",
    "        unify(model)\n",
    "        models[model_name] = model, tokenizer#, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cdacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = gpt_model_names[0]\n",
    "# model_name = engines[-1]\n",
    "model, tokenizer = models[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = model.transformer.h\n",
    "for i, b in enumerate(blocks): b.layer = i\n",
    "ln_f = model.transformer.ln_f\n",
    "L, H, embed_dim = len(blocks), blocks[0].attn.num_heads, blocks[0].attn.embed_dim\n",
    "\n",
    "# we = model.transformer.wte.weight.data\n",
    "# wu = model.lm_head.weight.data\n",
    "\n",
    "# es = [we]\n",
    "# for b in blocks[:1]: es.append(es[-1] + mlp_forward(b, es[-1]))\n",
    "# model.es = es\n",
    "# weBTAs = [es[i].T @ es[i] for i in range(2)]\n",
    "# model.weBTAs = weBTAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ecb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "_ = clone_model_to(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/transformers/notebooks/wab/Easy-Transformer/easy_transformer')\n",
    "from ioi_dataset import (\n",
    "    IOIDataset,\n",
    ")\n",
    "N = 50\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "ioi_dataset = IOIDataset(\n",
    "    prompt_type=\"mixed\",\n",
    "    N=N,\n",
    "    tokenizer=tokenizer,\n",
    "    prepend_bos=False,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = None\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e19130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_predictions_by_data_tuples(model, tokenizer, r1.data_tuples, k_shot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxt, query, cands, *_ = all_examples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxt, query, cands\n",
    "all_examples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333de8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, all_examples = generate_and_predict_batch(model, tokenizer, task= None, nrows=1, k_shot=0, batch_size=10,ioi_dataset = ioi_dataset,\n",
    "                                        trim=False, result=r1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe561ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True or r1.root is None: r1.root = add_node(None, layer=L, label_type='labels')\n",
    "r1.root = attribute_tree_on(r1.data_tuples, model, r1.root, 0, topk=10, k_shot=0, mix=True, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde2341",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_tree(r1.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = r1.root.children[-1]\n",
    "plot_attn_attrs(r1.data_tuples[:1], model, tokenizer, node, topi=[0], attn_patterns=['B->S'], k_shot=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
