{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ea7a72",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: sacremoses in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: filelock in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (2021.10.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/xd/.local/lib/python3.8/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xd/.local/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages/PyYAML-6.0-py3.8-linux-x86_64.egg (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: requests in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/xd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e11bc1d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyfunctional\n",
      "  Using cached PyFunctional-1.4.3-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: dill>=0.2.5 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from pyfunctional) (0.3.4)\n",
      "Requirement already satisfied: tabulate<=1.0.0 in /home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages (from pyfunctional) (0.8.9)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyfunctional\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pyfunctional-1.4.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yfunctional (/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyfunctional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7292808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a886cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid/xd/.cache/torch/transformers\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from transformers.trainer_utils import EvaluationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd287822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _\n",
    "from collections import namedtuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cba5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/vocab.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/merges.txt\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/added_tokens.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/special_tokens_map.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30243f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520d34a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/EleutherAI/gpt-neo-1.3B/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/42252c2220ae3f9f1ea86a994b63e1dcab20953ba8982117c2384587f7c01c5d.102e6e06599c480a8e55be9ba8dc6226140c958f3cd489f61627520db6817595\n",
      "In cached_path: url_or_filename = https://huggingface.co/EleutherAI/gpt-neo-1.3B/resolve/main/pytorch_model.bin\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/7c5fac9d60b015cbc7c007ab8fe6d0512787fbaef81968922959898c49468d73.4c6a483fbfb5a25ac384bfcd71a1ff15245f06583a00c4ab4c44ed0f761f0b08\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/vocab.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/merges.txt\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/added_tokens.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/special_tokens_map.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052c1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = 'Ġ!'; prompt_id = tokenizer._convert_token_to_id(prompt_token)\n",
    "bop_str = 'Instruction: '; bop_id = tokenizer.encode(bop_str)[0]  # 'Inst'\n",
    "eop_str = '. For example:'; eop_id = tokenizer.encode(eop_str)[2] # 'Ġexample'\n",
    "bos_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "eos_id = tokenizer._convert_token_to_id('Ċ')\n",
    "\n",
    "\n",
    "class CHILDDataset(Dataset):\n",
    "    def __init__(self, input_strs, tokenizer):\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = '!'\n",
    "        self.inputs = tokenizer.batch_encode_plus(input_strs, add_special_tokens=False, padding=True, return_tensors='pt')#长的截，短的补\n",
    "        input_ids = self.inputs.input_ids\n",
    "        self.labels = torch.ones_like(input_ids) * (-100)\n",
    "        for bi in range(input_ids.size(0)): #bi 0-127 0-63\n",
    "            bop_idx = (input_ids[bi] == bop_id).nonzero().squeeze(1) #prompt\n",
    "            eop_idx = (input_ids[bi] == eop_id).nonzero().squeeze(1) #context\n",
    "            if len(bop_idx) > 0:\n",
    "                assert len(bop_idx) == 1 and len(eop_idx) == 1\n",
    "                bop_idx, eop_idx = bop_idx.item(), eop_idx.item() #取出单元素张量的元素值并返回该值，保持原元素类型不变\n",
    "                #bop: 0   eop:6\n",
    "                input_ids[bi, bop_idx: eop_idx + 2] *= -1  # use prompt embedding for prompt tokens\n",
    "  \n",
    "            bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "#             print(\"bos_indices:\",bos_indices)\n",
    "            eos_indices = (input_ids[bi] == eos_id).nonzero()[-len(bos_indices):].squeeze(1) #每一位 eos都比bos大2\n",
    "#             print(\"eos_indices:\",eos_indices)\n",
    "            for i, (bos_i, eos_i) in enumerate(zip(bos_indices.tolist(), eos_indices.tolist())):\n",
    "                assert eos_i > bos_i + 1\n",
    "                if i >= 2: self.labels[bi, bos_i + 1: eos_i] = input_ids[bi, bos_i + 1: eos_i] \n",
    "                    \n",
    "    def re_input(self):\n",
    "        return self.inputs['input_ids']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.inputs['input_ids'][i],  #输入\n",
    "                'attention_mask': self.inputs['attention_mask'][i],\n",
    "                'labels': self.labels[i]}   #结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696ea332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from child_utils import *\n",
    "torch.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fcd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEmbedding(nn.Module):\n",
    "    def __init__(self,  \n",
    "                wte: nn.Embedding,  #正常向量\n",
    "                prompt_id: int = None,\n",
    "                prompt_len: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(WrappedEmbedding, self).__init__()\n",
    "#         self.wte = wte\n",
    "#         self.prompt_id = prompt_id\n",
    "#         self.prompt_len = prompt_len\n",
    "        self.__dict__.update(locals()); del self.self #locals()以字典类型返回当前位置的全部局部变量\n",
    "        if self.prompt_id is not None: #prompt_embedding prompt词向量\n",
    "            self.prompt_embedding = nn.parameter.Parameter( #将一个不可训练的类型Tensor转换成可以训练的类型parameter\n",
    "                self.initialize_embedding(random_range, initialize_from_vocab)).to(self.wte.weight.device) #在-0.5-0.5中随机取值初始化\n",
    "        else:\n",
    "            self.prompt_embedding = nn.Embedding(self.prompt_len, self.wte.weight.size(1)).to(self.wte.weight.device)\n",
    "                                        #词典大小（总共输入多少词） 嵌入向量维度（多少维表示一个符号）\n",
    "            assert initialize_from_vocab\n",
    "            self.init_prompt_embedding_()  #将wte的weight值作为初始化\n",
    "#             self.prompt_embedding.weight.data = self.initialize_embedding(random_range, initialize_from_vocab)     \n",
    " \n",
    "    def initialize_embedding(self, random_range: float = 0.5, initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab: return self.wte.weight[:self.prompt_len].clone().detach() #返回一个新的tensor，新的tensor和原来的tensor共享数据内存，但不涉及梯度计算\n",
    "        return torch.FloatTensor(self.prompt_len, self.wte.weight.size(1)).uniform_(-random_range, random_range) #产生随机数\n",
    "    \n",
    "    def init_prompt_embedding_(self):\n",
    "        print(self.wte.weight)\n",
    "        self.prompt_embedding.weight.data[:] = self.wte.weight[:self.prompt_len]\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        if self.prompt_id is not None:\n",
    "            input_embeds = self.wte(input_ids)\n",
    "            input_embeds[input_ids == self.prompt_id] = self.prompt_embedding.expand(input_embeds.size(0), -1, -1)\n",
    "        else: # adapted from cpm-2\n",
    "            prompt_mask = input_ids < 0\n",
    "            prompt_ids = -input_ids * prompt_mask\n",
    "            print(prompt_ids < self.prompt_len)\n",
    "            print(prompt_ids)\n",
    "            assert torch.all(prompt_ids < self.prompt_len)\n",
    "            p_embeds = self.prompt_embedding(prompt_ids) * prompt_mask.float().unsqueeze(-1)\n",
    "            input_ids = input_ids * ~prompt_mask\n",
    "            w_embeds = self.wte(input_ids) * (~prompt_mask).float().unsqueeze(-1)\n",
    "            input_embeds = w_embeds + p_embeds\n",
    "        return input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b17b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from cpm-2: https://github.com/TsinghuaAI/CPM-2-Finetune/blob/master/utils.py#L133-L164\n",
    "def get_params_for_prompt_optimization(module: nn.Module): #不训练模型参数，只训练prompt_embading，这个函数是取这些参数\n",
    "    params = []\n",
    "    for t in module.named_modules():\n",
    "        if \"prompt_embedding\" in t[0]:\n",
    "            params.append({'params': [p for p in list(t[1]._parameters.values()) if p is not None]})\n",
    "    for t in module.named_parameters():\n",
    "        if \"prompt\" not in t[0]:\n",
    "            t[1].requires_grad_(False)    \n",
    "    return params\n",
    "\n",
    "def create_optimizer(model, training_args):\n",
    "    from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "    while isinstance(model, (DDP, )): model = model.module\n",
    "        \n",
    "    we.init_prompt_embedding_()\n",
    "    param_groups = get_params_for_prompt_optimization(model)\n",
    "    optimizer = AdamW(param_groups, lr=training_args.learning_rate, \n",
    "                      betas=(training_args.adam_beta1, training_args.adam_beta2),eps=training_args.adam_epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = model.get_input_embeddings() #提取 embedding 中的 词向量部分\n",
    "if hasattr(wte, 'wte'): wte = wte.wte  # already been wrapped\n",
    "we = WrappedEmbedding(wte, prompt_len=15000)\n",
    "model.set_input_embeddings(we) #为词向量赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalize(obj):\n",
    "    if type(obj) == bool: return 'Yes' if obj else 'No'\n",
    "    return str(obj)\n",
    "\n",
    "def list2str(l): return ' '.join(str(i) for i in l)\n",
    "def pairs2str(pairs): return ', '.join(str(k) + ': ' + str(v) for k, v in pairs)\n",
    "\n",
    "def make_context_str(cxt):\n",
    "    if type(cxt) == list:\n",
    "        return pairs2str(cxt) if type(cxt[0]) == tuple and len(cxt[0]) == 2 else list2str(cxt)\n",
    "    if type(cxt) == tuple:\n",
    "        return '; '.join(make_context_str(c) for c in cxt)  # 用分号分隔context的不同部分\n",
    "    \n",
    "def make_query_str(instruction, query):\n",
    "    if instruction is None and query is None: return ''\n",
    "    s = '.'\n",
    "    if instruction is not None: s = s + ' ' + instruction\n",
    "    if query is not None:\n",
    "        if type(query) in [int, bool, str]: query = [query]\n",
    "        if type(query) == dict:# and list(query.keys())[0] != \"CS\"):  # by nrk\n",
    "            s = s + ' ' + '{' + ','.join([' replace %s with %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "        if type(query) in [list, tuple]:\n",
    "            s = s + ' ' + ' '.join([list2str(i) if type(i) == list else str(i) for i in query])\n",
    "    return s\n",
    "\n",
    "def make_example_str(example, query2str):\n",
    "    instruction, cxt, query, ans = example\n",
    "    if type(ans) not in [Sequence, list]: ans = [ans]\n",
    "    ans = [verbalize(a) for a in ans]\n",
    "#     return '%s -> %s' % (''.join(l[0]) + make_query_str(instruction if with_instruction else None, query), ' '.join(ans))  # by nrk\n",
    "#     return '%s -> %s' % (' '.join(l) + make_query_str(instruction if with_instruction else None, query), ' '.join(ans))  # by XD\n",
    "#     return '%s -> %s' % (make_context_str(cxt) + make_query_str(instruction if with_instruction else None, query[0]), ' '.join(ans))\n",
    "    return '%s -> %s' % (make_context_str(cxt) + query2str(query), ' '.join(ans))\n",
    "\n",
    "\n",
    "def sample_rand_len(vocab, k): return sample(vocab, k=randint(1, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df63ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptize(s):\n",
    "#     return prompt_token * len(s.split())\n",
    "    return bop_str + s + eop_str\n",
    "\n",
    "courses_vocab=[\"Sql\",\"Math\",\"English\",\"Chinese\",\"Art\",\"Music\",\"History\",\"Biology\",\"Chemistry\",\"Physics\",\"Geography\"]\n",
    "all_vocab = [\"apple\",\"pear\",\"peach\",\"grape\",\"banana\",\"pineapple\",\"lemon\",\"strawberry\",\"cola\",\"juice\",\"coffee\",\"milk\",\"wine\",\"beer\",\"whisky\",\"vodka\",\"brandy\"]\n",
    "solid_vocab=[\"apple\",\"pear\",\"peach\",\"grape\",\"banana\",\"pineapple\",\"lemon\",\"strawberry\"]\n",
    "liquid_vocab=[\"cola\",\"juice\",\"coffee\",\"milk\",\"wine\",\"beer\",\"whisky\",\"vodka\",\"brandy\"]\n",
    "names_vocab =  [i for i in string.ascii_uppercase]\n",
    "depts_vocab = [\"CS\",\"Chi\",\"Eng\",\"Mat\"]\n",
    "sl = [\"solid\"]*len(solid_vocab)+[\"liquid\"]*len(liquid_vocab)\n",
    "sl_vocab = dict(zip(all_vocab,sl))\n",
    "\n",
    "def make_input_str(task, nrows=4, ncols=4, full_vocab=None, ans_vocab=[True, False]):\n",
    "    if full_vocab is None: full_vocab = string.ascii_uppercase + string.digits\n",
    "    transform_fn, vocab_fn, sample_fn, query_fn, query2str = task\n",
    "    instruction = transform_fn.__name__.replace('_', ' ')\n",
    "    if vocab_fn is None: vocab_fn = lambda: full_vocab\n",
    "    if query_fn is None: query_fn = lambda *_: None\n",
    "        \n",
    "    examples = []\n",
    "    query = None\n",
    "    for i in range(nrows):\n",
    "        vocab = vocab_fn()\n",
    "        l = sample_fn(vocab, k=ncols)\n",
    "        query = query_fn(l, vocab, ncols)\n",
    "        examples.append([instruction, l, query, transform_fn(l, query=query)])\n",
    "    examples = balance(examples,ans_vocab)\n",
    "\n",
    "    desc = promptize(instruction) if True else ''\n",
    "    text = '\\n'.join([make_example_str(e, query2str) for e in examples])\n",
    "    text = desc + '\\n' + text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd0c286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def balance(examples, ans_vocab=[True, False]):\n",
    "def balance(examples, ans_vocab):\n",
    "    groups = seq(examples).group_by(_[-1]).map(_[1])  # 按ans分组\n",
    "    assert groups.len() == len(ans_vocab), '%d < %d' % (groups.len(), len(ans_vocab))  # 保证每种ans都出现\n",
    "    min_cnt = groups.map(lambda x: len(x)).min()\n",
    "    examples = groups.map(lambda x: sample(x, min_cnt)).flatten().list() # 每组都采样最小个数后去分组\n",
    "    return sample(examples, len(examples))  # 重新打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f75834f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tasks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e250ac18b8ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery2str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tasks' is not defined"
     ]
    }
   ],
   "source": [
    "task = tasks[-8]\n",
    "transform_fn, vocab_fn, sample_fn, query_fn, query2str = task\n",
    "vocab = vocab_fn()\n",
    "ncols = (3, 3, 2, 5)\n",
    "l = sample_fn(vocab, k=ncols)\n",
    "SC, CD = l\n",
    "query = query_fn(l, vocab, ncols)\n",
    "ss, d = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab80cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  \n",
    "def all_a(cxt, query):\n",
    "    SC, CD = cxt  # SC paris: studeng-course relation, CD pairs: course-department function\n",
    "    ss, d = query  # ss: 学生子集（可以*不止两个学生*），d: 课程\n",
    "#     return seq(ss).map(lambda s: seq(SC).filter(_[0] == s).map(_[1]).intersection(CD.filter(_.[1] == d).map(_.[0])).non_empty()).all()\n",
    "    return (seq(ss)\n",
    "            .map(lambda s: seq(SC).filter(_[0] == s).map(_[1])  # 学生s选的所有课程\n",
    "                 .intersection(\n",
    "                     seq(CD).filter(_[1] == d).map(_[0])) # d系的课程\n",
    "                 .non_empty())  # s选了d系的课程\n",
    "            .all())  # 学生子集ss都选了d系的课程\n",
    "\n",
    "def all_a_sample(vocab, k):\n",
    "    S_vocab, C_vocab, D_vocab = vocab  # vocabs of students, courses, departments\n",
    "    k_S, k_C, k_D, k_SC = k  # default values: k_S = 3, k_C = 3, k_D = 2, k_SC = 5\n",
    "    S, C, D = sample(S_vocab, k_S), sample(C_vocab, k_C), sample(D_vocab, k_D)\n",
    "    \n",
    "    while len(set(CD := choices(D, k=k_C))) < k_D: continue  # ds里每个系的课都要出现\n",
    "    CD = list(zip(C, CD))  # 得到每门课所属的系\n",
    "    \n",
    "    all_SC = list(itertools.product(S, C))  # or seq(S).cartesian(C).list()\n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue  # ss里每个学生都要选课\n",
    "    return SC, CD\n",
    "\n",
    "def select_distinct(tuples, col): return seq(tuples).map(_[col]).distinct().list()\n",
    "    \n",
    "def all_a_query(cxt,vocab,k):\n",
    "    SC, CD = cxt\n",
    "    k_S, k_C, k_D, k_SC = k\n",
    "    S, D = select_distinct(SC, 0), select_distinct(CD, 1)\n",
    "    k_ss = randint(2, len(S))\n",
    "    ss = sample(S, k_ss)\n",
    "    d = choice(D)\n",
    "    return ss, d\n",
    "\n",
    "def all_a_query2str(query):\n",
    "    ss, d = query\n",
    "    query_str = 'Do %s all take %s courses?' % (', '.join(ss[:-1]) + ' and ' + ss[-1], d)\n",
    "    return '. ' + query_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8103db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nrk\n",
    "def all_b(cxt, query):\n",
    "    SC, CD = cxt\n",
    "#     ss,d = str_solve(query)  # XD: 不要在transform_fn里做解析\n",
    "    ss,d = query\n",
    "    return (seq(CD).filter(_[1] == d).map(_[0])\n",
    "                 .difference(\n",
    "                     seq(SC).filter(_[0] == ss).map(_[1]))\n",
    "                 .empty())\n",
    "\n",
    "def all_b_sample(vocab, k):\n",
    "    S_vocab, C_vocab, D_vocab = vocab  \n",
    "    k_S, k_C, k_D, k_SC = k  # default values: k_S = 2, k_C = 3, k_D = 2, k_SC = 5\n",
    "    S, C, D = sample(S_vocab, k_S), sample(C_vocab, k_C), sample(D_vocab, k_D)\n",
    "    \n",
    "    while len(set(CD := choices(D, k=k_C))) < k_D: continue  \n",
    "    CD = list(zip(C, CD)) \n",
    "    \n",
    "    all_SC = list(itertools.product(S, C))  \n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue  # ss里每个学生都要选课\n",
    "    return SC, CD\n",
    "   \n",
    "def all_b_query(cxt,vocab,k):  # XD: 不要给qeury_fn加st参数\n",
    "    SC, CD = cxt\n",
    "    k_S, k_C, k_D, k_SC = k\n",
    "    S, D = select_distinct(SC, 0), select_distinct(CD, 1)\n",
    "#     k_ss = randint(2, len(S))  # XD: k_ss unused\n",
    "    ss = choice(S)\n",
    "    d = choice(D)\n",
    "    return ss, d\n",
    "    # XD: 不要在query_fn里转str！！这里转str，transform_fn里再解析回来，两边不是白折腾吗！\n",
    "\n",
    "def all_b_query2str(query):\n",
    "    ss, d = query\n",
    "    query_str = 'Does %s take all %s courses?' % (ss, d)\n",
    "    return '. ' + query_str\n",
    "\n",
    "\n",
    "def Find_the_intersection_of_two_elements(cxt, query):\n",
    "    SC= cxt\n",
    "    s,d = query\n",
    "    return (seq(SC).filter(_[0] == s).map(_[1])\n",
    "                 .intersection(\n",
    "                     seq(SC).filter(_[0] == d).map(_[1]))\n",
    "                 .non_empty())\n",
    "\n",
    "def intersection_query(cxt,vocab,k):\n",
    "    SC= cxt\n",
    "    k_S, k_C, k_SC = k #k_s = 3, k_C = 5 , k_sc = 6\n",
    "    S = select_distinct(SC, 0)\n",
    "    s,d = sample(S,2)\n",
    "    return s, d\n",
    "\n",
    "def intersection_query2str(query):\n",
    "    s, d = query\n",
    "    query_str = \"Is there an intersection between %s and %s?\" % (s, d)\n",
    "    return '. ' + query_str\n",
    "\n",
    "def intersection_sample(vocab, k):\n",
    "    S_vocab, C_vocab = vocab  \n",
    "    k_S, k_C, k_SC = k  # k_S = 3, k_C = 5, , k_SC = 6\n",
    "    S, C= sample(S_vocab, k_S), sample(C_vocab, k_C)\n",
    "    all_SC = list(itertools.product(S, C))  \n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue \n",
    "    return SC\n",
    "\n",
    "def complement(cxt, query):\n",
    "    SC= cxt\n",
    "    s,d = query\n",
    "    return (seq(SC).filter(_[0] == d).map(_[1])\n",
    "                 .union(\n",
    "                     seq(SC).filter(_[0] == s).map(_[1]))\n",
    "                 ).distinct().len()== seq(SC).filter(_[0] == d).map(_[1]).distinct().len()\n",
    "\n",
    "def complement_query(cxt,vocab,k):\n",
    "    SC= cxt\n",
    "    k_S, k_C, k_SC = k #k_s = 3, k_C = 5 , k_sc = 6\n",
    "    S = select_distinct(SC, 0)\n",
    "    s,d = sample(S,2)\n",
    "    return s, d\n",
    "\n",
    "def complement_query2str(query):\n",
    "    s, d = query\n",
    "    query_str = \"Is %s a subset of %s?\" % (s, d)\n",
    "    return '. ' + query_str\n",
    "\n",
    "def complement_sample(vocab, k):\n",
    "    S_vocab, C_vocab = vocab  \n",
    "    k_S, k_C, k_SC = k  # k_S = 3, k_C = 4, k_SC = 5\n",
    "    S, C= sample(S_vocab, k_S), sample(C_vocab, k_C)\n",
    "    all_SC = list(itertools.product(S, C))  \n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue \n",
    "    return SC\n",
    "\n",
    "def union(cxt, query):\n",
    "    SC, DC = cxt\n",
    "    ss,d = query\n",
    "    return (seq(ss)\n",
    "            .map(lambda s: seq(SC).filter(_[0] == s).map(_[1]) \n",
    "                 .union(\n",
    "                     seq(DC).filter(_[0] == d).map(_[1])) \n",
    "                 .distinct().len() == seq(DC).filter(_[0] == d).map(_[1]).distinct().len())  \n",
    "            .all()) \n",
    "\n",
    "def union_sample(vocab, k):\n",
    "    S_vocab, C_vocab, D_vocab = vocab  \n",
    "    k_S, k_C, k_D, k_SC = k  # k_S = 3, k_C = 4, k_D = 2, k_SC = 6\n",
    "    S, C, D = sample(S_vocab, k_S), sample(C_vocab, k_C), sample(D_vocab, k_D)\n",
    "    \n",
    "    while len(set(DC := choices(D, k=k_C))) < k_D: continue  \n",
    "    DC = list(zip(DC,C)) \n",
    "    \n",
    "    all_SC = list(itertools.product(S, C))  \n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue  \n",
    "    return SC, DC\n",
    "   \n",
    "def union_query(cxt,vocab,k): \n",
    "    SC, DC = cxt\n",
    "    k_S, k_C, k_D, k_SC = k\n",
    "    S, D = select_distinct(SC, 0), select_distinct(DC, 0)\n",
    "    k_ss = randint(2, len(S))\n",
    "    ss = sample(S, k_ss)\n",
    "    d = choice(D)\n",
    "    return ss, d\n",
    "\n",
    "def union_query2str(query):\n",
    "    ss, d = query\n",
    "    query_str = 'Is the union of %s a subset of %s?' % (', '.join(ss[:-1]) + ' and ' + ss[-1], d)\n",
    "    return '. ' + query_str\n",
    "\n",
    "def find_same(cxt, query): #新建联系，把vocab和属性连起来形成新集合\n",
    "    NA, DA = cxt\n",
    "    s,d = query\n",
    "    D = seq(DA).filter(_[0] == d).map(_[1])\n",
    "    return (seq(NA).filter(_[0] == s).map(_[1]).select(lambda x: sl_vocab[x] == sl_vocab[D[0]]).any())\n",
    "      \n",
    "    \n",
    "def find_sample(vocab, k): \n",
    "    name_vocab, all_vocab = vocab\n",
    "    k_name, k_all, k_D, k_SA = k  # k_name = 3, k_all = 4, k_D = 2, k_SA = 6\n",
    "    S = list(all_vocab.keys())\n",
    "    N, A, D = sample(name_vocab, k_name), sample(S, k_all), sample(string.ascii_lowercase, k_D)\n",
    "#     D = sample(d := name_vocab.pop(name_vocab.index(a) for a in N),k_D)\n",
    "    A1 = sample(S, k_D)\n",
    "    DA = list(zip(D,A1)) \n",
    "    \n",
    "    all_NA = list(itertools.product(N, A))  \n",
    "    while seq(NA := sample(all_NA, k_SA)).map(_[0]).distinct().len() < k_name: continue  \n",
    "    return NA, DA\n",
    "   \n",
    "def find_query(cxt,vocab,k): \n",
    "    NA, DA = cxt\n",
    "    k_name, k_all, k_D, k_SA = k\n",
    "    S,D = select_distinct(NA, 0), select_distinct(DA, 0)\n",
    "    s,d = choice(S), choice(D)\n",
    "    return s, d\n",
    "\n",
    "def find_query2str(query):\n",
    "    s, d = query\n",
    "    query_str = 'Is there an element in %s that belongs to the same class as %s?' % (s, d)\n",
    "    return '. ' + query_str\n",
    "\n",
    "\n",
    "def find_differ(cxt, query):\n",
    "    NA = cxt\n",
    "    ss = query\n",
    "    return (seq(ss).map(lambda s: seq(NA).filter(_[0] == s).map(_[1])\n",
    "                        .select(lambda x: sl_vocab[x])[0])\n",
    "            .distinct().len( ) == 2)\n",
    "               \n",
    "def find_dif_sample(vocab, k):\n",
    "    name_vocab, all_vocab = vocab\n",
    "    k_name, k_all, k_SA = k  # k_name = 4, k_all = 4, k_SA= 3\n",
    "    N, A = sample(name_vocab, k_name), sample(list(all_vocab.keys()), k_all) \n",
    "    NA = list(zip(N,A)) \n",
    "    return NA\n",
    "   \n",
    "def find_dif_query(cxt,vocab,k): \n",
    "    NA = cxt\n",
    "    k_name, k_all, k_NA = k\n",
    "    S = select_distinct(NA, 0)\n",
    "    ss = sample(S,k_NA)\n",
    "    return ss\n",
    "\n",
    "\n",
    "def find_dif_query2str(query):\n",
    "    ss = query\n",
    "    query_str = 'Are there any elements different from other elements in %s?' % (', '.join(ss[:-1]) + ' and ' + ss[-1])\n",
    "    return '. ' + query_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "464efbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(cxt, query):\n",
    "    s = cxt\n",
    "    q = query\n",
    "    return seq(s).filter(lambda x: sl_vocab[x] == sl_vocab[q[0]]).len()\n",
    "                     \n",
    "def count_sample(vocab, k):\n",
    "    all_vocab = vocab\n",
    "    k_cxt,k_query = k  # k_cxt = 4 ,k_query =1\n",
    "    N = list(all_vocab.keys())\n",
    "    s = sample(N, k_cxt) \n",
    "    return s\n",
    "   \n",
    "def count_query(cxt,vocab,k): \n",
    "    NA = cxt\n",
    "    k_cxt,k_query = k\n",
    "    N = list(vocab.keys())\n",
    "    q = sample(N,k_query)\n",
    "    return q\n",
    "\n",
    "def count_query2str(query):\n",
    "    q = query\n",
    "    query_str = 'How many elements are similar to %s?' % (q[0])\n",
    "    return '. ' + query_str\n",
    "\n",
    "def compare(cxt, query):\n",
    "    s = cxt\n",
    "    q = query\n",
    "    return (seq(s).filter(lambda x: sl_vocab[x] == sl_vocab[q[0]]).len() >= len(s)/2)\n",
    "                     \n",
    "def compare_sample(vocab, k):\n",
    "    all_vocab,s_vocab,l_vocab = vocab\n",
    "    k_cxt,k_query = k  # k_cxt = 5 ,k_query =2\n",
    "    N = list(all_vocab.keys())\n",
    "    s = sample(N, k_cxt) \n",
    "    return s\n",
    "   \n",
    "def compare_query(cxt,vocab,k): \n",
    "    NA = cxt\n",
    "    all_vocab,s_vocab,l_vocab = vocab\n",
    "    k_cxt,k_query = k\n",
    "    q = sample(s_vocab,1)+sample(l_vocab,1)\n",
    "    random.shuffle(q)\n",
    "    return q\n",
    "\n",
    "def compare_query2str(query):\n",
    "    ss,q = query\n",
    "    query_str = 'Is the quantity similar to %s greater than that of %s?' % (q[0],q[1])\n",
    "    return '. ' + query_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62aee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ascii_size_existence(l, query): return seq(l).map(_[0] > query).any()\n",
    "def Ascii_size_all(l, query): return seq(l).map(_[0] > query).all()\n",
    "def Ascii_size_None(l, query): return seq(l).filter(_[0] > query).empty()\n",
    "def ith_element(l, query=None): return seq(l).slice(1, 2)\n",
    "def ith_group(l, query=None): return seq(l).group_by(_).select(_[1]).slice(1, 2).flatten()#.distinct()# davinci F w/ and wo dist\n",
    "# def element_at_index(l, query): return seq(l).slice(query, query + 1) # davinci F\n",
    "def element_at_index(l, query): return seq(l).enumerate().filter(_[0] == query).select(_[1])\n",
    "def replace(l, query): return seq(l).map(lambda x: query.get(x, x))\n",
    "def replace_with_the_other(l, query): # davinci F\n",
    "    query = {k: (set(l) - {k}).pop() for k in l}\n",
    "    return replace(l, query)\n",
    "def replace_all_with(l, query): return seq(l).map(lambda x: query)  # davinci F?!\n",
    "def interleave_with(l, query): return seq(l).flat_map(lambda x: [x, query])  # davinci T!!\n",
    "def unique_elements(l, query=None): return seq(l).distinct() # davinci F\n",
    "def how_many_unique_elements(l, query=None): return seq(l).distinct().len()  # davinci F\n",
    "def how_many(l, query): return seq(l).filter(_ == query).len() # davinci F\n",
    "def select_same_as(l, query): return seq(l).filter(_ == query) # simpler version of how_many. davinci F\n",
    "def select_same_number_as(l, query): return seq(l).group_by(_).select(_[1]).filter(lambda x: len(x) == len(query)).flatten() # F\n",
    "def includes(l, query): return seq(l).union(seq(query)).distinct().len() == seq(l).distinct().len() # davinci F\n",
    "def is_included_by(l, query): return seq(l).difference(seq(query)).empty() # davinci F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f199b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    (ith_element,            None,                               sample,    None,None),\n",
    "    (ith_group,              None, lambda vocab, k: seq(sample(vocab, k)).map(lambda x:[x]*randint(1, 3)).flatten().list(),None,None),\n",
    "    (element_at_index,       lambda: upper_letters,              sample,    lambda l,vocab,k: randint(0, min(2,len(l)-1))),\n",
    "    (replace,                None,                               sample,    lambda l,vocab,k: {choice(l): choice(vocab)}),\n",
    "    (replace_with_the_other, lambda: sample(full_vocab, 2),   lambda vocab,k: sample(vocab+choices(vocab, k=k-2),k), None),\n",
    "    (replace_all_with,       None,                               sample_rand_len, lambda l,vocab,k: choice(vocab)),\n",
    "    (interleave_with,        None,                               sample_rand_len, lambda l,vocab,k: choice(vocab)),\n",
    "    (unique_elements,        lambda: sample(upper_letters, 3),   choices,   None),\n",
    "    (how_many_unique_elements,lambda: sample(upper_letters, 3),  choices,   None),\n",
    "    (how_many,               lambda: sample(upper_letters, 3),   choices,   lambda l,vocab,k: choice(list(set(l)))),\n",
    "    (select_same_as,         lambda: sample(upper_letters, 3),   choices,   lambda l,vocab,k: choice(list(set(l)))),\n",
    "    (select_same_number_as,  None, lambda vocab, k: seq(sample(vocab, k)).map(lambda x:[x]*randint(1, 3)).flatten().list(),   \n",
    "     lambda l,vocab,k: [choice(vocab)]*randint(1, 3)),\n",
    "    (includes,               lambda: sample(upper_letters, 6),   sample,    lambda l,vocab,k: sample(vocab, 3)),\n",
    "    (is_included_by,         lambda: sample(upper_letters, 6),   sample,    lambda l,vocab,k: sample(vocab, 5)),\n",
    "    \n",
    "    (Ascii_size_None,        lambda: upper_letters,              sample,    lambda l,vocab,k: \"Is there no element greater than %s?\" % choice(list(set(l)))),\n",
    "    (Ascii_size_all,         lambda: upper_letters,              sample,    lambda l,vocab,k: \"Are all elements greater than %s?\" % choice(list(set(l)))),\n",
    "    (Ascii_size_existence,   lambda: upper_letters,              sample,    lambda l,vocab,k: \"Is there an element greater than %s?\" % choice(list(set(l)))),\n",
    "    \n",
    "    (all_a,                  lambda: [names_vocab,courses_vocab,depts_vocab],     all_a_sample,    all_a_query, all_a_query2str),\n",
    "    (all_b,                  lambda: [names_vocab,courses_vocab,depts_vocab],     all_b_sample,    all_b_query, all_b_query2str),\n",
    "    (Find_the_intersection_of_two_elements,           lambda: [upper_letters,string.ascii_lowercase],     intersection_sample,    intersection_query, intersection_query2str),\n",
    "    (complement,           lambda: [upper_letters,string.ascii_lowercase],     complement_sample,    complement_query, complement_query2str),\n",
    "    (union,                lambda: [upper_letters,string.ascii_lowercase,depts_vocab],     union_sample,    union_query, union_query2str),\n",
    "    (find_same,            lambda: [upper_letters,sl_vocab],      find_sample,    find_query, find_query2str ),\n",
    "    (find_differ,          lambda: [upper_letters,sl_vocab],      find_dif_sample,  find_dif_query, find_dif_query2str ),\n",
    "    (count,                lambda: sl_vocab,                                      count_sample,            count_query,count_query2str),\n",
    "    (compare,              lambda: [sl_vocab,solid_vocab,liquid_vocab],          compare_sample,            compare_query,compare_query2str),\n",
    "]"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8ca20270",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e98e87d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: find differ. For example:\n",
      "M: brandy, R: vodka, H: lemon, D: strawberry. Are there any elements different from other elements in D, M and H? -> Yes\n",
      "R: pineapple, S: apple, G: coffee, C: whisky. Are there any elements different from other elements in G, C and R? -> Yes\n",
      "S: peach, D: pineapple, J: strawberry, F: pear. Are there any elements different from other elements in D, F and S? -> No\n",
      "P: apple, H: pear, T: beer, E: coffee. Are there any elements different from other elements in H, T and E? -> Yes\n",
      "L: pear, M: banana, J: strawberry, D: peach. Are there any elements different from other elements in L, D and J? -> No\n",
      "H: peach, A: pear, C: strawberry, B: banana. Are there any elements different from other elements in B, C and A? -> No\n",
      "B: vodka, E: brandy, X: coffee, S: milk. Are there any elements different from other elements in E, X and S? -> No\n",
      "B: vodka, S: juice, W: pineapple, T: banana. Are there any elements different from other elements in S, T and W? -> Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-3], nrows=8, ncols=(4,4,3))) "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "affea386",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fb4988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: find same. For example:\n",
      "E: juice, M: wine, M: banana, A: juice; b: vodka, d: whisky. Is there an element in M that belongs to the same class as b? -> Yes\n",
      "L: milk, X: milk, M: beer, X: vodka; n: wine, o: apple. Is there an element in L that belongs to the same class as o? -> No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-4], nrows=6, ncols=(3,4,2,4)))  #找相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4070da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: compare. For example:\n",
      "pear lemon peach vodka beer. Is the quantity similar to c greater than that of o? -> Yes\n",
      "juice cola pineapple whisky wine. Is the quantity similar to w greater than that of i? -> No\n",
      "grape juice cola apple banana. Is the quantity similar to g greater than that of r? -> No\n",
      "coffee peach juice whisky apple. Is the quantity similar to g greater than that of r? -> Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-1], nrows=8, ncols=(5,2))) #比较，query中给了两个元素（固体、液体），\n",
    "                                                        #若前一个元素的类型数目大于后一个，就输出YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f3668d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: count. For example:\n",
      "pear peach brandy. How many elements are similar to vodka? -> 1\n",
      "apple grape banana. How many elements are similar to coffee? -> 0\n",
      "beer milk cola. How many elements are similar to juice? -> 3\n",
      "cola pineapple juice. How many elements are similar to juice? -> 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-2], nrows=10, ncols=(3,1), ans_vocab=[0,1,2,3]))    #数数，数cxt中与query中元素同类的个数\n",
    "                                                                                  #这里修改了balance函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db442a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: union. For example:\n",
      "A: w, X: a, M: a, M: w; Eng: a, CS: q, Eng: w, CS: v. Is the union of M, A and X a subset of Eng? -> Yes\n",
      "R: k, M: a, E: k, R: a; Mat: a, Eng: t, Mat: v, Mat: k. Is the union of E and R a subset of Eng? -> No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-5], nrows=18, ncols=(3,4,2,4))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eadf0bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: complement. For example:\n",
      "C: l, H: p, J: s, J: m, H: l. Is C a subset of H? -> Yes\n",
      "S: y, J: o, S: q, R: q, J: r. Is R a subset of J? -> No\n",
      "L: x, T: n, L: w, L: u, A: s. Is T a subset of L? -> No\n",
      "F: h, S: z, D: h, D: r, S: u. Is F a subset of D? -> Yes\n",
      "X: e, X: v, W: g, G: e, X: n. Is G a subset of X? -> Yes\n",
      "B: w, E: y, B: k, P: d, E: r. Is P a subset of E? -> No\n",
      "X: v, S: f, X: w, C: q, C: y. Is S a subset of C? -> No\n",
      "W: p, B: r, G: x, W: r, W: c. Is B a subset of W? -> Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-6], nrows=18, ncols=(3,11,5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8c59a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: intersection. For example:\n",
      "M: e, M: j, P: d, M: d, C: t. Is there an intersection between M and P? -> Yes\n",
      "G: p, H: p, J: r, G: u, H: r. Is there an intersection between J and G? -> No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-7], nrows=4, ncols=(3,4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "105e915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: all b. For example:\n",
      "H: English, W: English, H: Math, S: Math, W: History; English: Chi, Math: Chi, History: Eng. Does S take all Chi courses? -> No\n",
      "M: History, S: Art, Z: Chemistry, M: Chemistry, M: Art; Art: Eng, History: CS, Chemistry: Eng. Does M take all CS courses? -> Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-8], nrows=8, ncols=(3, 3, 2, 5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df9c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e420842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "6f1f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = ['Math', 'Chem', \"Algebra\"]\n",
    "depts = ['CS', 'Phy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "id": "67b27b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Phy', 'Math'), ('CS', 'Chem'), ('Phy', 'Algebra')]"
      ]
     },
     "execution_count": 1028,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(random.choices(depts, k=len(courses)), courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bde21d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġintersection']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(' intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "bea4157b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: all. For example:\n",
      "B:Math A:Analytcal_Chemistry B:Physics A:Linear_Algebra CS:Math CS:Analytcal_Chemistry. Do A,B all choose CS courses? -> Yes\n",
      "A:Art CS:Analytcal_Chemistry B:Biology B:English CS:Biology A:Analytcal_Chemistry. Do A,B all choose CS courses? -> Yes\n",
      "A:General_Chemistry CS:General_Chemistry CS:Environmental_Monitoring A:Environmental_Monitoring B:Sql B:C_language. Do A,B all choose CS courses? -> No\n",
      "B:Math B:Sql A:History CS:Math A:Physics CS:History. Do A,B all choose CS courses? -> Yes\n",
      "A:Physics B:History A:Biology CS:Physics CS:History B:General_Chemistry. Do A,B all choose CS courses? -> Yes\n",
      "A:Analytcal_Chemistry B:Chinese CS:General_Chemistry CS:Sql B:Sql A:General_Chemistry. Do A,B all choose CS courses? -> Yes\n",
      "A:Sql CS:Music B:Environmental_Monitoring CS:Sql A:Music B:English. Do A,B all choose CS courses? -> No\n",
      "B:Geography A:Math B:Physics CS:Math A:General_Chemistry CS:Geography. Do A,B all choose CS courses? -> Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-6], nrows=8, ncols=6)) #AB两人是否全部选了CS课程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7fe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "06d90c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: all. For example:\n",
      "CS:Analytcal_Chemistry A:Chinese B:Linear_Algebra A:Math. Do A,B all choose CS courses? -> Yes\n",
      "CS:Environmental_Monitoring CS:C_language B:C_language A:Environmental_Monitoring. Do A,B all choose CS courses? -> No\n",
      "A:Linear_Algebra CS:General_Chemistry B:Physics B:Art. Do A,B all choose CS courses? -> Yes\n",
      "A:Geography CS:Biology B:Environmental_Monitoring B:C_language. Do A,B all choose CS courses? -> Yes\n",
      "B:Art CS:Art CS:Music A:Operating_Systems. Do A,B all choose CS courses? -> No\n",
      "CS:Physics B:General_Chemistry A:Chinese CS:Chinese. Do A,B all choose CS courses? -> No\n",
      "CS:Linear_Algebra CS:Music B:Music B:Geography. Do A,B all choose CS courses? -> No\n",
      "A:Linear_Algebra B:General_Chemistry CS:C_language A:Math. Do A,B all choose CS courses? -> Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-2], nrows=8, ncols=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17373019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Find the intersection of two elements. For example:\n",
      "M: a, I: k, I: c, M: k, D: m. Is there an intersection between I and D? -> No\n",
      "T: i, P: t, W: i, T: y, P: y. Is there an intersection between P and W? -> No\n",
      "E: r, E: o, D: r, D: v, X: q. Is there an intersection between D and X? -> No\n",
      "E: v, R: u, X: u, E: k, X: d. Is there an intersection between X and E? -> No\n",
      "W: j, A: q, W: q, L: j, W: e. Is there an intersection between A and W? -> Yes\n",
      "R: j, R: e, W: c, P: j, W: e. Is there an intersection between R and P? -> Yes\n",
      "R: b, D: w, C: c, C: b, D: c. Is there an intersection between D and C? -> Yes\n",
      "H: w, D: k, P: k, H: k, D: q. Is there an intersection between H and P? -> Yes\n",
      "C: p, M: y, M: p, M: i, W: p. Is there an intersection between W and M? -> Yes\n",
      "W: j, C: n, B: o, W: o, W: n. Is there an intersection between B and C? -> No\n",
      "M: l, M: i, F: l, X: g, M: q. Is there an intersection between X and M? -> No\n",
      "J: l, I: l, I: u, J: u, A: l. Is there an intersection between I and J? -> Yes\n",
      "T: y, A: j, M: f, T: f, T: s. Is there an intersection between M and A? -> No\n",
      "R: r, J: g, R: w, J: w, P: r. Is there an intersection between J and R? -> Yes\n",
      "\n",
      "Instruction: Find the intersection of two elements. For example:\n",
      "J: l, A: l, C: w, J: w, C: l. Is there an intersection between C and A? -> Yes\n",
      "P: t, P: o, P: m, L: o, T: m. Is there an intersection between T and L? -> No\n",
      "S: b, S: p, I: p, I: b, T: p. Is there an intersection between T and S? -> Yes\n",
      "H: i, B: p, A: u, H: u, A: p. Is there an intersection between B and H? -> No\n",
      "C: z, X: v, C: e, J: v, X: e. Is there an intersection between J and C? -> No\n",
      "G: h, G: p, M: h, D: r, G: r. Is there an intersection between D and M? -> No\n",
      "J: x, J: t, E: e, A: e, A: t. Is there an intersection between E and A? -> Yes\n",
      "E: r, S: g, T: g, S: h, S: r. Is there an intersection between S and E? -> Yes\n",
      "A: n, W: q, S: d, A: d, S: n. Is there an intersection between W and A? -> No\n",
      "H: k, R: d, S: p, H: p, R: k. Is there an intersection between R and S? -> No\n",
      "R: s, G: o, I: y, R: o, I: o. Is there an intersection between G and R? -> Yes\n",
      "P: i, P: o, L: o, H: k, L: k. Is there an intersection between L and P? -> Yes\n",
      "R: g, A: n, R: n, R: r, B: g. Is there an intersection between A and R? -> Yes\n",
      "F: u, J: a, F: a, H: x, F: l. Is there an intersection between F and J? -> Yes\n",
      "I: s, F: h, G: s, F: a, F: f. Is there an intersection between G and F? -> No\n",
      "W: r, B: j, I: s, W: s, B: s. Is there an intersection between I and B? -> Yes\n",
      "R: b, R: r, X: r, A: b, R: i. Is there an intersection between X and A? -> No\n",
      "W: u, D: u, W: g, I: g, D: o. Is there an intersection between I and D? -> No\n",
      "\n",
      "Instruction: Find the intersection of two elements. For example:\n",
      "E: b, L: o, L: b, L: s, H: b. Is there an intersection between L and H? -> Yes\n",
      "E: o, C: o, R: o, C: v, E: n. Is there an intersection between C and R? -> Yes\n",
      "I: z, E: a, T: z, T: a, I: u. Is there an intersection between E and T? -> Yes\n",
      "H: l, A: w, H: i, A: r, S: r. Is there an intersection between A and H? -> No\n",
      "E: y, E: c, D: b, F: c, D: y. Is there an intersection between F and D? -> No\n",
      "A: j, A: w, A: y, R: j, P: y. Is there an intersection between A and P? -> Yes\n",
      "M: k, P: g, D: g, M: a, D: k. Is there an intersection between D and M? -> Yes\n",
      "C: i, F: b, I: c, F: c, F: p. Is there an intersection between C and F? -> No\n",
      "R: d, G: g, I: y, R: p, R: y. Is there an intersection between I and R? -> Yes\n",
      "S: i, S: w, M: i, J: o, J: e. Is there an intersection between M and J? -> No\n",
      "F: v, I: v, F: z, J: a, I: z. Is there an intersection between J and I? -> No\n",
      "I: g, H: s, L: k, H: m, L: s. Is there an intersection between H and I? -> No\n",
      "F: y, E: b, F: j, E: y, I: b. Is there an intersection between E and F? -> Yes\n",
      "A: q, L: q, J: j, A: r, J: r. Is there an intersection between L and J? -> No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n_total, n_valid = 1800, 360  #全部数目，测试数目\n",
    "n_total, n_valid = 180, 60\n",
    "n_train = n_total - n_valid\n",
    "input_strs = [make_input_str(tasks[-7], nrows=25, ncols=(3,4,5)) for __ in range(n_total)]\n",
    "# input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)\n",
    "# print(input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s.count('Yes') for s in input_strs)\n",
    "sum(s.count('No') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91d9f555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ebf074a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\",  #模型预测和检查点的输出目录\n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True, prediction_loss_only=False,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,  #每个GPU / TPU内核/ CPU的批处理大小\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,    #weight_decay要应用的权重衰减,adam_epsilon AdamW优化器的ε超参数\n",
    "    lr_scheduler_type='constant', learning_rate=1e-3, num_train_epochs=2,  #learning_rate:Adam初始学习率\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,  #save_steps保存两个检查点之前的更新步骤数\n",
    "    no_cuda=False, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/nas/xd/projects/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74343a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1440\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nas/xd/projects/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
      "\u001b[0;32m/nas/xd/projects/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kf/miniconda3/envs/pytorch1.7.1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4e4946ed53d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprompt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprompt_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprompt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_ids\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mp_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprompt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mprompt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c00d277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 128\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 32:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.786300</td>\n",
       "      <td>0.735548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.675856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.660071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.639453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.6875235140323639, metrics={'train_runtime': 2022.7462, 'train_samples_per_second': 0.253, 'train_steps_per_second': 0.016, 'total_flos': 2674345778872320.0, 'train_loss': 0.6875235140323639, 'epoch': 4.0})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7802388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 128\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 1:11:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.786300</td>\n",
       "      <td>0.735548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.675856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.660071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.639453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.606500</td>\n",
       "      <td>0.638131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>0.619811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>0.613889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.620138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>0.600722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.606176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.6249251246452332, metrics={'train_runtime': 4331.7206, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.018, 'total_flos': 6685864447180800.0, 'train_loss': 0.6249251246452332, 'epoch': 10.0})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ea5fc8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 01:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8791098594665527,\n",
       " 'eval_runtime': 102.8425,\n",
       " 'eval_samples_per_second': 0.622,\n",
       " 'eval_steps_per_second': 0.039,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f0ea5e10",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 128\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 15:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.786300</td>\n",
       "      <td>0.735548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.675856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=0.7423961162567139, metrics={'train_runtime': 980.2575, 'train_samples_per_second': 0.261, 'train_steps_per_second': 0.016, 'total_flos': 1337172889436160.0, 'train_loss': 0.7423961162567139, 'epoch': 2.0})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  index</th><th>value  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">      0</td><td>A      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">      1</td><td>B      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">      2</td><td>C      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">      3</td><td>B      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[Element(index=0, value='A'), Element(index=1, value='B'), Element(index=2, value='C'), Element(index=3, value='B')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['C']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['b']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['C', 'A', 'B', 'B']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['B', 'B', 'C', 'A']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  index</th><th>value  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">      0</td><td>A      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">      1</td><td>A      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">      2</td><td>C      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">      3</td><td>B      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[Element(index=0, value='A'), Element(index=1, value='A'), Element(index=2, value='C'), Element(index=3, value='B')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'B', 'C', 'C']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['B']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['B']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3e124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
