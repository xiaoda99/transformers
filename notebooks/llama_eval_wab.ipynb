{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "import random as rnd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In utils/hub.py: default_cache_path: /home/wab/.cache/huggingface/hub->/nas/xd/.cache/torch/transformers/\n",
      "In const.py: Loading tokenizer ... done 0:00:00.110961\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/config.json\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-config.json -> /nas/xd/.cache/torch/transformers/fe10cc433491a8058e42e13996d5a2c8d7d8eb336d4c10686b80e1118839e1fa.f4f76b6689b6a63600ca3396ba861970f33ce58d299ff2d8ed9a88b8707d0db8 not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model.bin.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ce4781e85e4eca9623a38c1b285515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00001-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00001-of-00007.bin -> /nas/xd/.cache/torch/transformers/128272c39b1758216e9805d4c2b06364b7c6be3a11f3fc1b4a9a2768ce162ffc.dfd680ccc93f8b148a8bf33ed51ff6fac6cb71923b73b31524618794f53ba454 not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00002-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00002-of-00007.bin -> /nas/xd/.cache/torch/transformers/c64b09a11187fcf762bf950545b89c014062653c7cb0522b025bfd6422ef0a05.2b40a8e63ebb8cd2bf4dcdc5b03620936b95e6384786882858b44e0c24ecaa23 not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00003-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00003-of-00007.bin -> /nas/xd/.cache/torch/transformers/d6a8096565c38bca33ca7f47d510110fe235391e95d7e4647837567207d72a9e.860ef472b6d4598bdec51bcaf1926c05f9c819be49033e6c72bb6d773cb02c5c not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00004-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00004-of-00007.bin -> /nas/xd/.cache/torch/transformers/87d5eeada28829969da644b31375156af76074bb5d8c4d9b9244c003c4f58cfa.dc77c23fac263115672c0c80b89477a2f9f098d559e91b8121e9ef9cc863d718 not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00005-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00005-of-00007.bin -> /nas/xd/.cache/torch/transformers/ac2e15aed16413cdf8574756eee7a6af8317cac229adfe1cb7d836840388c90b.fd14ef92bd398d6fd85a773c59bb6183ba108039a822f081ddb80bcdc567ce48 not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00006-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00006-of-00007.bin -> /nas/xd/.cache/torch/transformers/76a8f264b94b1ed3aa7e833c3e13f0c5662ab0a2acf151ade33b64b06f0d2931.23c80d15a782619db64304f464780d71cbef07ddd212aef0a186362fa538c14c not exist!\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/pytorch_model-00007-of-00007.bin\n",
      "/nas/xd/.cache/torch/transformers/vicuna-33b-v1.3-pytorch_model-00007-of-00007.bin -> /nas/xd/.cache/torch/transformers/de0115c3edda7e585966cdbdc08acb956624edfd4aaa0681d04f31a2babea1a9.136db224fa42855f6dd5dff22a5ad70f9a98905c803fcc5f9ce2dda320c83ce6 not exist!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2096e3903d504e5dbb157a2eee9db66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/generation_config.json\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/tokenizer.model\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/special_tokens_map.json\n",
      "In huggingface_hub.file_download.cached_download: url = https://huggingface.co/lmsys/vicuna-33b-v1.3/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/nas/xd/.cache/torch/transformers/'\n",
    "model_name = 'lmsys/vicuna-33b-v1.3'\n",
    "# model_name = '/nas2/xd/data/models/llama_hf/13B'\n",
    "_model_name = 'vicuna'  # model_name.split('/')[-1]\n",
    "device = [2,3]\n",
    "# device_map = {'model': device[0], 'lm_head': device[0]}\n",
    "device_map = get_device_map(devices=device, **name2mapping['vicuna'])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map, load_in_8bit=True,\n",
    "                                             cache_dir=cache_dir, low_cpu_mem_usage=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集piqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset piqa (/home/wab/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de168d290904812a58aa433bc429b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"/nas/wab/lm-evaluation-harness/llama_eval/datasets/piqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': 'S:The city councilmen refused the demonstrators a permit because they feared violence. Q: In the previous statement, does \"they\" refer to the city councilmen or the demonstrators? A: the',\n",
       " 'target_true': 'city councilmen',\n",
       " 'target_false': 'demonstrators'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_docs(dataset):\n",
    "    return map(_process_doc, dataset[\"validation\"])\n",
    "\n",
    "def _process_doc(doc):\n",
    "    out_doc = {\n",
    "        \"goal\": doc[\"goal\"],\n",
    "        \"choices\": [doc[\"sol1\"], doc[\"sol2\"]],\n",
    "        \"gold\": doc[\"label\"],\n",
    "    }\n",
    "    return out_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = torch.tensor([[ 1169, 16383,  1595,   470,  4197,   656,   262, 45391,   780,   262,\n",
    "         16383,   318,  1588,   198]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1169, 16383,  1595,   470,  4197,   656,   262, 45391,   780,   262,\n",
       "         16383,   318,  1588,   198]], device='cuda:3')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, continuation_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key, _, _), logits, inp, inplen, cont_toks in zip(\n",
    "        requests, multi_logits, inps, inplens, cont_toks_list\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "        if (cache_key[1]=='Yes' and bool(final_token[1939]<final_token[3869]))or(cache_key[1]=='No' and bool(final_token[1939]>final_token[3869])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (float(final_token[3869]),float(final_token[1939]), bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(doc):\n",
    "    return \"Question: \" + doc[\"goal\"]\n",
    "def construct_requests(doc, ctx):\n",
    "    lls = []\n",
    "    for index,choice in enumerate(doc[\"choices\"]):\n",
    "        input = ctx + \" {}\".format(choice) + ' Is this correct? Reply with \"Yes\" or \"No\". Answer:'\n",
    "        if index == doc['gold']:\n",
    "            target = \"Yes\"\n",
    "        else:\n",
    "            target = \"No\"\n",
    "        lls.append([loglikelihood(input,target)[0]])\n",
    "    return lls\n",
    "def loglikelihood(input,target):\n",
    "    new_reqs = []\n",
    "    for context, continuation in zip([input],[target]):\n",
    "        # if context == \"\":\n",
    "        #     # end of text as context\n",
    "        #     context_enc, continuation_enc = [self.eot_token_id], self.tok_encode(\n",
    "        #         continuation\n",
    "        #     )\n",
    "        # else:\n",
    "        context_enc, continuation_enc = _encode_pair(context, continuation)\n",
    "        new_reqs.append(((context, continuation), context_enc, continuation_enc))\n",
    "    return _loglikelihood_tokens(new_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(string: str):\n",
    "    return tokenizer.encode(string, add_special_tokens=False)\n",
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc\n",
    "def _model_call(inps):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits returned from the model\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return model(inps)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1838/1838 [18:22<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "task_doc_func = validation_docs(dataset)\n",
    "task_docs = list(task_doc_func)\n",
    "docs = {}\n",
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    docs[doc_id] = doc\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests(doc, ctx)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_back = []\n",
    "for req in reqs_all:\n",
    "    for choi in req:\n",
    "        if not choi[0][2]:\n",
    "            ans =\"F\"\n",
    "        else:\n",
    "            ans =\"T\"\n",
    "        ans_back.append(ans)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_doc_func = validation_docs(dataset)\n",
    "task_docs = list(task_doc_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = \"Question: \" + task_docs[0][\"goal\"] + \" {}\".format(task_docs[0][\"choices\"][0]) + ' Is this correct? Reply with \"yes\" or \"no\". Answer:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_task = tok_encode(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ans_back' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ans_back\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ans_back' is not defined"
     ]
    }
   ],
   "source": [
    "ans_back.count(\"T\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counterfact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/wab/.cache/huggingface/datasets/azhx___parquet/azhx--counterfact-d38424b990fb5450/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c031ce7be51a4dbbab68682a2a37f7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter_fact = load_dataset(\"azhx/counterfact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_fact['test'][0]['requested_rewrite']['target_true']['str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_docs(dataset):\n",
    "    return map(_process_doc, dataset[\"test\"])\n",
    "\n",
    "def _process_doc(doc):\n",
    "    out_doc = {\n",
    "        \"goal\": doc['requested_rewrite']['prompt'].replace(\"{}\", doc['requested_rewrite'][\"subject\"]),\n",
    "        \"target_true\": doc['requested_rewrite']['target_true']['str'], \n",
    "        \"target_new\":doc['requested_rewrite']['target_new']['str'],\n",
    "    }\n",
    "    return out_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(string: str):\n",
    "    return tokenizer.encode(string, add_special_tokens=True)\n",
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc\n",
    "def _model_call(inps):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits returned from the model\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return model(inps)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(doc):\n",
    "    return doc[\"goal\"]\n",
    "def construct_requests(doc, ctx):\n",
    "    lls = []\n",
    "    input = ctx \n",
    "    lls.append([loglikelihood(input,doc)[0]])\n",
    "    return lls\n",
    "def loglikelihood(input,doc):\n",
    "    new_reqs = []\n",
    "    context_enc = tok_encode(input)\n",
    "    true_enc,new_enc = tok_encode(doc[\"target_true\"]),tok_encode(doc[\"target_new\"])\n",
    "    new_reqs.append(((input,doc[\"target_true\"],doc[\"target_new\"] ), context_enc, true_enc,new_enc))\n",
    "    return _loglikelihood_tokens(new_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    # cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, true_enc, new_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        # assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        # cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        # cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key,context_enc,true_enc, new_enc), logits, inp, inplen in zip(\n",
    "        requests, multi_logits, inps, inplens\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "\n",
    "        if (bool(final_token[true_enc[0]]>final_token[new_enc[0]])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        # cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "        #     0\n",
    "        # )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        # logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "        #     -1\n",
    "        # )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (float(final_token[true_enc[0]]),float(final_token[new_enc[0]]), bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [10:29<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "task_doc_func = validation_docs(counter_fact)\n",
    "task_docs = list(task_doc_func)\n",
    "docs = {}\n",
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    docs[doc_id] = doc\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests(doc, ctx)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs_all[0][0][0][2]\n",
    "result = []\n",
    "for re in reqs_all:\n",
    "    if re[0][0][2]:\n",
    "        result.append(\"T\")\n",
    "    else:\n",
    "        result.append(\"F\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_doc_func = validation_docs(winograd)\n",
    "task_docs = list(task_doc_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_task = tok_encode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'city councilmen'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[0][\"target_true\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fewshotex = rnd.sample(task_docs, 2 + 1)\n",
    "fewshotex = [x for x in fewshotex if x != doc][:2]\n",
    "# fewshotex = task_docs[:2]\n",
    "labeled_examples = (\"\\n\\n\".join([doc_to_text(d) + doc_to_target(d) for d in fewshotex ])+ \"\\n\\n\" )\n",
    "# labeled_examples = \"Sam took French classes from Adam, because he was eager to speak it fluently. Question: In the previous sentence, what does the pronoun 'he' refer to? Answer: the pronoun 'he' refers to Sam\\n\\nDan took the rear seat while Bill claimed the front because his 'Dibs!' was slow. Question: In the previous sentence, what does the pronoun 'his' refer to? Answer: the pronoun 'his' refers to Dan\\n\\n\"\n",
    "input = labeled_examples + task_docs[4][\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f63aa88f510>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁Susan': 0.997,\n",
       " '▁Joan': 0.003,\n",
       " '</s>': 0.0,\n",
       " '▁both': 0.0,\n",
       " '▁Sus': 0.0,\n",
       " '▁Sue': 0.0,\n",
       " 'S': 0.0,\n",
       " '▁sus': 0.0,\n",
       " '<0x0A>': 0.0,\n",
       " '▁Sarah': 0.0}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = task_docs[99][\"goal\"]; print(text)\n",
    "token_ans = tok_encode(input)\n",
    "inp = torch.tensor(token_ans, dtype=torch.long).to(3)\n",
    "with torch.no_grad(): logits = model(inp.unsqueeze(0))[0]\n",
    "multi_logits = logits.softmax(dim=-1).cpu().float()\n",
    "show_topk(*multi_logits[0,-1].topk(k=10), indices_fn=tokenizer.convert_ids_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numpy(a, decimals=None):\n",
    "    v = np.array(a) if isinstance(a, list) else a.detach().cpu().numpy()\n",
    "    if decimals is not None: v = v.round(decimals)\n",
    "    return v\n",
    "\n",
    "def show_topk(values, indices, values_fn=lambda x: numpy(x, decimals=3), indices_fn=None, transpose=False):\n",
    "    if indices_fn is None:\n",
    "        indices_fn = show_topk.indices_fn if getattr(show_topk, 'indices_fn', None) is not None else lambda x: x\n",
    "    return dict(OrderedDict(zip(indices_fn(indices), values_fn(values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': 'Joan made sure to thank Susan for all the help she had recieved.',\n",
       " 'pronoun': 'she',\n",
       " 'target_true': 'Joan',\n",
       " 'target_false': 'Susan'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "context =task_docs[4][\"goal\"].split(task_docs[4]['pronoun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joan made sure to thank Susan for all the help she had recieved.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = doc_to_text(task_docs[5])\n",
    "reqs,input_true,input_false = construct_requests_panduan(task_docs[5], ctx,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "context,continuation = task_docs[0][\"goal\"].split(task_docs[0]['pronoun'])[0],task_docs[0]['target_true'].lower()+task_docs[0][\"goal\"].split(task_docs[0]['pronoun'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "context,continuation = input_true,\"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Does the above sentence conform to the common sense logic of adults? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Does the above sentence conform to the common sense logic of adults? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Does the above sentence conform to the common sense logic of adults? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Does the above sentence conform to the common sense logic of adults? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help susan had given. Does the above sentence conform to the common sense logic of adults? Answer:\",\n",
       " 'Yes')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context,continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_enc,continuation_enc = _encode_pair(context,continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8241]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuation_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(\n",
    "    (context_enc + continuation_enc)[:-1],\n",
    "    dtype=torch.long,\n",
    ").to(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([189])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): logits = model(inp.unsqueeze(0))[0]\n",
    "multi_logits = logits.softmax(dim=-1).cpu().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multi_logits[0,-1].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3098), tensor(0.6875))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[-1][1939],logits[-1][3869]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁the'"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_toks = torch.tensor(continuation_enc, dtype=torch.long).unsqueeze(\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[278, 4272, 18701, 1527, 1238, 1965, 21448, 29889]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuation_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_a = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  278],\n",
       "         [ 4272],\n",
       "         [18701],\n",
       "         [ 1527],\n",
       "         [ 1238],\n",
       "         [ 1965],\n",
       "         [21448],\n",
       "         [29889]]])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_toks.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3672e+00, -3.1270e+00, -3.4512e+00, -1.0664e+00, -2.3242e+00,\n",
       "         -3.2306e-05, -8.2275e-01, -4.5166e-01]])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = float(logits_a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.610383987426758"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset winograd_wsc (/home/wab/.cache/huggingface/datasets/winograd_wsc/wsc285/0.0.0/0651311f3b6dda14889d9a063030a02458395ee50ab9f41cca4cd5a89c0c3dce)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c3febb342d417bace555def74cc8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "winograd = load_dataset(\"winograd_wsc\",'wsc285')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city councilmen refused the demonstrators a permit because they feared violence.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winograd['test']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_docs(dataset):\n",
    "    return map(_process_doc, dataset[\"test\"])\n",
    "\n",
    "def _process_doc(doc):\n",
    "    op = doc['options']\n",
    "    target_true = op[doc['label']]\n",
    "#     op.pop(doc['label'])\n",
    "#     target_false = op[0]\n",
    "    target_false = list(set(doc['options']).difference(set([op[doc['label']]])))[0]\n",
    "    true_list,false_list = target_true.split(' '),target_false.split(' ')\n",
    "    max_lenth = 0\n",
    "    for i in range(min(len(true_list),len(false_list))):\n",
    "        if true_list[i] == false_list[i]:\n",
    "            max_lenth+=1\n",
    "        else:\n",
    "            break\n",
    "    input = doc['text']\n",
    "    # input = doc['text'] + \" In the previous sentence, the pronoun '{}' refers to\".format(doc['pronoun']) + \" {}\".format(' '.join(true_list[:max_lenth]))\n",
    "#     input = doc['text'] + \" Question: In the previous sentence, what does the pronoun '{}' refer to? Answer: the pronoun '{}' refers to\".format(doc['pronoun'], doc['pronoun']) + \" {}\".format(\" \".join(true_list[:max_lenth]).lower())\n",
    "#     input = f'S:{doc[\"text\"]} Q: In the previous statement, does \"{doc[\"pronoun\"]}\" refer to {doc[\"options\"][0].lower()} or {doc[\"options\"][1].lower()}? A: {\" \".join(true_list[:max_lenth]).lower()}'\n",
    "    out_doc = {\n",
    "        \"goal\":  input.rstrip() ,\n",
    "#         \"target_true\": ' '.join(true_list[max_lenth:]), \n",
    "#         \"target_false\": ' '.join(false_list[max_lenth:]), \n",
    "        \"pronoun\":doc['pronoun'],\n",
    "        \"target_true\": ' '.join(true_list), \n",
    "        \"target_false\": ' '.join(false_list), \n",
    "    }\n",
    "    return out_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(string: str):\n",
    "    return tokenizer.encode(string, add_special_tokens=False)\n",
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc\n",
    "def _model_call(inps):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits returned from the model\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return model(inps)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(doc):\n",
    "    return doc[\"goal\"]\n",
    "def doc_to_target(doc):\n",
    "    return \" \" + doc['target_true']\n",
    "def construct_requests(doc, ctx,num_shot):\n",
    "    lls = []\n",
    "#     init_prompt_starter = \"The following are pairs of Winograd Schema in the form of a statement S, a question Q, and an answer A:\"\n",
    "#     init_prompt1 = \"S: The cat went through the door, but it's tail got stuck. Q: In the previous statement, what does 'it' refer to? A: The cat.\"\n",
    "#     init_prompt2 = \"S: The cat tried to go through the door, but it was too small. Q: In the previous statement, what does 'it' refer to? A: The door.\"\n",
    "#     init_prompt3 = \"S: Fedex made more profit than UPS last year, but that was mostly due to the success of the new delivery system they implemented. Q: In the previous statement, what does 'they' refer to? A: Fedex.\"\n",
    "#     init_prompt4 = \"S: Sam tried to buy Xerxes lunch, but he wouldn't allow it. Q: In the previous statement, who does 'he' refer to? A: Xerxes.\"\n",
    "    \n",
    "#     fewshotex = rnd.sample(task_docs, num_shot + 1)\n",
    "#     fewshotex = [x for x in fewshotex if x != doc][:num_shot]\n",
    "# #     # fewshotex = task_docs[:2]\n",
    "#     labeled_examples = (\"\\n\\n\".join([doc_to_text(d) + doc_to_target(d) for d in fewshotex ])+ \"\\n\\n\" )\n",
    "#     input = labeled_examples + ctx \n",
    "#     input =f\"{init_prompt_starter}\\n\\n{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt3}\\n\\n{init_prompt4}\\n\\n{ctx}\"\n",
    "    input = ctx\n",
    "    print(input)\n",
    "    lls.append([loglikelihood(input,doc)[0]])\n",
    "    return lls\n",
    "def construct_requests_panduan(doc, ctx,num_shot):\n",
    "    lls = []\n",
    "    init_prompt1 = \"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\"\n",
    "    init_prompt2 = \"The trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\"\n",
    "    init_prompt3 = \"The city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\"\n",
    "    init_prompt4 = \"The city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\"\n",
    "    ctx_split = ctx.split(' ')\n",
    "    def conti(con_list,target,pronoun):\n",
    "        return ' '.join([i if i != pronoun else target for i in con_list])\n",
    "    input_true =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt4}\\n\\n{init_prompt3}\\n\\n{conti(ctx_split,doc['target_true'].lower(),doc['pronoun'])+' Is the setence above plausible? Answer:'}\"\n",
    "    input_false =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt4}\\n\\n{init_prompt3}\\n\\n{conti(ctx_split,doc['target_false'].lower(),doc['pronoun'])+' Is the setence above plausible? Answer:'}\"\n",
    "#     print(input_true)\n",
    "#     print(input_false)\n",
    "    lls.append([loglikelihood_panduan(input_true,\"Yes\")[0]])\n",
    "    lls.append([loglikelihood_panduan(input_false,\"No\")[0]])\n",
    "    return lls,input_true,input_false\n",
    "def construct_requests_conti(doc, ctx,num_shot):\n",
    "    lls = []\n",
    "#     init_prompt_starter = \"The following are pairs of Winograd Schema in the form of a statement S, a question Q, and an answer A:\"\n",
    "#     init_prompt1 = \"S: The cat went through the door, but it's tail got stuck. Q: In the previous statement, what does 'it' refer to? A: The cat.\"\n",
    "#     init_prompt2 = \"S: The cat tried to go through the door, but it was too small. Q: In the previous statement, what does 'it' refer to? A: The door.\"\n",
    "#     init_prompt3 = \"S: Fedex made more profit than UPS last year, but that was mostly due to the success of the new delivery system they implemented. Q: In the previous statement, what does 'they' refer to? A: Fedex.\"\n",
    "#     init_prompt4 = \"S: Sam tried to buy Xerxes lunch, but he wouldn't allow it. Q: In the previous statement, who does 'he' refer to? A: Xerxes.\"\n",
    "    \n",
    "#     fewshotex = rnd.sample(task_docs, num_shot + 1)\n",
    "#     fewshotex = [x for x in fewshotex if x != doc][:num_shot]\n",
    "#     # fewshotex = task_docs[:2]\n",
    "#     labeled_examples = (\"\\n\\n\".join([doc_to_text(d) + doc_to_target(d) for d in fewshotex ])+ \"\\n\\n\" )\n",
    "#     context,continuation = ctx.split(\" \"+doc['pronoun']+\" \")\n",
    "#     first_index = ctx.find(\" \"+doc['pronoun']+\" \")\n",
    "#     context = ctx[:first_index]\n",
    "#     contiunation = ctx[first_index:]\n",
    "#     input =f\"{init_prompt_starter}\\n\\n{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt3}\\n\\n{init_prompt4}\\n\\n{ctx}\"\n",
    "    # input = ctx\n",
    "#     print(context)\n",
    "#     print(continuation)\n",
    "#     print(context+continuation.replace(\" \"+doc['pronoun']+\" \",\" \"+doc[\"target_true\"].lower()+\" \"))\n",
    "    first_index = 0\n",
    "    ctx_split = ctx.split(' ')\n",
    "    for index,tok in enumerate(ctx_split):\n",
    "        if tok == doc['pronoun'] and first_index ==0:\n",
    "            first_index = index\n",
    "    context = ' '.join(ctx_split[:first_index])\n",
    "    def conti(con_list,target,pronoun):\n",
    "        return ' '+' '.join([i if i != pronoun else target for i in con_list])\n",
    "                \n",
    "    print(context)\n",
    "    print(conti(ctx_split,doc[\"target_true\"].lower(),doc['pronoun']))\n",
    "    print(conti(ctx_split,doc[\"target_false\"].lower(),doc['pronoun']))\n",
    "    lls.append(loglikelihood_conti(context,conti(ctx_split[first_index:],doc[\"target_true\"].lower(),doc['pronoun']),doc)[0])\n",
    "    lls.append(loglikelihood_conti(context,conti(ctx_split[first_index:],doc[\"target_false\"].lower(),doc['pronoun']),doc)[0])\n",
    "    return lls\n",
    "def loglikelihood(input,doc):\n",
    "    new_reqs = []\n",
    "    context_enc = tok_encode(input)\n",
    "    true_enc,new_enc = tok_encode(doc[\"target_true\"]),tok_encode(doc[\"target_false\"])\n",
    "    new_reqs.append(((input,doc[\"target_true\"],doc[\"target_false\"] ), context_enc, true_enc,new_enc))\n",
    "    return _loglikelihood_tokens(new_reqs)\n",
    "def loglikelihood_conti(context,continuation,doc):\n",
    "    new_reqs = []\n",
    "    context_enc,continuation_enc = _encode_pair(context,continuation)\n",
    "#     true_enc,new_enc = tok_encode(doc[\"target_true\"]),tok_encode(doc[\"target_false\"])\n",
    "    new_reqs.append(((context,continuation), context_enc, continuation_enc))\n",
    "#     print(new_reqs)\n",
    "    return _loglikelihood_tokens_conti(new_reqs)\n",
    "def loglikelihood_panduan(input,target):\n",
    "    new_reqs = []\n",
    "    for context, continuation in zip([input],[target]):\n",
    "        # if context == \"\":\n",
    "        #     # end of text as context\n",
    "        #     context_enc, continuation_enc = [self.eot_token_id], self.tok_encode(\n",
    "        #         continuation\n",
    "        #     )\n",
    "        # else:\n",
    "        context_enc, continuation_enc = _encode_pair(context, continuation)\n",
    "        new_reqs.append(((context, continuation), context_enc, continuation_enc))\n",
    "    return _loglikelihood_tokens_panduan(new_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    # cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, true_enc, new_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        # assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        # cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        # cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key,context_enc,true_enc, new_enc), logits, inp, inplen in zip(\n",
    "        requests, multi_logits, inps, inplens\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "\n",
    "        if (bool(final_token[true_enc[0]]>final_token[new_enc[0]])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        # cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "        #     0\n",
    "        # )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        # logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "        #     -1\n",
    "        # )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (float(final_token[true_enc[0]]),float(final_token[new_enc[0]]), bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens_conti(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    # cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, continuation_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc+continuation_enc)[:-1],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "#         print(inp)\n",
    "        # cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        # cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.log_softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key,context_enc,continuation_enc), logits, inp, inplen in zip(\n",
    "        requests, multi_logits, inps, inplens\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = len(continuation_enc)\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        cont_toks = torch.tensor(continuation_enc, dtype=torch.long).unsqueeze(\n",
    "            0\n",
    "        )\n",
    "        logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        ) \n",
    "#         final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "\n",
    "#         if (bool(final_token[true_enc[0]]>final_token[new_enc[0]])) :\n",
    "#             max_equal = True\n",
    "#         else:\n",
    "#             max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        # cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "        #     0\n",
    "        # )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        # logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "        #     -1\n",
    "        # )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "#         answer = (float(final_token[true_enc[0]]),float(final_token[new_enc[0]]), bool(max_equal))\n",
    "        answer = (float(logits.sum()))\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens_panduan(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, continuation_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key, _, _), logits, inp, inplen, cont_toks in zip(\n",
    "        requests, multi_logits, inps, inplens, cont_toks_list\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "        if (cache_key[1]=='Yes' and bool(final_token[1939]<final_token[3869]))or(cache_key[1]=='No' and bool(final_token[1939]>final_token[3869])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (cache_key[0],cache_key[1],float(final_token[3869]),float(final_token[1939]), bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "#         print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_doc_func = validation_docs(winograd)\n",
    "task_docs = list(task_doc_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': 'The city councilmen refused the demonstrators a permit because they feared violence.',\n",
       " 'pronoun': 'they',\n",
       " 'target_true': 'The city councilmen',\n",
       " 'target_false': 'The demonstrators'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is the setence above plausible? Answer: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt1 = \"The trophy doesn't fit into the brown suitcase because the trophy is too large.Is the setence above plausible? Answer: Yes\"\n",
    "init_prompt2 = \"The trophy doesn't fit into the brown suitcase because the trophy is too small.Is the setence above plausible? Answer: No\"\n",
    "init_prompt3 = \"The city councilmen refused the demonstrators a permit because the city councilmen feared violence.Is the setence above plausible? Answer: Yes\"\n",
    "init_prompt4 = \"The city councilmen refused the demonstrators a permit because the demonstrators feared violence.Is the setence above plausible? Answer: No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conti(con_list,target,pronoun):\n",
    "    return ' '.join([i if i != pronoun else target for i in con_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [02:12<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "task_doc_func = validation_docs(winograd)\n",
    "task_docs = list(task_doc_func)\n",
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests(doc, ctx,3)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[0][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m result\u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m req \u001b[38;5;129;01min\u001b[39;00m reqs_all:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m req[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m      4\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "result= []\n",
    "for req in reqs_all:\n",
    "    if req[0][0][2]:\n",
    "        result.append(\"T\")\n",
    "    else:\n",
    "        result.append(\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city councilmen refused the demonstrators a permit because they feared violence.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ctx \u001b[38;5;241m=\u001b[39m doc_to_text(task_docs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m reqs \u001b[38;5;241m=\u001b[39m construct_requests(task_docs[\u001b[38;5;241m0\u001b[39m], ctx,\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m, in \u001b[0;36mconstruct_requests\u001b[0;34m(doc, ctx, num_shot)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ctx\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m lls\u001b[38;5;241m.\u001b[39mappend([loglikelihood(\u001b[38;5;28minput\u001b[39m,doc)[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lls\n",
      "Cell \u001b[0;32mIn[26], line 77\u001b[0m, in \u001b[0;36mloglikelihood\u001b[0;34m(input, doc)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloglikelihood\u001b[39m(\u001b[38;5;28minput\u001b[39m,doc):\n\u001b[1;32m     76\u001b[0m     new_reqs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 77\u001b[0m     context_enc \u001b[38;5;241m=\u001b[39m tok_encode(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     78\u001b[0m     true_enc,new_enc \u001b[38;5;241m=\u001b[39m tok_encode(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_true\u001b[39m\u001b[38;5;124m\"\u001b[39m]),tok_encode(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_false\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     79\u001b[0m     new_reqs\u001b[38;5;241m.\u001b[39mappend(((\u001b[38;5;28minput\u001b[39m,doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_true\u001b[39m\u001b[38;5;124m\"\u001b[39m],doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_false\u001b[39m\u001b[38;5;124m\"\u001b[39m] ), context_enc, true_enc,new_enc))\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mtok_encode\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtok_encode\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(string, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "ctx = doc_to_text(task_docs[0])\n",
    "reqs = construct_requests(task_docs[0], ctx,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6175438596491228"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "176/285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help susan had given. Is the setence above plausible? Answer:\",\n",
       "   'Yes',\n",
       "   0.802734375,\n",
       "   0.193603515625,\n",
       "   True)],\n",
       " [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help joan had given. Is the setence above plausible? Answer:\",\n",
       "   'No',\n",
       "   0.04803466796875,\n",
       "   0.9501953125,\n",
       "   True)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = doc_to_text(task_docs[5])\n",
    "reqs = construct_requests_panduan(task_docs[5], ctx,3)\n",
    "reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4-shot\n",
    "2:代词位不变，句子变\n",
    "2：句子不变，代词位变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city councilmen refused the demonstrators a permit because\n",
      " The city councilmen refused the demonstrators a permit because the city councilmen feared violence.\n",
      " The city councilmen refused the demonstrators a permit because the demonstrators feared violence.\n"
     ]
    }
   ],
   "source": [
    "ctx = doc_to_text(task_docs[0])\n",
    "reqs = construct_requests_conti(task_docs[0], ctx,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-14.609375, -15.9765625]"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = doc_to_text(task_docs[0])\n",
    "# context,continuation = ctx.split(\" \"+doc['pronoun']+\" \")\n",
    "first_index = ctx.find(\" \"+task_docs[0]['pronoun']+\" \")\n",
    "context = ctx[:first_index]\n",
    "contiunation = ctx[first_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [04:20<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "task_doc_func = validation_docs(winograd)\n",
    "task_docs = list(task_doc_func)\n",
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests_panduan(doc, ctx,3)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_back = []\n",
    "for req in reqs_all:\n",
    "    ans = \"T\"\n",
    "    for choi in req:\n",
    "        if not choi[0][4]:\n",
    "            ans =\"F\"\n",
    "    ans_back.append(ans)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_back = []\n",
    "for req in reqs_all:\n",
    "    for choi in req:\n",
    "        if not choi[0][4]:\n",
    "            ans =\"F\"\n",
    "        else:\n",
    "            ans =\"T\"\n",
    "        ans_back.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.93505859375,\n",
       "    0.052764892578125,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.219482421875,\n",
       "    0.76611328125,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators advocated violence. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.014923095703125,\n",
       "    0.98291015625,\n",
       "    False)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen advocated violence. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    5.251169204711914e-05,\n",
       "    0.99755859375,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.97119140625,\n",
       "    0.02587890625,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the suitcase is too large. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.64697265625,\n",
       "    0.351806640625,\n",
       "    False)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the suitcase is too small. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.984375,\n",
       "    0.01494598388671875,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.056427001953125,\n",
       "    0.939453125,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help joan had recieved. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.12066650390625,\n",
       "    0.8779296875,\n",
       "    False)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help susan had recieved. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.068359375,\n",
       "    0.92919921875,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help susan had given. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.802734375,\n",
       "    0.193603515625,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nJoan made sure to thank Susan for all the help joan had given. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.04803466796875,\n",
       "    0.9501953125,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nPaul tried to call George on the phone, but paul wasn't successful. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.99658203125,\n",
       "    0.0018358230590820312,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nPaul tried to call George on the phone, but george wasn't successful. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.12359619140625,\n",
       "    0.87158203125,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nPaul tried to call George on the phone, but george wasn't available. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.998046875,\n",
       "    0.0001251697540283203,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nPaul tried to call George on the phone, but paul wasn't available. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.3544921875,\n",
       "    0.6416015625,\n",
       "    True)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe lawyer asked the witness a question, but the lawyer was reluctant to repeat it. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.7060546875,\n",
       "    0.272216796875,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe lawyer asked the witness a question, but the witness was reluctant to repeat it. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.96337890625,\n",
       "    0.0150909423828125,\n",
       "    False)]],\n",
       " [[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe lawyer asked the witness a question, but the witness was reluctant to answer it. Is the setence above plausible? Answer:\",\n",
       "    'Yes',\n",
       "    0.9990234375,\n",
       "    0.0001232624053955078,\n",
       "    True)],\n",
       "  [(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe lawyer asked the witness a question, but the lawyer was reluctant to answer it. Is the setence above plausible? Answer:\",\n",
       "    'No',\n",
       "    0.921875,\n",
       "    0.07568359375,\n",
       "    False)]]]"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\\n\\nThe trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\\n\\nThe city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\\n\\nThe city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer:\",\n",
       "  'No',\n",
       "  0.219482421875,\n",
       "  0.76611328125,\n",
       "  True)]"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in reqs_all:\n",
    "    for cho in i:\n",
    "        all_data.append([cho[0][0].split(\"\\n\\n\")[-1]]+list(cho[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/xd/projects/transformers/notebooks'"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cho[0][0].split(\"\\n\\n\")[-1],list(cho[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer:',\n",
       " 'No',\n",
       " 0.219482421875,\n",
       " 0.76611328125,\n",
       " True]"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0] ['./pptree', '/raid/xd/projects/transformers/notebooks', '/raid/xd/miniconda3/envs/tune/lib/python311.zip', '/raid/xd/miniconda3/envs/tune/lib/python3.11', '/raid/xd/miniconda3/envs/tune/lib/python3.11/lib-dynload', '', '/home/wab/.local/lib/python3.11/site-packages', '/raid/xd/miniconda3/envs/tune/lib/python3.11/site-packages', '/raid/xd/projects/huggingface_hub/src', '/raid/xd/projects/transformers/src', '/home/wab/.cache/huggingface/modules']\n",
      "0.27.8\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# openai.api_base = \"https://api.nekoai.top/v1\"\n",
    "openai.api_key = \"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\"\n",
    "import sys\n",
    "print(sys.version, sys.path)\n",
    "print(openai.__version__)\n",
    "# openai.api_base = None\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "#     engine=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"你的名字是什么？\",\n",
    "    max_tokens=10,\n",
    "    temperature=0.0001,\n",
    "    # temperature=1,\n",
    "    logprobs=10,\n",
    "    timeout=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7o7wDTLmlv7cYXDVuEvMLJKf1rJCH\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1692181269,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n\\u6211\\u7684\\u540d\\u5b57\\u662f\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"tokens\": [\n",
      "          \"\\n\",\n",
      "          \"\\n\",\n",
      "          \"bytes:\\\\xe6\\\\x88\",\n",
      "          \"bytes:\\\\x91\",\n",
      "          \"\\u7684\",\n",
      "          \"bytes:\\\\xe5\\\\x90\",\n",
      "          \"bytes:\\\\x8d\",\n",
      "          \"bytes:\\\\xe5\\\\xad\",\n",
      "          \"bytes:\\\\x97\",\n",
      "          \"\\u662f\"\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.0019322382,\n",
      "          -0.0034275567,\n",
      "          -0.0019454958,\n",
      "          0.0,\n",
      "          -0.16907983,\n",
      "          -8.7932596e-05,\n",
      "          0.0,\n",
      "          -1.3900239e-05,\n",
      "          0.0,\n",
      "          -0.19157492\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \"\\n\": -0.0019322382,\n",
      "            \" \": -7.2668304,\n",
      "            \"\\n\\n\": -7.6990128,\n",
      "            \"  \": -8.830442,\n",
      "            \"bytes:\\\\xe6\\\\x88\": -9.87835\n",
      "          },\n",
      "          {\n",
      "            \"\\n\": -0.0034275567,\n",
      "            \"bytes:\\\\xe6\\\\x88\": -6.4140797,\n",
      "            \"My\": -7.1304927,\n",
      "            \"bytes:\\\\xe4\\\\xbd\": -8.351685,\n",
      "            \"+\": -8.5814295\n",
      "          },\n",
      "          {\n",
      "            \"bytes:\\\\xe6\\\\x88\": -0.0019454958,\n",
      "            \"My\": -6.669097,\n",
      "            \"bytes:\\\\xe4\\\\xbd\": -7.7015457,\n",
      "            \"bytes:\\\\xe6\\\\x9d\": -10.495986,\n",
      "            \"bytes: \\\\xe6\": -10.590549\n",
      "          },\n",
      "          {\n",
      "            \"bytes:\\\\x91\": 0.0,\n",
      "            \"bytes:\\\\xb4\": -16.952053,\n",
      "            \"bytes:\\\\xbf\": -19.674982,\n",
      "            \"bytes:\\\\x91\\\\xe5\\\\xa3\\\\xab\": -19.730503,\n",
      "            \"bytes:\\\\x90\": -19.749815\n",
      "          },\n",
      "          {\n",
      "            \"\\u7684\": -0.16907983,\n",
      "            \"bytes:\\\\xe5\\\\x8f\": -1.8630837,\n",
      "            \"bytes:\\\\xe5\\\\x90\": -8.083384,\n",
      "            \"<|endoftext|>\": -11.352361,\n",
      "            \"\\u662f\": -11.437632\n",
      "          },\n",
      "          {\n",
      "            \"bytes:\\\\xe5\\\\x90\": -8.7932596e-05,\n",
      "            \"<|endoftext|>\": -9.934782,\n",
      "            \"bytes:\\\\xe5\\\\xa7\": -11.018477,\n",
      "            \"bytes: \\\\xe5\": -12.104929,\n",
      "            \"bytes:\\\\xe5\\\\x8f\": -12.388753\n",
      "          },\n",
      "          {\n",
      "            \"bytes:\\\\x8d\": 0.0,\n",
      "            \"bytes:\\\\x8c\": -20.611973,\n",
      "            \"<|endoftext|>\": -21.44675,\n",
      "            \"bytes:\\\\x8e\": -22.330997,\n",
      "            \"bytes:\\\\x88\": -23.346186\n",
      "          },\n",
      "          {\n",
      "            \"bytes:\\\\xe5\\\\xad\": -1.3900239e-05,\n",
      "            \"<|endoftext|>\": -11.617263,\n",
      "            \"\\u5b50\": -12.750421,\n",
      "            \"bytes:\\\\xe5\\\\x8f\": -14.062202,\n",
      "            \"bytes:\\\\xe7\": -14.143688\n",
      "          },\n",
      "          {\n",
      "            \"bytes:\\\\x97\": 0.0,\n",
      "            \"bytes:\\\\x99\": -26.211031,\n",
      "            \"<|endoftext|>\": -30.520115,\n",
      "            \"bytes:\\\\x98\\\\x85\": -31.185266,\n",
      "            \"bytes:\\\\x98\": -32.140488\n",
      "          },\n",
      "          {\n",
      "            \"\\u662f\": -0.19157492,\n",
      "            \"bytes:\\\\xe5\\\\x8f\": -1.7472504,\n",
      "            \"<|endoftext|>\": -10.224472,\n",
      "            \"bytes: \\\\xe6\": -11.092935,\n",
      "            \"bytes:\\\\xef\": -11.285038\n",
      "          }\n",
      "        ],\n",
      "        \"text_offset\": [\n",
      "          8,\n",
      "          9,\n",
      "          10,\n",
      "          10,\n",
      "          11,\n",
      "          12,\n",
      "          12,\n",
      "          13,\n",
      "          13,\n",
      "          14\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 15,\n",
      "    \"completion_tokens\": 10,\n",
      "    \"total_tokens\": 25\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_requests_gpt(doc, ctx,num_shot):\n",
    "#     init_prompt1 = \"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\"\n",
    "#     init_prompt2 = \"The trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\"\n",
    "#     init_prompt3 = \"The city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\"\n",
    "#     init_prompt4 = \"The city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\"\n",
    "    ctx_split = ctx.split(' ')\n",
    "    def conti(con_list,target,pronoun):\n",
    "        return ' '.join([i if i != pronoun else target for i in con_list])\n",
    "#     input_true =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt4}\\n\\n{init_prompt3}\\n\\n{conti(ctx_split,doc['target_true'].lower(),doc['pronoun'])+' Is the setence above plausible? Answer:'}\"\n",
    "#     input_false =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt4}\\n\\n{init_prompt3}\\n\\n{conti(ctx_split,doc['target_false'].lower(),doc['pronoun'])+' Is the setence above plausible? Answer:'}\"\n",
    "    input_true =f\"{conti(ctx_split,doc['target_true'].lower(),doc['pronoun'])+' Is the setence above plausible? Answer:'}\"\n",
    "    input_false =f\"{conti(ctx_split,doc['target_false'].lower(),doc['pronoun'])+' Is the setence above plausible? Answer:'}\"\n",
    "    return input_true,input_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = doc_to_text(task_docs[0])\n",
    "input_true,input_false= construct_requests_gpt(task_docs[0], ctx,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_false = openai.ChatCompletion.create(\n",
    "    # model=\"gpt-4\", \"gpt-3.5-turbo\"\n",
    "    # model=\"gpt-3.5-turbo\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I'm sure that my map will show this building; it is very good.\"}],\n",
    "    # temperature=0.0001,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    # n=3,\n",
    "    timeout=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_doc_func = validation_docs(winograd)\n",
    "task_docs = list(task_doc_func)\n",
    "reqs_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ctx = doc_to_text(task_docs[0])\n",
    "    input_true,input_false = construct_requests_gpt(task_docs[0], ctx,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    response_true = openai.ChatCompletion.create(\n",
    "        # model=\"gpt-4\", \"gpt-3.5-turbo\"\n",
    "        # model=\"gpt-3.5-turbo\",\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": input_true}],\n",
    "        # temperature=0.0001,\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000,\n",
    "        # n=3,\n",
    "        timeout=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7o7khLB8cC3mnsGd37uvfiGc4TkOk at 0x7f872c046e70> JSON: {\n",
       "  \"id\": \"chatcmpl-7o7khLB8cC3mnsGd37uvfiGc4TkOk\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1692180555,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Yes, the sentence is plausible. The city councilmen may refuse to grant a permit to demonstrators if they have concerns about potential violence or public safety issues.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 33,\n",
       "    \"completion_tokens\": 31,\n",
       "    \"total_tokens\": 64\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_true[\"cho\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [06:05<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    ctx = doc_to_text(doc)\n",
    "    input_true,input_false = construct_requests_gpt(doc, ctx,3)\n",
    "#     init_prompt1 = \"The trophy doesn't fit into the brown suitcase because the trophy is too large. Is the setence above plausible? Answer: Yes\"\n",
    "#     init_prompt2 = \"The trophy doesn't fit into the brown suitcase because the trophy is too small. Is the setence above plausible? Answer: No\"\n",
    "#     init_prompt3 = \"The city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer: Yes\"\n",
    "#     init_prompt4 = \"The city councilmen refused the demonstrators a permit because the demonstrators feared violence. Is the setence above plausible? Answer: No\"\n",
    "#     input_true =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt4}\\n\\n{init_prompt3}\\n\\n{input_true}\"\n",
    "#     input_false =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt4}\\n\\n{init_prompt3}\\n\\n{input_false}\"\n",
    "    \n",
    "    response_true = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "    #     engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=input_true,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.0001,\n",
    "        # temperature=1,\n",
    "        logprobs=10,\n",
    "        timeout=15,\n",
    "    )\n",
    "    response_false = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "    #     engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=input_false,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.0001,\n",
    "        # temperature=1,\n",
    "        logprobs=10,\n",
    "        timeout=15,\n",
    "    )\n",
    "    reqs_all.append([input_true,response_true[\"choices\"][0][\"text\"]])\n",
    "    reqs_all.append([input_false,response_false[\"choices\"][0][\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The city councilmen refused the demonstrators a permit because the city councilmen feared violence. Is the setence above plausible? Answer:',\n",
       " ' Yes, it is plausible.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data =[]\n",
    "for i  in reqs_all:\n",
    "    all_data.append([i[0].split(\"\\n\\n\")[-1],i[1][1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "all_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ\\x88\\x91æ\\x98¯AIï¼\\x8cæ²¡æ\\x9c\\x89å\\x85·ä½\\x93ç\\x9a\\x84å\\x90\\x8då\\xad\\x97'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "unicode.encode('utf-8').decode(\"unicode_escape\")\n",
    "# re = unicode.decode(\"unicode_escape\")\n",
    "# print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  将数据写入新文件\n",
    "def data_write(file_path, datas):\n",
    "    f = xlwt.Workbook()\n",
    "    sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True) #创建sheet\n",
    "    \n",
    "    #将数据写入第 i 行，第 j 列\n",
    "    i = 0\n",
    "    for data in datas:\n",
    "        for j in range(len(data)):\n",
    "            sheet1.write(i,j,data[0][j])\n",
    "        i = i + 1\n",
    "        \n",
    "    f.save(file_path) #保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = openpyxl.Workbook()\n",
    "worksheet = workbook.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in enumerate(reqs_all):\n",
    "    worksheet.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook.save(filename=\"/raid/xd/projects/transformers/notebooks/eval_llama_result/eval_gpt35_panduan.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_back.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3824561403508772"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "109/285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6789473684210526"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "387/570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8070175438596491"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "230/285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= []\n",
    "for req in reqs_all:\n",
    "    if req[0]>req[1]:\n",
    "        result.append(\"T\")\n",
    "    else:\n",
    "        result.append(\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[0][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T']"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': \"My meeting started at 4:00 and I needed to catch the train at 4:30, so there wasn't much time. Luckily, it was short, so it worked out.\",\n",
       " 'pronoun': 'it',\n",
       " 'target_true': 'The meeting',\n",
       " 'target_false': 'The train'}"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.603515625, 2.87890625]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for re in reqs_all:\n",
    "    if re[0][0][2]:\n",
    "        result.append(\"T\")\n",
    "    else:\n",
    "        result.append(\"F\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5795053003533569"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "164/283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_doc_func = validation_docs(winograd)\n",
    "task_docs = list(task_doc_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': \"The city councilmen refused the demonstrators a permit because they feared violence. In the previous sentence, the pronoun 'they' refers to The\",\n",
       " 'target_true': 'city councilmen',\n",
       " 'target_false': 'demonstrators'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "152/285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openbookqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset openbookqa/additional to /home/wab/.cache/huggingface/datasets/openbookqa/additional/1.0.1/f338ccacfbc86fb8c2de3aa1c06d2ce686933de3bca284dba97d32592c52b33f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d27c1d7e51492db61ed1eff9172d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0756402c3dfd431ca847ae73a72605ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a475e9afb44f3c91eb86be3449c845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset openbookqa downloaded and prepared to /home/wab/.cache/huggingface/datasets/openbookqa/additional/1.0.1/f338ccacfbc86fb8c2de3aa1c06d2ce686933de3bca284dba97d32592c52b33f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec1eec93a5845c2b9d505d7d8328236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openbookqa = load_dataset(\"openbookqa\",\"additional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question_stem', 'choices', 'answerKey', 'fact1', 'humanScore', 'clarity', 'turkIdAnonymized'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openbookqa['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_docs(dataset):\n",
    "    return map(_process_doc, dataset[\"validation\"])\n",
    "\n",
    "def _process_doc(doc):\n",
    "    out_doc = {\n",
    "        \"goal\": doc[\"question_stem\"],\n",
    "        \"choices\": doc['choices']['text'],\n",
    "        \"gold\": ord(doc[\"answerKey\"])-ord(\"A\"),\n",
    "        \"fact\": doc[\"fact1\"]\n",
    "    }\n",
    "    return out_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(doc):\n",
    "    return 'Based on the fact that \"{}\": '.format(doc['fact']) + doc[\"goal\"]\n",
    "def construct_requests(doc, ctx):\n",
    "    lls = []\n",
    "    for index,choice in enumerate(doc[\"choices\"]):\n",
    "        input = ctx + \" {}\".format(choice) + ' Is the statement above correct? Reply with \"Yes\" or \"No\". Answer:'\n",
    "        if index == doc['gold']:\n",
    "            target = \"Yes\"\n",
    "        else:\n",
    "            target = \"No\"\n",
    "        lls.append([loglikelihood(input,target)[0]])\n",
    "    return lls\n",
    "def loglikelihood(input,target):\n",
    "    new_reqs = []\n",
    "    for context, continuation in zip([input],[target]):\n",
    "        # if context == \"\":\n",
    "        #     # end of text as context\n",
    "        #     context_enc, continuation_enc = [self.eot_token_id], self.tok_encode(\n",
    "        #         continuation\n",
    "        #     )\n",
    "        # else:\n",
    "        context_enc, continuation_enc = _encode_pair(context, continuation)\n",
    "        new_reqs.append(((context, continuation), context_enc, continuation_enc))\n",
    "    return _loglikelihood_tokens(new_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, continuation_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key, _, _), logits, inp, inplen, cont_toks in zip(\n",
    "        requests, multi_logits, inps, inplens, cont_toks_list\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "        if (cache_key[1]=='Yes' and bool(final_token[1939]<final_token[3869]))or(cache_key[1]=='No' and bool(final_token[1939]>final_token[3869])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (float(final_token[3869]),float(final_token[1939]), bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(string: str):\n",
    "    return tokenizer.encode(string, add_special_tokens=False)\n",
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc\n",
    "def _model_call(inps):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits returned from the model\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return model(inps)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:02<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "task_doc_func = validation_docs(openbookqa)\n",
    "task_docs = list(task_doc_func)\n",
    "docs = {}\n",
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    docs[doc_id] = doc\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests(doc, ctx)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_back = []\n",
    "for req in reqs_all:\n",
    "    ans = \"T\"\n",
    "    for choi in req:\n",
    "        if not choi[0][2]:\n",
    "            ans =\"F\"\n",
    "    ans_back.append(ans)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_back = []\n",
    "for req in reqs_all:\n",
    "    for choi in req:\n",
    "        if not choi[0][2]:\n",
    "            ans =\"F\"\n",
    "        else:\n",
    "            ans =\"T\"\n",
    "        ans_back.append(ans)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_back.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7295"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1459/2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[(0.03314208984375, 0.861328125, True)],\n",
       "  [(0.07000732421875, 0.83984375, True)],\n",
       "  [(0.438720703125, 0.459716796875, False)],\n",
       "  [(0.0615234375, 0.85595703125, True)]],\n",
       " [[(0.1956787109375, 0.72705078125, True)],\n",
       "  [(0.1995849609375, 0.70751953125, True)],\n",
       "  [(0.263916015625, 0.64306640625, True)],\n",
       "  [(0.35302734375, 0.52587890625, False)]],\n",
       " [[(0.2191162109375, 0.66455078125, True)],\n",
       "  [(0.400634765625, 0.48681640625, False)],\n",
       "  [(0.1810302734375, 0.73291015625, True)],\n",
       "  [(0.21240234375, 0.69091796875, True)]],\n",
       " [[(0.396240234375, 0.4970703125, True)],\n",
       "  [(0.53955078125, 0.359375, True)],\n",
       "  [(0.261962890625, 0.6484375, True)],\n",
       "  [(0.2763671875, 0.63232421875, True)]],\n",
       " [[(0.1082763671875, 0.79345703125, True)],\n",
       "  [(0.1444091796875, 0.7568359375, True)],\n",
       "  [(0.11767578125, 0.79150390625, True)],\n",
       "  [(0.230224609375, 0.6767578125, False)]],\n",
       " [[(0.53466796875, 0.314453125, True)],\n",
       "  [(0.458251953125, 0.413818359375, False)],\n",
       "  [(0.36474609375, 0.50634765625, True)],\n",
       "  [(0.469482421875, 0.417724609375, False)]],\n",
       " [[(0.3056640625, 0.58935546875, True)],\n",
       "  [(0.472900390625, 0.424072265625, False)],\n",
       "  [(0.50048828125, 0.408447265625, True)],\n",
       "  [(0.316650390625, 0.564453125, True)]],\n",
       " [[(0.392333984375, 0.52783203125, True)],\n",
       "  [(0.4482421875, 0.4921875, True)],\n",
       "  [(0.480224609375, 0.437255859375, True)],\n",
       "  [(0.402099609375, 0.53271484375, True)]],\n",
       " [[(0.2509765625, 0.671875, True)],\n",
       "  [(0.2880859375, 0.61962890625, True)],\n",
       "  [(0.139404296875, 0.765625, True)],\n",
       "  [(0.546875, 0.3701171875, True)]],\n",
       " [[(0.218505859375, 0.6572265625, False)],\n",
       "  [(0.11505126953125, 0.78662109375, True)],\n",
       "  [(0.0823974609375, 0.83203125, True)],\n",
       "  [(0.1419677734375, 0.7265625, True)]]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the fact that 'deep sea animals live deep in the ocean': Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_text(task_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = doc_to_text(task_docs[0]) + \" {}\".format(task_docs[0][\"choices\"][0]) + ' Is the statement above correct? Reply with \"Yes\" or \"No\". Answer:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the fact that \"deep sea animals live deep in the ocean\": Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as Deep sea animals Is the statement above correct? Reply with \"Yes\" or \"No\". Answer:'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1a9b71f3884b33adfcf52b61826166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336a631968b143219dd375af476b49e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8fa98b5c8544ac9eda2dee6ae26c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset commonsense_qa/default to /home/wab/.cache/huggingface/datasets/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9cd6a2643c4b1487bb5f306ebefc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2b71b674f44857bb0d8c248d3fbfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d6a72538e34d1c8a10685528db577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84351913fe40437f9d521ad1fe76d997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b41f2f3ed14043a94480aadc365776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3130c376c81a45b0a897cac834b900ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73e2860ea3b4bfe9ffdb5c5d0b43af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83842b649684e16aa826377ebf73a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset commonsense_qa downloaded and prepared to /home/wab/.cache/huggingface/datasets/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61df5def53104f90899b4f27728693a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "commonsense = load_dataset(\"commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_docs(dataset):\n",
    "    return map(_process_doc, dataset[\"validation\"])\n",
    "\n",
    "def _process_doc(doc):\n",
    "    out_doc = {\n",
    "        \"goal\": doc['question'],\n",
    "        \"target\": doc['choices']['text'], \n",
    "        \"label\":ord(doc['answerKey'])-ord(\"A\"),\n",
    "    }\n",
    "    return out_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(doc):\n",
    "    return \"Question: \" + doc[\"goal\"] + \" Answer:\"\n",
    "def construct_requests(doc, ctx):\n",
    "    lls = []\n",
    "    input = ctx \n",
    "    lls.append([loglikelihood(input,doc)[0]])\n",
    "    return lls\n",
    "def loglikelihood(input,doc):\n",
    "    new_reqs = []\n",
    "    context_enc = tok_encode(input)\n",
    "    target_enc = []\n",
    "    for target in doc[\"target\"]:\n",
    "        target_enc.append(tok_encode(target)[0])\n",
    "    new_reqs.append(((input,doc['label']), context_enc, target_enc))\n",
    "    return _loglikelihood_tokens(new_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    # cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, target_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        # assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        # cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        # cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key,context_enc,target_enc), logits, inp, inplen in zip(\n",
    "        requests, multi_logits, inps, inplens\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "        indices = torch.tensor(target_enc)\n",
    "        pre_logits = torch.index_select(final_token,0,indices)\n",
    "        if (bool(pre_logits.argmax(dim=-1) == cache_key[1])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        # cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "        #     0\n",
    "        # )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        # logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "        #     -1\n",
    "        # )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (pre_logits.numpy(),cache_key[1], bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(string: str):\n",
    "    return tokenizer.encode(string, add_special_tokens=False)\n",
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc\n",
    "def _model_call(inps):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits returned from the model\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return model(inps)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [06:03<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "task_doc_func = validation_docs(commonsense)\n",
    "task_docs = list(task_doc_func)\n",
    "docs = {}\n",
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task_docs)):\n",
    "    docs[doc_id] = doc\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests(doc, ctx)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[(array([1.580e-03, 1.907e-05, 2.482e-04, 2.561e-04, 2.384e-07],\n",
       "          dtype=float16),\n",
       "    0,\n",
       "    True)]],\n",
       " [[(array([1.4e-06, 3.2e-06, 3.0e-07, 2.4e-07, 1.2e-06], dtype=float16),\n",
       "    0,\n",
       "    False)]],\n",
       " [[(array([0.000e+00, 4.182e-04, 1.192e-07, 5.960e-08, 0.000e+00],\n",
       "          dtype=float16),\n",
       "    1,\n",
       "    True)]],\n",
       " [[(array([6.086e-05, 3.636e-06, 1.192e-07, 5.960e-08, 5.960e-08],\n",
       "          dtype=float16),\n",
       "    0,\n",
       "    True)]],\n",
       " [[(array([1.788e-07, 8.345e-07, 1.192e-07, 5.233e-05, 0.000e+00],\n",
       "          dtype=float16),\n",
       "    0,\n",
       "    False)]],\n",
       " [[(array([0.00e+00, 1.19e-07, 2.09e-06, 7.75e-06, 8.34e-07], dtype=float16),\n",
       "    2,\n",
       "    False)]],\n",
       " [[(array([1.25e-06, 4.95e-06, 5.96e-08, 0.00e+00, 5.84e-06], dtype=float16),\n",
       "    1,\n",
       "    False)]],\n",
       " [[(array([2.e-07, 5.e-07, 6.e-07, 2.e-07, 0.e+00], dtype=float16),\n",
       "    3,\n",
       "    False)]],\n",
       " [[(array([7.5e-06, 3.6e-07, 2.4e-07, 6.0e-08, 2.4e-07], dtype=float16),\n",
       "    0,\n",
       "    True)]],\n",
       " [[(array([1.2e-07, 1.2e-07, 1.2e-07, 8.3e-07, 1.2e-07], dtype=float16),\n",
       "    2,\n",
       "    False)]]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = task_docs[0]['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_back = []\n",
    "for req in reqs_all:\n",
    "    for choi in req:\n",
    "        if not choi[0][2]:\n",
    "            ans =\"F\"\n",
    "        else:\n",
    "            ans =\"T\"\n",
    "        ans_back.append(ans)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_back.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5495495495495496"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "671/1221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_prompt_starter = \"The following are pairs of Winograd Schema in the form of a statement S, a question Q, and an answer A:\"\n",
    "init_prompt1 = \"S: The cat went through the door, but it's tail got stuck. Q: In the previous statement, what does 'it' refer to? A: The cat.\"\n",
    "init_prompt2 = \"S: The cat tried to go through the door, but it was too small. Q: In the previous statement, what does 'it' refer to? A: The door.\"\n",
    "init_prompt3 = \"S: Fedex made more profit than UPS last year, but that was mostly due to the success of the new delivery system they implemented. Q: In the previous statement, what does 'they' refer to? A: Fedex.\"\n",
    "init_prompt4 = \"S: Sam tried to buy Xerxes lunch, but he wouldn't allow it. Q: In the previous statement, who does 'he' refer to? A: Xerxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_format_helper(pronoun: str, answers: List[str]) -> str:\n",
    " return f'Q: In the previous statement, does \"{pronoun}\" refer to {answers[0]} or {answers[1]}? A:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_enc = []\n",
    "for target in task_docs[0][\"target\"]:\n",
    "    target_enc.append(tok_encode(target)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9124, 3489, 14311, 286, 716]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
       " 'target': ['bank', 'library', 'department store', 'mall', 'new york'],\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁bank', '▁library', '▁department', '▁m', '▁new']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([9124, 3489, 14311, 286, 716])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.8145, 0.1438, 0.0070, 0.0055, 0.0054, 0.0014, 0.0014, 0.0012, 0.0011,\n",
       "        0.0010], device='cuda:3', dtype=torch.float16),\n",
       "indices=tensor([   13,     2,   319,  5853,   313,   334,   472,    12, 29871,   901],\n",
       "       device='cuda:3'))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_logits[0,-1].to(3).topk(k=10, largest=True, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.tensor(target_enc)\n",
    "pre_logits = torch.index_select(multi_logits[0,-1],0,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8280e-06, 7.1526e-07, 5.9605e-07, 2.0981e-05, 0.0000e+00],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### winograde创新版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from child_generator_old_wab import make_sentences\n",
    "from child_frames_old import frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a,b = frames[0]['orig_sentence'].split(' ')[-1][:-1].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('large', 'small')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 12,\n",
       " 'orig_sentence': 'Frank felt vindicated/crushed when his longtime rival Bill revealed that [he] was the winner of the competition.',\n",
       " 'entities': ['John', 'Susan'],\n",
       " 'entity_substitutes': [['David', 'Michael'], ['Mary', 'Linda']],\n",
       " 'packed_relations': [\"beat/didn't beat\", \"lost to/didn't lose to\"],\n",
       " 'packed_relation_substitutes': [[\"defeated/didn't defeat\"],\n",
       "  [\"was defeated by/wasn't defeated by\"]],\n",
       " 'relation_suffix': 'in the game',\n",
       " 'packed_predicates': [\"was happy/wasn't happy\", \"was sad/wasn't sad\"],\n",
       " 'reverse_causal': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_docs(dataset):\n",
    "    return map(_process_doc, dataset)\n",
    "\n",
    "def _process_doc(doc):\n",
    "    predict = doc['orig_sentence'].split(' ')[-1][:-1].split('/')\n",
    "    if  len(predict)<2:\n",
    "        return None\n",
    "    target_true,target_false = predict\n",
    "    pre = doc['orig_sentence'].index('[')\n",
    "    post = doc['orig_sentence'].index(']')\n",
    "    input = doc['orig_sentence'][:pre] + doc['entities'][0] + ' '.join(doc['orig_sentence'][post+1:].split(' ')[:-1])\n",
    "    out_doc = {\n",
    "        \"goal\":  input.rstrip() ,\n",
    "#         \"target_true\": ' '.join(true_list[max_lenth:]), \n",
    "#         \"target_false\": ' '.join(false_list[max_lenth:]), \n",
    "        \"target_true\": target_true, \n",
    "        \"target_false\":target_false, \n",
    "    }\n",
    "    return out_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_doc_func = validation_docs(frames)\n",
    "task_docs_a = list(task_doc_func)\n",
    "task_docs = list(i for i in task_doc_func if i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'goal': \"The trophy doesn't fit into the brown suitcase because trophy is too\",\n",
       "  'target_true': 'large',\n",
       "  'target_false': 'small'},\n",
       " {'goal': 'Joan made sure to thank Susan for all the help John had',\n",
       "  'target_true': 'recieved',\n",
       "  'target_false': 'given'},\n",
       " {'goal': 'John gave a lot of money to Susan because John was very',\n",
       "  'target_true': 'rich',\n",
       "  'target_false': 'poor'},\n",
       " {'goal': 'The delivery truck zoomed by the school bus because truck was going so',\n",
       "  'target_true': 'fast',\n",
       "  'target_false': 'slow'},\n",
       " None,\n",
       " {'goal': 'The large ball crashed right through the table because ball was made of',\n",
       "  'target_true': 'steel',\n",
       "  'target_false': 'styrofoam'},\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'goal': \"The trophy doesn't fit into the brown suitcase because trophy is too\",\n",
       "  'target_true': 'large',\n",
       "  'target_false': 'small'},\n",
       " {'goal': 'Joan made sure to thank Susan for all the help John had',\n",
       "  'target_true': 'recieved',\n",
       "  'target_false': 'given'},\n",
       " {'goal': 'John gave a lot of money to Susan because John was very',\n",
       "  'target_true': 'rich',\n",
       "  'target_false': 'poor'},\n",
       " {'goal': 'The delivery truck zoomed by the school bus because truck was going so',\n",
       "  'target_true': 'fast',\n",
       "  'target_false': 'slow'},\n",
       " {'goal': 'The large ball crashed right through the table because ball was made of',\n",
       "  'target_true': 'steel',\n",
       "  'target_false': 'styrofoam'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [make_sentences(**i) for i in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"the trophy doesn't fit into the suitcase ||| because the [trophy] is large.\",\n",
       "  \"the trophy doesn't fit into the suitcase ||| because the [suitcase] is small.\",\n",
       "  \"the trophy doesn't fit into the suitcase ||| because the [trophy] isn't small.\",\n",
       "  \"the trophy doesn't fit into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| because the [trophy] is large.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| because the [suitcase] is small.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| because the [trophy] isn't small.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| because the [suitcase] isn't large.\",\n",
       "  \"the trophy can fit into the suitcase ||| because the [trophy] isn't large.\",\n",
       "  \"the trophy can fit into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "  'the trophy can fit into the suitcase ||| because the [trophy] is small.',\n",
       "  'the trophy can fit into the suitcase ||| because the [suitcase] is large.',\n",
       "  \"the suitcase can hold the trophy ||| because the [trophy] isn't large.\",\n",
       "  \"the suitcase can hold the trophy ||| because the [suitcase] isn't small.\",\n",
       "  'the suitcase can hold the trophy ||| because the [trophy] is small.',\n",
       "  'the suitcase can hold the trophy ||| because the [suitcase] is large.'],\n",
       " [\"the trophy doesn't fit into the suitcase ||| although the [trophy] isn't large.\",\n",
       "  \"the trophy doesn't fit into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "  \"the trophy doesn't fit into the suitcase ||| although the [trophy] is small.\",\n",
       "  \"the trophy doesn't fit into the suitcase ||| although the [suitcase] is large.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| although the [trophy] isn't large.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| although the [suitcase] isn't small.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| although the [trophy] is small.\",\n",
       "  \"the suitcase doesn't hold the trophy ||| although the [suitcase] is large.\",\n",
       "  'the trophy can fit into the suitcase ||| although the [trophy] is large.',\n",
       "  'the trophy can fit into the suitcase ||| although the [suitcase] is small.',\n",
       "  \"the trophy can fit into the suitcase ||| although the [trophy] isn't small.\",\n",
       "  \"the trophy can fit into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "  'the suitcase can hold the trophy ||| although the [trophy] is large.',\n",
       "  'the suitcase can hold the trophy ||| although the [suitcase] is small.',\n",
       "  \"the suitcase can hold the trophy ||| although the [trophy] isn't small.\",\n",
       "  \"the suitcase can hold the trophy ||| although the [suitcase] isn't large.\"],\n",
       " [[\"the trophy doesn't fit into the suitcase ||| because the [trophy] is large.\",\n",
       "   \"the trophy doesn't fit into the bag ||| because the [trophy] is large.\",\n",
       "   \"the trophy doesn't fit into the box ||| because the [trophy] is large.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| because the [ball] is large.\",\n",
       "   \"the ball doesn't fit into the bag ||| because the [ball] is large.\",\n",
       "   \"the ball doesn't fit into the box ||| because the [ball] is large.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| because the [toy] is large.\",\n",
       "   \"the toy doesn't fit into the bag ||| because the [toy] is large.\",\n",
       "   \"the toy doesn't fit into the box ||| because the [toy] is large.\",\n",
       "   \"the trophy can't be put into the suitcase ||| because the [trophy] is large.\",\n",
       "   \"the trophy can't be put into the bag ||| because the [trophy] is large.\",\n",
       "   \"the trophy can't be put into the box ||| because the [trophy] is large.\",\n",
       "   \"the ball can't be put into the suitcase ||| because the [ball] is large.\",\n",
       "   \"the ball can't be put into the bag ||| because the [ball] is large.\",\n",
       "   \"the ball can't be put into the box ||| because the [ball] is large.\",\n",
       "   \"the toy can't be put into the suitcase ||| because the [toy] is large.\",\n",
       "   \"the toy can't be put into the bag ||| because the [toy] is large.\",\n",
       "   \"the toy can't be put into the box ||| because the [toy] is large.\"],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| because the [suitcase] is small.\",\n",
       "   \"the trophy doesn't fit into the bag ||| because the [bag] is small.\",\n",
       "   \"the trophy doesn't fit into the box ||| because the [box] is small.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| because the [suitcase] is small.\",\n",
       "   \"the ball doesn't fit into the bag ||| because the [bag] is small.\",\n",
       "   \"the ball doesn't fit into the box ||| because the [box] is small.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| because the [suitcase] is small.\",\n",
       "   \"the toy doesn't fit into the bag ||| because the [bag] is small.\",\n",
       "   \"the toy doesn't fit into the box ||| because the [box] is small.\",\n",
       "   \"the trophy can't be put into the suitcase ||| because the [suitcase] is small.\",\n",
       "   \"the trophy can't be put into the bag ||| because the [bag] is small.\",\n",
       "   \"the trophy can't be put into the box ||| because the [box] is small.\",\n",
       "   \"the ball can't be put into the suitcase ||| because the [suitcase] is small.\",\n",
       "   \"the ball can't be put into the bag ||| because the [bag] is small.\",\n",
       "   \"the ball can't be put into the box ||| because the [box] is small.\",\n",
       "   \"the toy can't be put into the suitcase ||| because the [suitcase] is small.\",\n",
       "   \"the toy can't be put into the bag ||| because the [bag] is small.\",\n",
       "   \"the toy can't be put into the box ||| because the [box] is small.\"],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| because the [trophy] isn't small.\",\n",
       "   \"the trophy doesn't fit into the bag ||| because the [trophy] isn't small.\",\n",
       "   \"the trophy doesn't fit into the box ||| because the [trophy] isn't small.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| because the [ball] isn't small.\",\n",
       "   \"the ball doesn't fit into the bag ||| because the [ball] isn't small.\",\n",
       "   \"the ball doesn't fit into the box ||| because the [ball] isn't small.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| because the [toy] isn't small.\",\n",
       "   \"the toy doesn't fit into the bag ||| because the [toy] isn't small.\",\n",
       "   \"the toy doesn't fit into the box ||| because the [toy] isn't small.\",\n",
       "   \"the trophy can't be put into the suitcase ||| because the [trophy] isn't small.\",\n",
       "   \"the trophy can't be put into the bag ||| because the [trophy] isn't small.\",\n",
       "   \"the trophy can't be put into the box ||| because the [trophy] isn't small.\",\n",
       "   \"the ball can't be put into the suitcase ||| because the [ball] isn't small.\",\n",
       "   \"the ball can't be put into the bag ||| because the [ball] isn't small.\",\n",
       "   \"the ball can't be put into the box ||| because the [ball] isn't small.\",\n",
       "   \"the toy can't be put into the suitcase ||| because the [toy] isn't small.\",\n",
       "   \"the toy can't be put into the bag ||| because the [toy] isn't small.\",\n",
       "   \"the toy can't be put into the box ||| because the [toy] isn't small.\"],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "   \"the trophy doesn't fit into the bag ||| because the [bag] isn't large.\",\n",
       "   \"the trophy doesn't fit into the box ||| because the [box] isn't large.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "   \"the ball doesn't fit into the bag ||| because the [bag] isn't large.\",\n",
       "   \"the ball doesn't fit into the box ||| because the [box] isn't large.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "   \"the toy doesn't fit into the bag ||| because the [bag] isn't large.\",\n",
       "   \"the toy doesn't fit into the box ||| because the [box] isn't large.\",\n",
       "   \"the trophy can't be put into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "   \"the trophy can't be put into the bag ||| because the [bag] isn't large.\",\n",
       "   \"the trophy can't be put into the box ||| because the [box] isn't large.\",\n",
       "   \"the ball can't be put into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "   \"the ball can't be put into the bag ||| because the [bag] isn't large.\",\n",
       "   \"the ball can't be put into the box ||| because the [box] isn't large.\",\n",
       "   \"the toy can't be put into the suitcase ||| because the [suitcase] isn't large.\",\n",
       "   \"the toy can't be put into the bag ||| because the [bag] isn't large.\",\n",
       "   \"the toy can't be put into the box ||| because the [box] isn't large.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| because the [trophy] is large.\",\n",
       "   \"the bag doesn't hold the trophy ||| because the [trophy] is large.\",\n",
       "   \"the box doesn't hold the trophy ||| because the [trophy] is large.\",\n",
       "   \"the suitcase doesn't hold the ball ||| because the [ball] is large.\",\n",
       "   \"the bag doesn't hold the ball ||| because the [ball] is large.\",\n",
       "   \"the box doesn't hold the ball ||| because the [ball] is large.\",\n",
       "   \"the suitcase doesn't hold the toy ||| because the [toy] is large.\",\n",
       "   \"the bag doesn't hold the toy ||| because the [toy] is large.\",\n",
       "   \"the box doesn't hold the toy ||| because the [toy] is large.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| because the [trophy] is large.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| because the [trophy] is large.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| because the [trophy] is large.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| because the [ball] is large.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| because the [ball] is large.\",\n",
       "   \"the box doesn't have enough room for the ball ||| because the [ball] is large.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| because the [toy] is large.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| because the [toy] is large.\",\n",
       "   \"the box doesn't have enough room for the toy ||| because the [toy] is large.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| because the [suitcase] is small.\",\n",
       "   \"the bag doesn't hold the trophy ||| because the [bag] is small.\",\n",
       "   \"the box doesn't hold the trophy ||| because the [box] is small.\",\n",
       "   \"the suitcase doesn't hold the ball ||| because the [suitcase] is small.\",\n",
       "   \"the bag doesn't hold the ball ||| because the [bag] is small.\",\n",
       "   \"the box doesn't hold the ball ||| because the [box] is small.\",\n",
       "   \"the suitcase doesn't hold the toy ||| because the [suitcase] is small.\",\n",
       "   \"the bag doesn't hold the toy ||| because the [bag] is small.\",\n",
       "   \"the box doesn't hold the toy ||| because the [box] is small.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| because the [suitcase] is small.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| because the [bag] is small.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| because the [box] is small.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| because the [suitcase] is small.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| because the [bag] is small.\",\n",
       "   \"the box doesn't have enough room for the ball ||| because the [box] is small.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| because the [suitcase] is small.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| because the [bag] is small.\",\n",
       "   \"the box doesn't have enough room for the toy ||| because the [box] is small.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| because the [trophy] isn't small.\",\n",
       "   \"the bag doesn't hold the trophy ||| because the [trophy] isn't small.\",\n",
       "   \"the box doesn't hold the trophy ||| because the [trophy] isn't small.\",\n",
       "   \"the suitcase doesn't hold the ball ||| because the [ball] isn't small.\",\n",
       "   \"the bag doesn't hold the ball ||| because the [ball] isn't small.\",\n",
       "   \"the box doesn't hold the ball ||| because the [ball] isn't small.\",\n",
       "   \"the suitcase doesn't hold the toy ||| because the [toy] isn't small.\",\n",
       "   \"the bag doesn't hold the toy ||| because the [toy] isn't small.\",\n",
       "   \"the box doesn't hold the toy ||| because the [toy] isn't small.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| because the [trophy] isn't small.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| because the [trophy] isn't small.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| because the [trophy] isn't small.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| because the [ball] isn't small.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| because the [ball] isn't small.\",\n",
       "   \"the box doesn't have enough room for the ball ||| because the [ball] isn't small.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| because the [toy] isn't small.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| because the [toy] isn't small.\",\n",
       "   \"the box doesn't have enough room for the toy ||| because the [toy] isn't small.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| because the [suitcase] isn't large.\",\n",
       "   \"the bag doesn't hold the trophy ||| because the [bag] isn't large.\",\n",
       "   \"the box doesn't hold the trophy ||| because the [box] isn't large.\",\n",
       "   \"the suitcase doesn't hold the ball ||| because the [suitcase] isn't large.\",\n",
       "   \"the bag doesn't hold the ball ||| because the [bag] isn't large.\",\n",
       "   \"the box doesn't hold the ball ||| because the [box] isn't large.\",\n",
       "   \"the suitcase doesn't hold the toy ||| because the [suitcase] isn't large.\",\n",
       "   \"the bag doesn't hold the toy ||| because the [bag] isn't large.\",\n",
       "   \"the box doesn't hold the toy ||| because the [box] isn't large.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| because the [suitcase] isn't large.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| because the [bag] isn't large.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| because the [box] isn't large.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| because the [suitcase] isn't large.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| because the [bag] isn't large.\",\n",
       "   \"the box doesn't have enough room for the ball ||| because the [box] isn't large.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| because the [suitcase] isn't large.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| because the [bag] isn't large.\",\n",
       "   \"the box doesn't have enough room for the toy ||| because the [box] isn't large.\"],\n",
       "  [\"the trophy can fit into the suitcase ||| because the [trophy] isn't large.\",\n",
       "   \"the trophy can fit into the bag ||| because the [trophy] isn't large.\",\n",
       "   \"the trophy can fit into the box ||| because the [trophy] isn't large.\",\n",
       "   \"the ball can fit into the suitcase ||| because the [ball] isn't large.\",\n",
       "   \"the ball can fit into the bag ||| because the [ball] isn't large.\",\n",
       "   \"the ball can fit into the box ||| because the [ball] isn't large.\",\n",
       "   \"the toy can fit into the suitcase ||| because the [toy] isn't large.\",\n",
       "   \"the toy can fit into the bag ||| because the [toy] isn't large.\",\n",
       "   \"the toy can fit into the box ||| because the [toy] isn't large.\",\n",
       "   \"the trophy can be put into the suitcase ||| because the [trophy] isn't large.\",\n",
       "   \"the trophy can be put into the bag ||| because the [trophy] isn't large.\",\n",
       "   \"the trophy can be put into the box ||| because the [trophy] isn't large.\",\n",
       "   \"the ball can be put into the suitcase ||| because the [ball] isn't large.\",\n",
       "   \"the ball can be put into the bag ||| because the [ball] isn't large.\",\n",
       "   \"the ball can be put into the box ||| because the [ball] isn't large.\",\n",
       "   \"the toy can be put into the suitcase ||| because the [toy] isn't large.\",\n",
       "   \"the toy can be put into the bag ||| because the [toy] isn't large.\",\n",
       "   \"the toy can be put into the box ||| because the [toy] isn't large.\"],\n",
       "  [\"the trophy can fit into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "   \"the trophy can fit into the bag ||| because the [bag] isn't small.\",\n",
       "   \"the trophy can fit into the box ||| because the [box] isn't small.\",\n",
       "   \"the ball can fit into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "   \"the ball can fit into the bag ||| because the [bag] isn't small.\",\n",
       "   \"the ball can fit into the box ||| because the [box] isn't small.\",\n",
       "   \"the toy can fit into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "   \"the toy can fit into the bag ||| because the [bag] isn't small.\",\n",
       "   \"the toy can fit into the box ||| because the [box] isn't small.\",\n",
       "   \"the trophy can be put into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "   \"the trophy can be put into the bag ||| because the [bag] isn't small.\",\n",
       "   \"the trophy can be put into the box ||| because the [box] isn't small.\",\n",
       "   \"the ball can be put into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "   \"the ball can be put into the bag ||| because the [bag] isn't small.\",\n",
       "   \"the ball can be put into the box ||| because the [box] isn't small.\",\n",
       "   \"the toy can be put into the suitcase ||| because the [suitcase] isn't small.\",\n",
       "   \"the toy can be put into the bag ||| because the [bag] isn't small.\",\n",
       "   \"the toy can be put into the box ||| because the [box] isn't small.\"],\n",
       "  ['the trophy can fit into the suitcase ||| because the [trophy] is small.',\n",
       "   'the trophy can fit into the bag ||| because the [trophy] is small.',\n",
       "   'the trophy can fit into the box ||| because the [trophy] is small.',\n",
       "   'the ball can fit into the suitcase ||| because the [ball] is small.',\n",
       "   'the ball can fit into the bag ||| because the [ball] is small.',\n",
       "   'the ball can fit into the box ||| because the [ball] is small.',\n",
       "   'the toy can fit into the suitcase ||| because the [toy] is small.',\n",
       "   'the toy can fit into the bag ||| because the [toy] is small.',\n",
       "   'the toy can fit into the box ||| because the [toy] is small.',\n",
       "   'the trophy can be put into the suitcase ||| because the [trophy] is small.',\n",
       "   'the trophy can be put into the bag ||| because the [trophy] is small.',\n",
       "   'the trophy can be put into the box ||| because the [trophy] is small.',\n",
       "   'the ball can be put into the suitcase ||| because the [ball] is small.',\n",
       "   'the ball can be put into the bag ||| because the [ball] is small.',\n",
       "   'the ball can be put into the box ||| because the [ball] is small.',\n",
       "   'the toy can be put into the suitcase ||| because the [toy] is small.',\n",
       "   'the toy can be put into the bag ||| because the [toy] is small.',\n",
       "   'the toy can be put into the box ||| because the [toy] is small.'],\n",
       "  ['the trophy can fit into the suitcase ||| because the [suitcase] is large.',\n",
       "   'the trophy can fit into the bag ||| because the [bag] is large.',\n",
       "   'the trophy can fit into the box ||| because the [box] is large.',\n",
       "   'the ball can fit into the suitcase ||| because the [suitcase] is large.',\n",
       "   'the ball can fit into the bag ||| because the [bag] is large.',\n",
       "   'the ball can fit into the box ||| because the [box] is large.',\n",
       "   'the toy can fit into the suitcase ||| because the [suitcase] is large.',\n",
       "   'the toy can fit into the bag ||| because the [bag] is large.',\n",
       "   'the toy can fit into the box ||| because the [box] is large.',\n",
       "   'the trophy can be put into the suitcase ||| because the [suitcase] is large.',\n",
       "   'the trophy can be put into the bag ||| because the [bag] is large.',\n",
       "   'the trophy can be put into the box ||| because the [box] is large.',\n",
       "   'the ball can be put into the suitcase ||| because the [suitcase] is large.',\n",
       "   'the ball can be put into the bag ||| because the [bag] is large.',\n",
       "   'the ball can be put into the box ||| because the [box] is large.',\n",
       "   'the toy can be put into the suitcase ||| because the [suitcase] is large.',\n",
       "   'the toy can be put into the bag ||| because the [bag] is large.',\n",
       "   'the toy can be put into the box ||| because the [box] is large.'],\n",
       "  [\"the suitcase can hold the trophy ||| because the [trophy] isn't large.\",\n",
       "   \"the bag can hold the trophy ||| because the [trophy] isn't large.\",\n",
       "   \"the box can hold the trophy ||| because the [trophy] isn't large.\",\n",
       "   \"the suitcase can hold the ball ||| because the [ball] isn't large.\",\n",
       "   \"the bag can hold the ball ||| because the [ball] isn't large.\",\n",
       "   \"the box can hold the ball ||| because the [ball] isn't large.\",\n",
       "   \"the suitcase can hold the toy ||| because the [toy] isn't large.\",\n",
       "   \"the bag can hold the toy ||| because the [toy] isn't large.\",\n",
       "   \"the box can hold the toy ||| because the [toy] isn't large.\",\n",
       "   \"the suitcase has enough room for the trophy ||| because the [trophy] isn't large.\",\n",
       "   \"the bag has enough room for the trophy ||| because the [trophy] isn't large.\",\n",
       "   \"the box has enough room for the trophy ||| because the [trophy] isn't large.\",\n",
       "   \"the suitcase has enough room for the ball ||| because the [ball] isn't large.\",\n",
       "   \"the bag has enough room for the ball ||| because the [ball] isn't large.\",\n",
       "   \"the box has enough room for the ball ||| because the [ball] isn't large.\",\n",
       "   \"the suitcase has enough room for the toy ||| because the [toy] isn't large.\",\n",
       "   \"the bag has enough room for the toy ||| because the [toy] isn't large.\",\n",
       "   \"the box has enough room for the toy ||| because the [toy] isn't large.\"],\n",
       "  [\"the suitcase can hold the trophy ||| because the [suitcase] isn't small.\",\n",
       "   \"the bag can hold the trophy ||| because the [bag] isn't small.\",\n",
       "   \"the box can hold the trophy ||| because the [box] isn't small.\",\n",
       "   \"the suitcase can hold the ball ||| because the [suitcase] isn't small.\",\n",
       "   \"the bag can hold the ball ||| because the [bag] isn't small.\",\n",
       "   \"the box can hold the ball ||| because the [box] isn't small.\",\n",
       "   \"the suitcase can hold the toy ||| because the [suitcase] isn't small.\",\n",
       "   \"the bag can hold the toy ||| because the [bag] isn't small.\",\n",
       "   \"the box can hold the toy ||| because the [box] isn't small.\",\n",
       "   \"the suitcase has enough room for the trophy ||| because the [suitcase] isn't small.\",\n",
       "   \"the bag has enough room for the trophy ||| because the [bag] isn't small.\",\n",
       "   \"the box has enough room for the trophy ||| because the [box] isn't small.\",\n",
       "   \"the suitcase has enough room for the ball ||| because the [suitcase] isn't small.\",\n",
       "   \"the bag has enough room for the ball ||| because the [bag] isn't small.\",\n",
       "   \"the box has enough room for the ball ||| because the [box] isn't small.\",\n",
       "   \"the suitcase has enough room for the toy ||| because the [suitcase] isn't small.\",\n",
       "   \"the bag has enough room for the toy ||| because the [bag] isn't small.\",\n",
       "   \"the box has enough room for the toy ||| because the [box] isn't small.\"],\n",
       "  ['the suitcase can hold the trophy ||| because the [trophy] is small.',\n",
       "   'the bag can hold the trophy ||| because the [trophy] is small.',\n",
       "   'the box can hold the trophy ||| because the [trophy] is small.',\n",
       "   'the suitcase can hold the ball ||| because the [ball] is small.',\n",
       "   'the bag can hold the ball ||| because the [ball] is small.',\n",
       "   'the box can hold the ball ||| because the [ball] is small.',\n",
       "   'the suitcase can hold the toy ||| because the [toy] is small.',\n",
       "   'the bag can hold the toy ||| because the [toy] is small.',\n",
       "   'the box can hold the toy ||| because the [toy] is small.',\n",
       "   'the suitcase has enough room for the trophy ||| because the [trophy] is small.',\n",
       "   'the bag has enough room for the trophy ||| because the [trophy] is small.',\n",
       "   'the box has enough room for the trophy ||| because the [trophy] is small.',\n",
       "   'the suitcase has enough room for the ball ||| because the [ball] is small.',\n",
       "   'the bag has enough room for the ball ||| because the [ball] is small.',\n",
       "   'the box has enough room for the ball ||| because the [ball] is small.',\n",
       "   'the suitcase has enough room for the toy ||| because the [toy] is small.',\n",
       "   'the bag has enough room for the toy ||| because the [toy] is small.',\n",
       "   'the box has enough room for the toy ||| because the [toy] is small.'],\n",
       "  ['the suitcase can hold the trophy ||| because the [suitcase] is large.',\n",
       "   'the bag can hold the trophy ||| because the [bag] is large.',\n",
       "   'the box can hold the trophy ||| because the [box] is large.',\n",
       "   'the suitcase can hold the ball ||| because the [suitcase] is large.',\n",
       "   'the bag can hold the ball ||| because the [bag] is large.',\n",
       "   'the box can hold the ball ||| because the [box] is large.',\n",
       "   'the suitcase can hold the toy ||| because the [suitcase] is large.',\n",
       "   'the bag can hold the toy ||| because the [bag] is large.',\n",
       "   'the box can hold the toy ||| because the [box] is large.',\n",
       "   'the suitcase has enough room for the trophy ||| because the [suitcase] is large.',\n",
       "   'the bag has enough room for the trophy ||| because the [bag] is large.',\n",
       "   'the box has enough room for the trophy ||| because the [box] is large.',\n",
       "   'the suitcase has enough room for the ball ||| because the [suitcase] is large.',\n",
       "   'the bag has enough room for the ball ||| because the [bag] is large.',\n",
       "   'the box has enough room for the ball ||| because the [box] is large.',\n",
       "   'the suitcase has enough room for the toy ||| because the [suitcase] is large.',\n",
       "   'the bag has enough room for the toy ||| because the [bag] is large.',\n",
       "   'the box has enough room for the toy ||| because the [box] is large.'],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| although the [trophy] isn't large.\",\n",
       "   \"the trophy doesn't fit into the bag ||| although the [trophy] isn't large.\",\n",
       "   \"the trophy doesn't fit into the box ||| although the [trophy] isn't large.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| although the [ball] isn't large.\",\n",
       "   \"the ball doesn't fit into the bag ||| although the [ball] isn't large.\",\n",
       "   \"the ball doesn't fit into the box ||| although the [ball] isn't large.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| although the [toy] isn't large.\",\n",
       "   \"the toy doesn't fit into the bag ||| although the [toy] isn't large.\",\n",
       "   \"the toy doesn't fit into the box ||| although the [toy] isn't large.\",\n",
       "   \"the trophy can't be put into the suitcase ||| although the [trophy] isn't large.\",\n",
       "   \"the trophy can't be put into the bag ||| although the [trophy] isn't large.\",\n",
       "   \"the trophy can't be put into the box ||| although the [trophy] isn't large.\",\n",
       "   \"the ball can't be put into the suitcase ||| although the [ball] isn't large.\",\n",
       "   \"the ball can't be put into the bag ||| although the [ball] isn't large.\",\n",
       "   \"the ball can't be put into the box ||| although the [ball] isn't large.\",\n",
       "   \"the toy can't be put into the suitcase ||| although the [toy] isn't large.\",\n",
       "   \"the toy can't be put into the bag ||| although the [toy] isn't large.\",\n",
       "   \"the toy can't be put into the box ||| although the [toy] isn't large.\"],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "   \"the trophy doesn't fit into the bag ||| although the [bag] isn't small.\",\n",
       "   \"the trophy doesn't fit into the box ||| although the [box] isn't small.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "   \"the ball doesn't fit into the bag ||| although the [bag] isn't small.\",\n",
       "   \"the ball doesn't fit into the box ||| although the [box] isn't small.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "   \"the toy doesn't fit into the bag ||| although the [bag] isn't small.\",\n",
       "   \"the toy doesn't fit into the box ||| although the [box] isn't small.\",\n",
       "   \"the trophy can't be put into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "   \"the trophy can't be put into the bag ||| although the [bag] isn't small.\",\n",
       "   \"the trophy can't be put into the box ||| although the [box] isn't small.\",\n",
       "   \"the ball can't be put into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "   \"the ball can't be put into the bag ||| although the [bag] isn't small.\",\n",
       "   \"the ball can't be put into the box ||| although the [box] isn't small.\",\n",
       "   \"the toy can't be put into the suitcase ||| although the [suitcase] isn't small.\",\n",
       "   \"the toy can't be put into the bag ||| although the [bag] isn't small.\",\n",
       "   \"the toy can't be put into the box ||| although the [box] isn't small.\"],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| although the [trophy] is small.\",\n",
       "   \"the trophy doesn't fit into the bag ||| although the [trophy] is small.\",\n",
       "   \"the trophy doesn't fit into the box ||| although the [trophy] is small.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| although the [ball] is small.\",\n",
       "   \"the ball doesn't fit into the bag ||| although the [ball] is small.\",\n",
       "   \"the ball doesn't fit into the box ||| although the [ball] is small.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| although the [toy] is small.\",\n",
       "   \"the toy doesn't fit into the bag ||| although the [toy] is small.\",\n",
       "   \"the toy doesn't fit into the box ||| although the [toy] is small.\",\n",
       "   \"the trophy can't be put into the suitcase ||| although the [trophy] is small.\",\n",
       "   \"the trophy can't be put into the bag ||| although the [trophy] is small.\",\n",
       "   \"the trophy can't be put into the box ||| although the [trophy] is small.\",\n",
       "   \"the ball can't be put into the suitcase ||| although the [ball] is small.\",\n",
       "   \"the ball can't be put into the bag ||| although the [ball] is small.\",\n",
       "   \"the ball can't be put into the box ||| although the [ball] is small.\",\n",
       "   \"the toy can't be put into the suitcase ||| although the [toy] is small.\",\n",
       "   \"the toy can't be put into the bag ||| although the [toy] is small.\",\n",
       "   \"the toy can't be put into the box ||| although the [toy] is small.\"],\n",
       "  [\"the trophy doesn't fit into the suitcase ||| although the [suitcase] is large.\",\n",
       "   \"the trophy doesn't fit into the bag ||| although the [bag] is large.\",\n",
       "   \"the trophy doesn't fit into the box ||| although the [box] is large.\",\n",
       "   \"the ball doesn't fit into the suitcase ||| although the [suitcase] is large.\",\n",
       "   \"the ball doesn't fit into the bag ||| although the [bag] is large.\",\n",
       "   \"the ball doesn't fit into the box ||| although the [box] is large.\",\n",
       "   \"the toy doesn't fit into the suitcase ||| although the [suitcase] is large.\",\n",
       "   \"the toy doesn't fit into the bag ||| although the [bag] is large.\",\n",
       "   \"the toy doesn't fit into the box ||| although the [box] is large.\",\n",
       "   \"the trophy can't be put into the suitcase ||| although the [suitcase] is large.\",\n",
       "   \"the trophy can't be put into the bag ||| although the [bag] is large.\",\n",
       "   \"the trophy can't be put into the box ||| although the [box] is large.\",\n",
       "   \"the ball can't be put into the suitcase ||| although the [suitcase] is large.\",\n",
       "   \"the ball can't be put into the bag ||| although the [bag] is large.\",\n",
       "   \"the ball can't be put into the box ||| although the [box] is large.\",\n",
       "   \"the toy can't be put into the suitcase ||| although the [suitcase] is large.\",\n",
       "   \"the toy can't be put into the bag ||| although the [bag] is large.\",\n",
       "   \"the toy can't be put into the box ||| although the [box] is large.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| although the [trophy] isn't large.\",\n",
       "   \"the bag doesn't hold the trophy ||| although the [trophy] isn't large.\",\n",
       "   \"the box doesn't hold the trophy ||| although the [trophy] isn't large.\",\n",
       "   \"the suitcase doesn't hold the ball ||| although the [ball] isn't large.\",\n",
       "   \"the bag doesn't hold the ball ||| although the [ball] isn't large.\",\n",
       "   \"the box doesn't hold the ball ||| although the [ball] isn't large.\",\n",
       "   \"the suitcase doesn't hold the toy ||| although the [toy] isn't large.\",\n",
       "   \"the bag doesn't hold the toy ||| although the [toy] isn't large.\",\n",
       "   \"the box doesn't hold the toy ||| although the [toy] isn't large.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| although the [trophy] isn't large.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| although the [trophy] isn't large.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| although the [trophy] isn't large.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| although the [ball] isn't large.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| although the [ball] isn't large.\",\n",
       "   \"the box doesn't have enough room for the ball ||| although the [ball] isn't large.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| although the [toy] isn't large.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| although the [toy] isn't large.\",\n",
       "   \"the box doesn't have enough room for the toy ||| although the [toy] isn't large.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| although the [suitcase] isn't small.\",\n",
       "   \"the bag doesn't hold the trophy ||| although the [bag] isn't small.\",\n",
       "   \"the box doesn't hold the trophy ||| although the [box] isn't small.\",\n",
       "   \"the suitcase doesn't hold the ball ||| although the [suitcase] isn't small.\",\n",
       "   \"the bag doesn't hold the ball ||| although the [bag] isn't small.\",\n",
       "   \"the box doesn't hold the ball ||| although the [box] isn't small.\",\n",
       "   \"the suitcase doesn't hold the toy ||| although the [suitcase] isn't small.\",\n",
       "   \"the bag doesn't hold the toy ||| although the [bag] isn't small.\",\n",
       "   \"the box doesn't hold the toy ||| although the [box] isn't small.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| although the [suitcase] isn't small.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| although the [bag] isn't small.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| although the [box] isn't small.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| although the [suitcase] isn't small.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| although the [bag] isn't small.\",\n",
       "   \"the box doesn't have enough room for the ball ||| although the [box] isn't small.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| although the [suitcase] isn't small.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| although the [bag] isn't small.\",\n",
       "   \"the box doesn't have enough room for the toy ||| although the [box] isn't small.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| although the [trophy] is small.\",\n",
       "   \"the bag doesn't hold the trophy ||| although the [trophy] is small.\",\n",
       "   \"the box doesn't hold the trophy ||| although the [trophy] is small.\",\n",
       "   \"the suitcase doesn't hold the ball ||| although the [ball] is small.\",\n",
       "   \"the bag doesn't hold the ball ||| although the [ball] is small.\",\n",
       "   \"the box doesn't hold the ball ||| although the [ball] is small.\",\n",
       "   \"the suitcase doesn't hold the toy ||| although the [toy] is small.\",\n",
       "   \"the bag doesn't hold the toy ||| although the [toy] is small.\",\n",
       "   \"the box doesn't hold the toy ||| although the [toy] is small.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| although the [trophy] is small.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| although the [trophy] is small.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| although the [trophy] is small.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| although the [ball] is small.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| although the [ball] is small.\",\n",
       "   \"the box doesn't have enough room for the ball ||| although the [ball] is small.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| although the [toy] is small.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| although the [toy] is small.\",\n",
       "   \"the box doesn't have enough room for the toy ||| although the [toy] is small.\"],\n",
       "  [\"the suitcase doesn't hold the trophy ||| although the [suitcase] is large.\",\n",
       "   \"the bag doesn't hold the trophy ||| although the [bag] is large.\",\n",
       "   \"the box doesn't hold the trophy ||| although the [box] is large.\",\n",
       "   \"the suitcase doesn't hold the ball ||| although the [suitcase] is large.\",\n",
       "   \"the bag doesn't hold the ball ||| although the [bag] is large.\",\n",
       "   \"the box doesn't hold the ball ||| although the [box] is large.\",\n",
       "   \"the suitcase doesn't hold the toy ||| although the [suitcase] is large.\",\n",
       "   \"the bag doesn't hold the toy ||| although the [bag] is large.\",\n",
       "   \"the box doesn't hold the toy ||| although the [box] is large.\",\n",
       "   \"the suitcase doesn't have enough room for the trophy ||| although the [suitcase] is large.\",\n",
       "   \"the bag doesn't have enough room for the trophy ||| although the [bag] is large.\",\n",
       "   \"the box doesn't have enough room for the trophy ||| although the [box] is large.\",\n",
       "   \"the suitcase doesn't have enough room for the ball ||| although the [suitcase] is large.\",\n",
       "   \"the bag doesn't have enough room for the ball ||| although the [bag] is large.\",\n",
       "   \"the box doesn't have enough room for the ball ||| although the [box] is large.\",\n",
       "   \"the suitcase doesn't have enough room for the toy ||| although the [suitcase] is large.\",\n",
       "   \"the bag doesn't have enough room for the toy ||| although the [bag] is large.\",\n",
       "   \"the box doesn't have enough room for the toy ||| although the [box] is large.\"],\n",
       "  ['the trophy can fit into the suitcase ||| although the [trophy] is large.',\n",
       "   'the trophy can fit into the bag ||| although the [trophy] is large.',\n",
       "   'the trophy can fit into the box ||| although the [trophy] is large.',\n",
       "   'the ball can fit into the suitcase ||| although the [ball] is large.',\n",
       "   'the ball can fit into the bag ||| although the [ball] is large.',\n",
       "   'the ball can fit into the box ||| although the [ball] is large.',\n",
       "   'the toy can fit into the suitcase ||| although the [toy] is large.',\n",
       "   'the toy can fit into the bag ||| although the [toy] is large.',\n",
       "   'the toy can fit into the box ||| although the [toy] is large.',\n",
       "   'the trophy can be put into the suitcase ||| although the [trophy] is large.',\n",
       "   'the trophy can be put into the bag ||| although the [trophy] is large.',\n",
       "   'the trophy can be put into the box ||| although the [trophy] is large.',\n",
       "   'the ball can be put into the suitcase ||| although the [ball] is large.',\n",
       "   'the ball can be put into the bag ||| although the [ball] is large.',\n",
       "   'the ball can be put into the box ||| although the [ball] is large.',\n",
       "   'the toy can be put into the suitcase ||| although the [toy] is large.',\n",
       "   'the toy can be put into the bag ||| although the [toy] is large.',\n",
       "   'the toy can be put into the box ||| although the [toy] is large.'],\n",
       "  ['the trophy can fit into the suitcase ||| although the [suitcase] is small.',\n",
       "   'the trophy can fit into the bag ||| although the [bag] is small.',\n",
       "   'the trophy can fit into the box ||| although the [box] is small.',\n",
       "   'the ball can fit into the suitcase ||| although the [suitcase] is small.',\n",
       "   'the ball can fit into the bag ||| although the [bag] is small.',\n",
       "   'the ball can fit into the box ||| although the [box] is small.',\n",
       "   'the toy can fit into the suitcase ||| although the [suitcase] is small.',\n",
       "   'the toy can fit into the bag ||| although the [bag] is small.',\n",
       "   'the toy can fit into the box ||| although the [box] is small.',\n",
       "   'the trophy can be put into the suitcase ||| although the [suitcase] is small.',\n",
       "   'the trophy can be put into the bag ||| although the [bag] is small.',\n",
       "   'the trophy can be put into the box ||| although the [box] is small.',\n",
       "   'the ball can be put into the suitcase ||| although the [suitcase] is small.',\n",
       "   'the ball can be put into the bag ||| although the [bag] is small.',\n",
       "   'the ball can be put into the box ||| although the [box] is small.',\n",
       "   'the toy can be put into the suitcase ||| although the [suitcase] is small.',\n",
       "   'the toy can be put into the bag ||| although the [bag] is small.',\n",
       "   'the toy can be put into the box ||| although the [box] is small.'],\n",
       "  [\"the trophy can fit into the suitcase ||| although the [trophy] isn't small.\",\n",
       "   \"the trophy can fit into the bag ||| although the [trophy] isn't small.\",\n",
       "   \"the trophy can fit into the box ||| although the [trophy] isn't small.\",\n",
       "   \"the ball can fit into the suitcase ||| although the [ball] isn't small.\",\n",
       "   \"the ball can fit into the bag ||| although the [ball] isn't small.\",\n",
       "   \"the ball can fit into the box ||| although the [ball] isn't small.\",\n",
       "   \"the toy can fit into the suitcase ||| although the [toy] isn't small.\",\n",
       "   \"the toy can fit into the bag ||| although the [toy] isn't small.\",\n",
       "   \"the toy can fit into the box ||| although the [toy] isn't small.\",\n",
       "   \"the trophy can be put into the suitcase ||| although the [trophy] isn't small.\",\n",
       "   \"the trophy can be put into the bag ||| although the [trophy] isn't small.\",\n",
       "   \"the trophy can be put into the box ||| although the [trophy] isn't small.\",\n",
       "   \"the ball can be put into the suitcase ||| although the [ball] isn't small.\",\n",
       "   \"the ball can be put into the bag ||| although the [ball] isn't small.\",\n",
       "   \"the ball can be put into the box ||| although the [ball] isn't small.\",\n",
       "   \"the toy can be put into the suitcase ||| although the [toy] isn't small.\",\n",
       "   \"the toy can be put into the bag ||| although the [toy] isn't small.\",\n",
       "   \"the toy can be put into the box ||| although the [toy] isn't small.\"],\n",
       "  [\"the trophy can fit into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "   \"the trophy can fit into the bag ||| although the [bag] isn't large.\",\n",
       "   \"the trophy can fit into the box ||| although the [box] isn't large.\",\n",
       "   \"the ball can fit into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "   \"the ball can fit into the bag ||| although the [bag] isn't large.\",\n",
       "   \"the ball can fit into the box ||| although the [box] isn't large.\",\n",
       "   \"the toy can fit into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "   \"the toy can fit into the bag ||| although the [bag] isn't large.\",\n",
       "   \"the toy can fit into the box ||| although the [box] isn't large.\",\n",
       "   \"the trophy can be put into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "   \"the trophy can be put into the bag ||| although the [bag] isn't large.\",\n",
       "   \"the trophy can be put into the box ||| although the [box] isn't large.\",\n",
       "   \"the ball can be put into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "   \"the ball can be put into the bag ||| although the [bag] isn't large.\",\n",
       "   \"the ball can be put into the box ||| although the [box] isn't large.\",\n",
       "   \"the toy can be put into the suitcase ||| although the [suitcase] isn't large.\",\n",
       "   \"the toy can be put into the bag ||| although the [bag] isn't large.\",\n",
       "   \"the toy can be put into the box ||| although the [box] isn't large.\"],\n",
       "  ['the suitcase can hold the trophy ||| although the [trophy] is large.',\n",
       "   'the bag can hold the trophy ||| although the [trophy] is large.',\n",
       "   'the box can hold the trophy ||| although the [trophy] is large.',\n",
       "   'the suitcase can hold the ball ||| although the [ball] is large.',\n",
       "   'the bag can hold the ball ||| although the [ball] is large.',\n",
       "   'the box can hold the ball ||| although the [ball] is large.',\n",
       "   'the suitcase can hold the toy ||| although the [toy] is large.',\n",
       "   'the bag can hold the toy ||| although the [toy] is large.',\n",
       "   'the box can hold the toy ||| although the [toy] is large.',\n",
       "   'the suitcase has enough room for the trophy ||| although the [trophy] is large.',\n",
       "   'the bag has enough room for the trophy ||| although the [trophy] is large.',\n",
       "   'the box has enough room for the trophy ||| although the [trophy] is large.',\n",
       "   'the suitcase has enough room for the ball ||| although the [ball] is large.',\n",
       "   'the bag has enough room for the ball ||| although the [ball] is large.',\n",
       "   'the box has enough room for the ball ||| although the [ball] is large.',\n",
       "   'the suitcase has enough room for the toy ||| although the [toy] is large.',\n",
       "   'the bag has enough room for the toy ||| although the [toy] is large.',\n",
       "   'the box has enough room for the toy ||| although the [toy] is large.'],\n",
       "  ['the suitcase can hold the trophy ||| although the [suitcase] is small.',\n",
       "   'the bag can hold the trophy ||| although the [bag] is small.',\n",
       "   'the box can hold the trophy ||| although the [box] is small.',\n",
       "   'the suitcase can hold the ball ||| although the [suitcase] is small.',\n",
       "   'the bag can hold the ball ||| although the [bag] is small.',\n",
       "   'the box can hold the ball ||| although the [box] is small.',\n",
       "   'the suitcase can hold the toy ||| although the [suitcase] is small.',\n",
       "   'the bag can hold the toy ||| although the [bag] is small.',\n",
       "   'the box can hold the toy ||| although the [box] is small.',\n",
       "   'the suitcase has enough room for the trophy ||| although the [suitcase] is small.',\n",
       "   'the bag has enough room for the trophy ||| although the [bag] is small.',\n",
       "   'the box has enough room for the trophy ||| although the [box] is small.',\n",
       "   'the suitcase has enough room for the ball ||| although the [suitcase] is small.',\n",
       "   'the bag has enough room for the ball ||| although the [bag] is small.',\n",
       "   'the box has enough room for the ball ||| although the [box] is small.',\n",
       "   'the suitcase has enough room for the toy ||| although the [suitcase] is small.',\n",
       "   'the bag has enough room for the toy ||| although the [bag] is small.',\n",
       "   'the box has enough room for the toy ||| although the [box] is small.'],\n",
       "  [\"the suitcase can hold the trophy ||| although the [trophy] isn't small.\",\n",
       "   \"the bag can hold the trophy ||| although the [trophy] isn't small.\",\n",
       "   \"the box can hold the trophy ||| although the [trophy] isn't small.\",\n",
       "   \"the suitcase can hold the ball ||| although the [ball] isn't small.\",\n",
       "   \"the bag can hold the ball ||| although the [ball] isn't small.\",\n",
       "   \"the box can hold the ball ||| although the [ball] isn't small.\",\n",
       "   \"the suitcase can hold the toy ||| although the [toy] isn't small.\",\n",
       "   \"the bag can hold the toy ||| although the [toy] isn't small.\",\n",
       "   \"the box can hold the toy ||| although the [toy] isn't small.\",\n",
       "   \"the suitcase has enough room for the trophy ||| although the [trophy] isn't small.\",\n",
       "   \"the bag has enough room for the trophy ||| although the [trophy] isn't small.\",\n",
       "   \"the box has enough room for the trophy ||| although the [trophy] isn't small.\",\n",
       "   \"the suitcase has enough room for the ball ||| although the [ball] isn't small.\",\n",
       "   \"the bag has enough room for the ball ||| although the [ball] isn't small.\",\n",
       "   \"the box has enough room for the ball ||| although the [ball] isn't small.\",\n",
       "   \"the suitcase has enough room for the toy ||| although the [toy] isn't small.\",\n",
       "   \"the bag has enough room for the toy ||| although the [toy] isn't small.\",\n",
       "   \"the box has enough room for the toy ||| although the [toy] isn't small.\"],\n",
       "  [\"the suitcase can hold the trophy ||| although the [suitcase] isn't large.\",\n",
       "   \"the bag can hold the trophy ||| although the [bag] isn't large.\",\n",
       "   \"the box can hold the trophy ||| although the [box] isn't large.\",\n",
       "   \"the suitcase can hold the ball ||| although the [suitcase] isn't large.\",\n",
       "   \"the bag can hold the ball ||| although the [bag] isn't large.\",\n",
       "   \"the box can hold the ball ||| although the [box] isn't large.\",\n",
       "   \"the suitcase can hold the toy ||| although the [suitcase] isn't large.\",\n",
       "   \"the bag can hold the toy ||| although the [bag] isn't large.\",\n",
       "   \"the box can hold the toy ||| although the [box] isn't large.\",\n",
       "   \"the suitcase has enough room for the trophy ||| although the [suitcase] isn't large.\",\n",
       "   \"the bag has enough room for the trophy ||| although the [bag] isn't large.\",\n",
       "   \"the box has enough room for the trophy ||| although the [box] isn't large.\",\n",
       "   \"the suitcase has enough room for the ball ||| although the [suitcase] isn't large.\",\n",
       "   \"the bag has enough room for the ball ||| although the [bag] isn't large.\",\n",
       "   \"the box has enough room for the ball ||| although the [box] isn't large.\",\n",
       "   \"the suitcase has enough room for the toy ||| although the [suitcase] isn't large.\",\n",
       "   \"the bag has enough room for the toy ||| although the [bag] isn't large.\",\n",
       "   \"the box has enough room for the toy ||| although the [box] isn't large.\"]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful = [0,2,3,4,5,6,12,14,15,16,18,21,25,26,27,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James ceded the presidency to Amy  because James was notorious.'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex[29][0][0].replace('|||','').replace('[','').replace(']','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = []\n",
    "for i in useful:\n",
    "    target = [i.split(' ')[-1][:-1] for i in ex[i][0][:2]]\n",
    "    task.append({'goal':' '.join(ex[i][0][0].replace('||| ','').replace('[','').replace(']','').split(' ')[:-1]),\n",
    "                'target_true':target[0],\"target_false\":target[1]})\n",
    "    task.append({'goal':' '.join(ex[i][0][1].replace('||| ','').replace('[','').replace(']','').split(' ')[:-1]),\n",
    "                'target_true':target[1],\"target_false\":target[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'goal': \"the trophy doesn't fit into the suitcase because the trophy is\",\n",
       "  'target_true': 'large',\n",
       "  'target_false': 'small'},\n",
       " {'goal': \"the trophy doesn't fit into the suitcase because the suitcase is\",\n",
       "  'target_true': 'small',\n",
       "  'target_false': 'large'},\n",
       " {'goal': 'John gave a lot of money to Susan because John was',\n",
       "  'target_true': 'rich',\n",
       "  'target_false': 'poor'},\n",
       " {'goal': 'John gave a lot of money to Susan because Susan was',\n",
       "  'target_true': 'poor',\n",
       "  'target_false': 'rich'},\n",
       " {'goal': 'the truck overtook the bus because the truck was going',\n",
       "  'target_true': 'fast',\n",
       "  'target_false': 'slow'},\n",
       " {'goal': 'the truck overtook the bus because the bus was going',\n",
       "  'target_true': 'slow',\n",
       "  'target_false': 'fast'},\n",
       " {'goal': 'John beat Susan in the game so John was',\n",
       "  'target_true': 'happy',\n",
       "  'target_false': 'sad'},\n",
       " {'goal': 'John beat Susan in the game so Susan was',\n",
       "  'target_true': 'sad',\n",
       "  'target_false': 'happy'},\n",
       " {'goal': 'the ball crashed right through the board because the ball was',\n",
       "  'target_true': 'hard',\n",
       "  'target_false': 'soft'},\n",
       " {'goal': 'the ball crashed right through the board because the board was',\n",
       "  'target_true': 'soft',\n",
       "  'target_false': 'hard'},\n",
       " {'goal': \"John couldn't see the stage behind Susan because John is\",\n",
       "  'target_true': 'short',\n",
       "  'target_false': 'tall'},\n",
       " {'goal': \"John couldn't see the stage behind Susan because Susan is\",\n",
       "  'target_true': 'tall',\n",
       "  'target_false': 'short'},\n",
       " {'goal': 'the bag of potatoes had been placed above the bag of flour so the bag of potatoes had to be moved',\n",
       "  'target_true': 'first',\n",
       "  'target_false': 'later'},\n",
       " {'goal': 'the bag of potatoes had been placed above the bag of flour so the bag of flour had to be moved',\n",
       "  'target_true': 'later',\n",
       "  'target_false': 'first'},\n",
       " {'goal': 'the bottle was filled with water from the cup after the bottle was',\n",
       "  'target_true': 'empty',\n",
       "  'target_false': 'full'},\n",
       " {'goal': 'the bottle was filled with water from the cup after the cup was',\n",
       "  'target_true': 'full',\n",
       "  'target_false': 'empty'},\n",
       " {'goal': 'Joe can beat Amy at tennis because Joe is',\n",
       "  'target_true': 'older',\n",
       "  'target_false': 'younger'},\n",
       " {'goal': 'Joe can beat Amy at tennis because Amy is',\n",
       "  'target_true': 'younger',\n",
       "  'target_false': 'older'},\n",
       " {'goal': 'Ann asked Henry what time the library closes because Ann had',\n",
       "  'target_true': 'forgotten',\n",
       "  'target_false': 'remembered'},\n",
       " {'goal': 'Ann asked Henry what time the library closes because Henry',\n",
       "  'target_true': 'remembered',\n",
       "  'target_false': 'forgotten'},\n",
       " {'goal': 'Jack always takes care of Betty because Jack is',\n",
       "  'target_true': 'older',\n",
       "  'target_false': 'younger'},\n",
       " {'goal': 'Jack always takes care of Betty because Betty is',\n",
       "  'target_true': 'younger',\n",
       "  'target_false': 'older'},\n",
       " {'goal': 'the sponsors were less in number than the opponents so the sponsors were in the',\n",
       "  'target_true': 'minority',\n",
       "  'target_false': 'majority'},\n",
       " {'goal': 'the sponsors were less in number than the opponents so the opponents were in the',\n",
       "  'target_true': 'majority',\n",
       "  'target_false': 'minority'},\n",
       " {'goal': 'the table will fit through the doorway because the table is',\n",
       "  'target_true': 'narrow',\n",
       "  'target_false': 'wide'},\n",
       " {'goal': 'the table will fit through the doorway because the doorway is',\n",
       "  'target_true': 'wide',\n",
       "  'target_false': 'narrow'},\n",
       " {'goal': 'the sweater is traded by Grace for the jacket because she thinks the sweater looks',\n",
       "  'target_true': 'bad',\n",
       "  'target_false': 'good'},\n",
       " {'goal': 'the sweater is traded by Grace for the jacket because she thinks the jacket looks',\n",
       "  'target_true': 'good',\n",
       "  'target_false': 'bad'},\n",
       " {'goal': 'Bill passed the half-empty plate to Amy because Bill was',\n",
       "  'target_true': 'full',\n",
       "  'target_false': 'hungry'},\n",
       " {'goal': 'Bill passed the half-empty plate to Amy because Amy was',\n",
       "  'target_true': 'hungry',\n",
       "  'target_false': 'full'},\n",
       " {'goal': 'James ceded the presidency to Amy because James was',\n",
       "  'target_true': 'notorious',\n",
       "  'target_false': 'popular'},\n",
       " {'goal': 'James ceded the presidency to Amy because Amy was',\n",
       "  'target_true': 'popular',\n",
       "  'target_false': 'notorious'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                  | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ small or large ] the trophy doesn't fit into the suitcase because the trophy is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██████▊                                                                                                                                                                                                                   | 1/32 [00:00<00:15,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ small or large ] the trophy doesn't fit into the suitcase because the suitcase is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████████████▋                                                                                                                                                                                                            | 2/32 [00:00<00:14,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ poor or rich ] John gave a lot of money to Susan because John was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|████████████████████▍                                                                                                                                                                                                     | 3/32 [00:01<00:13,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ rich or poor ] John gave a lot of money to Susan because Susan was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████████████▎                                                                                                                                                                                              | 4/32 [00:01<00:13,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ slow or fast ] the truck overtook the bus because the truck was going\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████████████████████                                                                                                                                                                                        | 5/32 [00:02<00:12,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ fast or slow ] the truck overtook the bus because the bus was going\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|████████████████████████████████████████▉                                                                                                                                                                                 | 6/32 [00:02<00:12,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ sad or happy ] John beat Susan in the game so John was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|███████████████████████████████████████████████▋                                                                                                                                                                          | 7/32 [00:03<00:11,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ happy or sad ] John beat Susan in the game so Susan was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████████████████████▌                                                                                                                                                                   | 8/32 [00:03<00:11,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ soft or hard ] the ball crashed right through the board because the ball was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████████████████████████████████████████████████████▎                                                                                                                                                            | 9/32 [00:04<00:10,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ hard or soft ] the ball crashed right through the board because the board was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████████████████████████████████████▊                                                                                                                                                     | 10/32 [00:04<00:10,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ short or tall ] John couldn't see the stage behind Susan because John is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████████████████████████████████████████████████████████████████▌                                                                                                                                              | 11/32 [00:05<00:09,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ tall or short ] John couldn't see the stage behind Susan because Susan is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                       | 12/32 [00:05<00:09,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ later or first ] the bag of potatoes had been placed above the bag of flour so the bag of potatoes had to be moved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                | 13/32 [00:06<00:08,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ later or first ] the bag of potatoes had been placed above the bag of flour so the bag of flour had to be moved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                          | 14/32 [00:06<00:08,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ empty or full ] the bottle was filled with water from the cup after the bottle was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                   | 15/32 [00:07<00:07,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ full or empty ] the bottle was filled with water from the cup after the cup was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                            | 16/32 [00:07<00:07,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ younger or older ] Joe can beat Amy at tennis because Joe is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 17/32 [00:07<00:07,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ older or younger ] Joe can beat Amy at tennis because Amy is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                               | 18/32 [00:08<00:06,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ remembered or forgotten ] Ann asked Henry what time the library closes because Ann had\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                        | 19/32 [00:08<00:06,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ remembered or forgotten ] Ann asked Henry what time the library closes because Henry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                 | 20/32 [00:09<00:05,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ younger or older ] Jack always takes care of Betty because Jack is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                          | 21/32 [00:09<00:05,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ younger or older ] Jack always takes care of Betty because Betty is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 22/32 [00:10<00:04,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ minority or majority ] the sponsors were less in number than the opponents so the sponsors were in the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 23/32 [00:10<00:04,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ minority or majority ] the sponsors were less in number than the opponents so the opponents were in the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 24/32 [00:11<00:03,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ wide or narrow ] the table will fit through the doorway because the table is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 25/32 [00:11<00:03,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ wide or narrow ] the table will fit through the doorway because the doorway is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 26/32 [00:12<00:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ good or bad ] the sweater is traded by Grace for the jacket because she thinks the sweater looks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 27/32 [00:12<00:02,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ good or bad ] the sweater is traded by Grace for the jacket because she thinks the jacket looks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 28/32 [00:13<00:01,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ hungry or full ] Bill passed the half-empty plate to Amy because Bill was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                    | 29/32 [00:13<00:01,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ hungry or full ] Bill passed the half-empty plate to Amy because Amy was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 30/32 [00:14<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ notorious or popular ] James ceded the presidency to Amy because James was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 31/32 [00:14<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ popular or notorious ] James ceded the presidency to Amy because Amy was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:15<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "reqs_all = []\n",
    "for doc_id, doc in enumerate(tqdm(task)):\n",
    "    ctx = doc_to_text(doc)\n",
    "    reqs = construct_requests(doc, ctx)\n",
    "    reqs_all.append(reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\n",
      "\n",
      "[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\n",
      "\n",
      "[ small or large ] John gave a lot of money to Susan  because John was\n"
     ]
    }
   ],
   "source": [
    "ctx = doc_to_text(task[2])\n",
    "reqs = construct_requests(task[0],ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in reqs_all:\n",
    "    if i[0][0][2]:\n",
    "        result.append(\"T\")\n",
    "    else:\n",
    "        result.append(\"F\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reqs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[(0.5380859375, 0.446044921875, True)]],\n",
       " [[(0.480712890625, 0.50390625, False)]],\n",
       " [[(0.982421875, 0.0004470348358154297, True)]],\n",
       " [[(0.97119140625, 0.01806640625, True)]],\n",
       " [[(0.55078125, 0.004306793212890625, True)]],\n",
       " [[(0.8359375, 0.0034427642822265625, True)]],\n",
       " [[(0.390625, 0.250244140625, True)]],\n",
       " [[(0.41796875, 0.12744140625, True)]],\n",
       " [[(0.986328125, 0.0026226043701171875, True)]],\n",
       " [[(0.58837890625, 0.216552734375, True)]],\n",
       " [[(0.951171875, 0.00977325439453125, True)]],\n",
       " [[(0.96826171875, 0.02386474609375, True)]],\n",
       " [[(0.72412109375, 0.004306793212890625, True)]],\n",
       " [[(0.006923675537109375, 0.59912109375, False)]],\n",
       " [[(0.058013916015625, 0.11181640625, False)]],\n",
       " [[(0.053192138671875, 0.005828857421875, True)]],\n",
       " [[(0.033416748046875, 0.96142578125, False)]],\n",
       " [[(0.64453125, 0.350341796875, True)]],\n",
       " [[(0.8076171875, 0.1541748046875, True)]],\n",
       " [[(0.150146484375, 0.0013399124145507812, True)]],\n",
       " [[(0.01427459716796875, 0.9853515625, False)]],\n",
       " [[(0.466064453125, 0.52783203125, False)]],\n",
       " [[(0.95458984375, 0.00554656982421875, True)]],\n",
       " [[(0.986328125, 0.004222869873046875, True)]],\n",
       " [[(0.88720703125, 0.10430908203125, True)]],\n",
       " [[(0.95947265625, 0.023651123046875, True)]],\n",
       " [[(0.404541015625, 0.23046875, True)]],\n",
       " [[(0.0660400390625, 0.0110321044921875, True)]],\n",
       " [[(0.96728515625, 0.02874755859375, True)]],\n",
       " [[(0.41748046875, 0.54443359375, False)]],\n",
       " [[(0.98828125, 0.0001361370086669922, True)]],\n",
       " [[(0.31494140625, 0.0288543701171875, True)]]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(string: str):\n",
    "    return tokenizer.encode(string, add_special_tokens=False)\n",
    "def _encode_pair(context, continuation):\n",
    "    n_spaces = len(context) - len(context.rstrip())\n",
    "    if n_spaces > 0:\n",
    "        continuation = context[-n_spaces:] + continuation\n",
    "        context = context[:-n_spaces]\n",
    "    whole_enc = tok_encode(context + continuation)\n",
    "    context_enc = tok_encode(context)\n",
    "    context_enc_len = len(context_enc)\n",
    "    continuation_enc = whole_enc[context_enc_len:]\n",
    "    return context_enc, continuation_enc\n",
    "def _model_call(inps):\n",
    "    \"\"\"\n",
    "    inps: a torch tensor of shape [batch, sequence]\n",
    "    the size of sequence may vary from call to call\n",
    "\n",
    "    returns: a torch tensor of shape [batch, sequence, vocab] with the\n",
    "    logits returned from the model\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return model(inps)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_text(doc):\n",
    "    return doc[\"goal\"]\n",
    "def doc_to_target(doc):\n",
    "    return \" \" + doc['target_true']\n",
    "def construct_requests(doc, ctx):\n",
    "    lls = []\n",
    "#     init_prompt_starter = \"The following are pairs of Winograd Schema in the form of a statement S, a question Q, and an answer A:\"\n",
    "#     init_prompt1 = \"S: The cat went through the door, but it's tail got stuck. Q: In the previous statement, what does 'it' refer to? A: The cat.\"\n",
    "#     init_prompt2 = \"S: The cat tried to go through the door, but it was too small. Q: In the previous statement, what does 'it' refer to? A: The door.\"\n",
    "#     init_prompt3 = \"S: Fedex made more profit than UPS last year, but that was mostly due to the success of the new delivery system they implemented. Q: In the previous statement, what does 'they' refer to? A: Fedex.\"\n",
    "#     init_prompt4 = \"S: Sam tried to buy Xerxes lunch, but he wouldn't allow it. Q: In the previous statement, who does 'he' refer to? A: Xerxes.\"\n",
    "    \n",
    "#     fewshotex = rnd.sample(task_docs, num_shot + 1)\n",
    "#     fewshotex = [x for x in fewshotex if x != doc][:num_shot]\n",
    "# #     # fewshotex = task_docs[:2]\n",
    "#     labeled_examples = (\"\\n\\n\".join([doc_to_text(d) + doc_to_target(d) for d in fewshotex ])+ \"\\n\\n\" )\n",
    "#     input = labeled_examples + ctx \n",
    "#     input =f\"{init_prompt_starter}\\n\\n{init_prompt1}\\n\\n{init_prompt2}\\n\\n{init_prompt3}\\n\\n{init_prompt4}\\n\\n{ctx}\"\n",
    "    init_prompt1 = \"[ large or small ] the trophy doesn't fit into the suitcase  because the trophy is large\"\n",
    "    init_prompt2 = \"[ large or small ] the trophy doesn't fit into the suitcase  because the suitcase is small\"\n",
    "    target = [doc['target_true'],doc['target_false']]\n",
    "    random.shuffle(target)\n",
    "    input =f\"{init_prompt1}\\n\\n{init_prompt2}\\n\\n\"+f\"[ {target[0]} or {target[1]} ] \" + ctx\n",
    "    print(input)\n",
    "    lls.append([loglikelihood(input,doc)[0]])\n",
    "    return lls\n",
    "def loglikelihood(input,doc):\n",
    "    new_reqs = []\n",
    "    context_enc = tok_encode(input)\n",
    "    true_enc,new_enc = tok_encode(doc[\"target_true\"]),tok_encode(doc[\"target_false\"])\n",
    "    new_reqs.append(((input,doc[\"target_true\"],doc[\"target_false\"] ), context_enc, true_enc,new_enc))\n",
    "    return _loglikelihood_tokens(new_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loglikelihood_tokens(requests, max_length =2048 ,device= 3,disable_tqdm=False, override_bs=None):\n",
    "    # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n",
    "    res = []\n",
    "    # for chunk in tqdm(requests):\n",
    "    inps = []\n",
    "    # cont_toks_list = []\n",
    "    inplens = []\n",
    "    padding_length = None\n",
    "    # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n",
    "    # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n",
    "    # again because vectorizing is annoying\n",
    "\n",
    "    for _, context_enc, true_enc, new_enc in requests:\n",
    "        # sanity check\n",
    "        assert len(context_enc) > 0\n",
    "        # assert len(continuation_enc) > 0\n",
    "\n",
    "        # how this all works:\n",
    "        #          CTX      CONT\n",
    "        # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n",
    "        # gpt2    \\               \\\n",
    "        # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n",
    "        # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n",
    "\n",
    "        # when too long to fit in context, truncate from the left\n",
    "        inp = torch.tensor(\n",
    "            (context_enc)[-(max_length + 1) :],\n",
    "            dtype=torch.long,\n",
    "        ).to(device)\n",
    "        (inplen,) = inp.shape\n",
    "\n",
    "        # cont = continuation_enc\n",
    "\n",
    "        # since in _collate we make sure length is descending, the longest is always the first one.\n",
    "        padding_length = (\n",
    "            padding_length if padding_length is not None else inplen\n",
    "        )\n",
    "\n",
    "        # pad length from seq to padding_length\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                inp,  # [seq]\n",
    "                torch.zeros(padding_length - inplen, dtype=torch.long).to(\n",
    "                    inp.device\n",
    "                ),  # [padding_length - seq]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        inps.append(inp.unsqueeze(0))  # [1, padding_length]\n",
    "        # cont_toks_list.append(cont)\n",
    "        inplens.append(inplen)\n",
    "\n",
    "    batched_inps = torch.cat(inps, dim=0)  # [batch, padding_length]\n",
    "    multi_logits = F.softmax(\n",
    "        _model_call(batched_inps), dim=-1\n",
    "    ).cpu()  # [batch, padding_length, vocab]\n",
    "    \n",
    "    for (cache_key,context_enc,true_enc, new_enc), logits, inp, inplen in zip(\n",
    "        requests, multi_logits, inps, inplens\n",
    "    ):\n",
    "\n",
    "        # Slice to original seq length\n",
    "        contlen = 1\n",
    "        inplen = inplen + (logits.shape[0] - padding_length) # if \"virtual tokens\" (from prompt tuning) are added, inplen is larger\n",
    "        logits = logits[inplen - contlen : inplen].unsqueeze(\n",
    "            0\n",
    "        )  # [1, seq, vocab]\n",
    "        final_token = logits[0][-1]\n",
    "        # Check if per-token argmax is exactly equal to continuation\n",
    "        # print(cache_key[1])\n",
    "\n",
    "        if (bool(final_token[true_enc[0]]>final_token[new_enc[0]])) :\n",
    "            max_equal = True\n",
    "        else:\n",
    "            max_equal = False\n",
    "        # greedy_tokens = logits.argmax(dim=-1)\n",
    "        # cont_toks = torch.tensor(cont_toks, dtype=torch.long).unsqueeze(\n",
    "        #     0\n",
    "        # )  # [1, seq]\n",
    "        # max_equal = (greedy_tokens == cont_toks).all()\n",
    "\n",
    "        # Obtain log-probs at the corresponding continuation token indices\n",
    "        # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n",
    "        # logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n",
    "        #     -1\n",
    "        # )  # [1, seq]\n",
    "\n",
    "        # Answer: (log prob, is-exact-match)\n",
    "        answer = (float(final_token[true_enc[0]]),float(final_token[new_enc[0]]), bool(max_equal))\n",
    "\n",
    "        # partial caching\n",
    "        # if cache_key is not None:\n",
    "        #     self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\n",
    "\n",
    "        res.append(answer)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4075554880.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[214], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    a = i++\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "i =0 \n",
    "a = i++\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'goal': \"the trophy doesn't fit into the suitcase because the trophy is\",\n",
       "  'target_true': 'large',\n",
       "  'target_false': 'small'},\n",
       " {'goal': \"the trophy doesn't fit into the suitcase  because the suitcase is\",\n",
       "  'target_true': 'small',\n",
       "  'target_false': 'large'},\n",
       " {'goal': 'John gave a lot of money to Susan because John was',\n",
       "  'target_true': 'rich',\n",
       "  'target_false': 'poor'},\n",
       " {'goal': 'John gave a lot of money to Susan  because Susan was',\n",
       "  'target_true': 'poor',\n",
       "  'target_false': 'rich'},\n",
       " {'goal': 'the truck overtook the bus because the truck was going',\n",
       "  'target_true': 'fast',\n",
       "  'target_false': 'slow'},\n",
       " {'goal': 'the truck overtook the bus  because the bus was going',\n",
       "  'target_true': 'slow',\n",
       "  'target_false': 'fast'},\n",
       " {'goal': 'John beat Susan in the game so John was',\n",
       "  'target_true': 'happy',\n",
       "  'target_false': 'sad'},\n",
       " {'goal': 'John beat Susan in the game  so Susan was',\n",
       "  'target_true': 'sad',\n",
       "  'target_false': 'happy'},\n",
       " {'goal': 'the ball crashed right through the board because the ball was',\n",
       "  'target_true': 'hard',\n",
       "  'target_false': 'soft'},\n",
       " {'goal': 'the ball crashed right through the board  because the board was',\n",
       "  'target_true': 'soft',\n",
       "  'target_false': 'hard'},\n",
       " {'goal': \"John couldn't see the stage behind Susan because John is\",\n",
       "  'target_true': 'short',\n",
       "  'target_false': 'tall'},\n",
       " {'goal': \"John couldn't see the stage behind Susan  because Susan is\",\n",
       "  'target_true': 'tall',\n",
       "  'target_false': 'short'},\n",
       " {'goal': 'the bag of potatoes had been placed above the bag of flour so the bag of potatoes had to be moved',\n",
       "  'target_true': 'first',\n",
       "  'target_false': 'later'},\n",
       " {'goal': 'the bag of potatoes had been placed above the bag of flour  so the bag of flour had to be moved',\n",
       "  'target_true': 'later',\n",
       "  'target_false': 'first'},\n",
       " {'goal': 'the bottle was filled with water from the cup after the bottle was',\n",
       "  'target_true': 'empty',\n",
       "  'target_false': 'full'},\n",
       " {'goal': 'the bottle was filled with water from the cup  after the cup was',\n",
       "  'target_true': 'full',\n",
       "  'target_false': 'empty'},\n",
       " {'goal': 'Joe can beat Amy at tennis because Joe is',\n",
       "  'target_true': 'older',\n",
       "  'target_false': 'younger'},\n",
       " {'goal': 'Joe can beat Amy at tennis  because Amy is',\n",
       "  'target_true': 'younger',\n",
       "  'target_false': 'older'},\n",
       " {'goal': 'Ann asked Henry what time the library closes because Ann had',\n",
       "  'target_true': 'forgotten',\n",
       "  'target_false': 'remembered'},\n",
       " {'goal': 'Ann asked Henry what time the library closes  because Henry',\n",
       "  'target_true': 'remembered',\n",
       "  'target_false': 'forgotten'},\n",
       " {'goal': 'Jack always takes care of Betty because Jack is',\n",
       "  'target_true': 'older',\n",
       "  'target_false': 'younger'},\n",
       " {'goal': 'Jack always takes care of Betty  because Betty is',\n",
       "  'target_true': 'younger',\n",
       "  'target_false': 'older'},\n",
       " {'goal': 'the sponsors were less in number than the opponents so the sponsors were in the',\n",
       "  'target_true': 'minority',\n",
       "  'target_false': 'majority'},\n",
       " {'goal': 'the sponsors were less in number than the opponents  so the opponents were in the',\n",
       "  'target_true': 'majority',\n",
       "  'target_false': 'minority'},\n",
       " {'goal': 'the table will fit through the doorway because the table is',\n",
       "  'target_true': 'narrow',\n",
       "  'target_false': 'wide'},\n",
       " {'goal': 'the table will fit through the doorway  because the doorway is',\n",
       "  'target_true': 'wide',\n",
       "  'target_false': 'narrow'},\n",
       " {'goal': 'the sweater is traded by Grace for the jacket because she thinks the sweater looks',\n",
       "  'target_true': 'bad',\n",
       "  'target_false': 'good'},\n",
       " {'goal': 'the sweater is traded by Grace for the jacket  because she thinks the jacket looks',\n",
       "  'target_true': 'good',\n",
       "  'target_false': 'bad'},\n",
       " {'goal': 'Bill passed the half-empty plate to Amy because Bill was',\n",
       "  'target_true': 'full',\n",
       "  'target_false': 'hungry'},\n",
       " {'goal': 'Bill passed the half-empty plate to Amy  because Amy was',\n",
       "  'target_true': 'hungry',\n",
       "  'target_false': 'full'},\n",
       " {'goal': 'James ceded the presidency to Amy because James was',\n",
       "  'target_true': 'notorious',\n",
       "  'target_false': 'popular'},\n",
       " {'goal': 'James ceded the presidency to Amy  because Amy was',\n",
       "  'target_true': 'popular',\n",
       "  'target_false': 'notorious'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (833021138.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    git clone https://github.com/redwoodresearch/Easy-Transformer.git\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/redwoodresearch/Easy-Transformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in task:\n",
    "    data.append(i['goal']+' '+i['target_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the trophy doesn't fit into the suitcase because the trophy is large\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Winograd:\n",
    "    def __init__(self,text):\n",
    "        self.text =text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Winograd(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'winograd' has no attribute 'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a\u001b[38;5;241m.\u001b[39mtoken\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'winograd' has no attribute 'token'"
     ]
    }
   ],
   "source": [
    "a.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('winoList.txt', 'w')  \n",
    "for ip in data:  \n",
    "    fileObject.write(ip)  \n",
    "    fileObject.write('\\n')  \n",
    "fileObject.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
