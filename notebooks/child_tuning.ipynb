{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7292808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "54a886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce, partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from transformers.trainer_utils import EvaluationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "id": "49c8f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _\n",
    "from collections import namedtuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "58cba5e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}\n",
    "# cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "for model_name in ['gpt2-large']:#, 'gpt2-xl', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name not in models:\n",
    "        with Timer(model_name):\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)  \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "fd0bbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_model(engine):\n",
    "    def forward(input_ids, attention_mask=None):\n",
    "        text = tokenizer.decode(input_ids[0])\n",
    "        response = openai.Completion.create(engine=engine, prompt=text, max_tokens=0, echo=True, logprobs=5)\n",
    "        return Outputs(logits=response.choices[0].logprobs)\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "ebbfc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large\n",
      "gpt2-xl\n",
      "EleutherAI/gpt-neo-1.3B\n",
      "EleutherAI/gpt-j-6B\n",
      "KoboldAI/fairseq-dense-6.7B\n",
      "KoboldAI/fairseq-dense-13B\n",
      "text-babbage-001\n",
      "text-curie-001\n",
      "text-davinci-001\n",
      "text-davinci-002\n"
     ]
    }
   ],
   "source": [
    "for model_name in models: print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "4427a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = ['text-babbage-001', 'text-curie-001', 'text-davinci-001', 'text-davinci-002']\n",
    "for engine in engines:\n",
    "    model = get_openai_model(engine)\n",
    "    models[engine] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "c64283f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult young. Saturday -> Friday\n",
      "microgram nanogram. doctor -> master\n",
      "master bachelor. micrometer -> nanometer\n",
      "c d. large -> huge\n",
      "modern renaissance. doctor -> master\n",
      "g h. child -> teenager\n",
      "planet continent. h -> g\n",
      "d e. microgram -> milligram\n",
      "magnitude percent. large -> huge\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = open('/nas/xd/projects/openai_api_keys.txt').readlines()[-1].split()[0]\n",
    "# text = 'Once upon a time'\n",
    "response = openai.Completion.create(engine=engines[1], prompt=text, max_tokens=20, echo=True, logprobs=5)\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052c1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = 'Ġ!'; prompt_id = tokenizer._convert_token_to_id(prompt_token)\n",
    "bop_str = 'Instruction: '; bop_id = tokenizer.encode(bop_str)[0]  # 'Inst'\n",
    "eop_str = '. For example:'; eop_id = tokenizer.encode(eop_str)[2] # 'Ġexample'\n",
    "bos_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "eos_id = tokenizer._convert_token_to_id('Ċ')\n",
    "\n",
    "\n",
    "class CHILDDataset(Dataset):\n",
    "    def __init__(self, input_strs, tokenizer):\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = '!'\n",
    "        self.inputs = tokenizer.batch_encode_plus(input_strs, add_special_tokens=False, padding=True, return_tensors='pt')\n",
    "        input_ids = self.inputs.input_ids\n",
    "        self.labels = torch.ones_like(input_ids) * (-100)\n",
    "        for bi in range(input_ids.size(0)):\n",
    "            bop_idx = (input_ids[bi] == bop_id).nonzero().squeeze(1)\n",
    "            eop_idx = (input_ids[bi] == eop_id).nonzero().squeeze(1)\n",
    "            if len(bop_idx) > 0:\n",
    "                assert len(bop_idx) == 1 and len(eop_idx) == 1\n",
    "                bop_idx, eop_idx = bop_idx.item(), eop_idx.item()\n",
    "                input_ids[bi, bop_idx: eop_idx + 2] *= -1  # use prompt embedding for prompt tokens\n",
    "            \n",
    "            bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "            eos_indices = (input_ids[bi] == eos_id).nonzero()[-len(bos_indices):].squeeze(1)\n",
    "            for i, (bos_i, eos_i) in enumerate(zip(bos_indices.tolist(), eos_indices.tolist())):\n",
    "                assert eos_i > bos_i + 1\n",
    "                if i >= 2: self.labels[bi, bos_i + 1: eos_i] = input_ids[bi, bos_i + 1: eos_i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(sel f.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.inputs['input_ids'][i],\n",
    "                'attention_mask': self.inputs['attention_mask'][i],\n",
    "                'labels': self.labels[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "56fcd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                prompt_id: int = None,\n",
    "                prompt_len: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(WrappedEmbedding, self).__init__()\n",
    "#         self.wte = wte\n",
    "#         self.prompt_id = prompt_id\n",
    "#         self.prompt_len = prompt_len\n",
    "        self.__dict__.update(locals()); del self.self\n",
    "        if self.prompt_id is not None:\n",
    "            self.prompt_embedding = nn.parameter.Parameter(\n",
    "                self.initialize_embedding(random_range, initialize_from_vocab)).to(self.wte.weight.device)\n",
    "        else:\n",
    "            self.prompt_embedding = nn.Embedding(self.prompt_len, self.wte.weight.size(1)).to(self.wte.weight.device)\n",
    "            assert initialize_from_vocab\n",
    "            self.init_prompt_embedding_()\n",
    "#             self.prompt_embedding.weight.data = self.initialize_embedding(random_range, initialize_from_vocab)     \n",
    "            \n",
    "    def initialize_embedding(self, random_range: float = 0.5, initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab: return self.wte.weight[:self.prompt_len].clone().detach()\n",
    "        return torch.FloatTensor(self.prompt_len, self.wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "    \n",
    "    def init_prompt_embedding_(self):\n",
    "        self.prompt_embedding.weight.data[:] = self.wte.weight[:self.prompt_len]\n",
    "            \n",
    "    def forward(self, input_ids):\n",
    "        if self.prompt_id is not None:\n",
    "            input_embeds = self.wte(input_ids)\n",
    "            input_embeds[input_ids == self.prompt_id] = self.prompt_embedding.expand(input_embeds.size(0), -1, -1)\n",
    "        else: # adapted from cpm-2\n",
    "            prompt_mask = input_ids < 0\n",
    "            prompt_ids = -input_ids * prompt_mask\n",
    "            assert torch.all(prompt_ids < self.prompt_len)\n",
    "            p_embeds = self.prompt_embedding(prompt_ids) * prompt_mask.float().unsqueeze(-1)\n",
    "            input_ids = input_ids * ~prompt_mask\n",
    "            w_embeds = self.wte(input_ids) * (~prompt_mask).float().unsqueeze(-1)\n",
    "            input_embeds = w_embeds + p_embeds\n",
    "        return input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b17b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from cpm-2: https://github.com/TsinghuaAI/CPM-2-Finetune/blob/master/utils.py#L133-L164\n",
    "def get_params_for_prompt_optimization(module: nn.Module):\n",
    "    params = []\n",
    "    for t in module.named_modules():\n",
    "        if \"prompt_embedding\" in t[0]:\n",
    "            params.append({'params': [p for p in list(t[1]._parameters.values()) if p is not None]})\n",
    "    for t in module.named_parameters():\n",
    "        if \"prompt\" not in t[0]:\n",
    "            t[1].requires_grad_(False)    \n",
    "    return params\n",
    "\n",
    "def create_optimizer(model, training_args):\n",
    "    from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "    while isinstance(model, (DDP, )): model = model.module\n",
    "    we.init_prompt_embedding_()\n",
    "    param_groups = get_params_for_prompt_optimization(model)\n",
    "    optimizer = AdamW(param_groups, lr=training_args.learning_rate, \n",
    "                      betas=(training_args.adam_beta1, training_args.adam_beta2),eps=training_args.adam_epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "39b4f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = model.get_input_embeddings()\n",
    "if hasattr(wte, 'wte'): wte = wte.wte  # already been wrapped\n",
    "we = WrappedEmbedding(wte, prompt_len=10000)\n",
    "model.set_input_embeddings(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ae3bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalize(obj):\n",
    "    if type(obj) == bool: return 'Yes' if obj else 'No'\n",
    "    return str(obj)\n",
    "    \n",
    "def make_query_str(instruction, query):\n",
    "    if instruction is None and query is None: return ''\n",
    "    s = '.'\n",
    "    if instruction is not None: s = s + ' ' + instruction\n",
    "    if query is not None:\n",
    "        if type(query) in [int, bool, str]: query = [query]\n",
    "        if type(query) == dict:\n",
    "    #         return '. ' + '{' + ','.join([' %s: %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "            s = s + ' ' + '{' + ','.join([' replace %s with %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "        elif type(query) in [list,]:\n",
    "            s = s + ' ' + ' '.join([str(i) for i in query])\n",
    "    return s\n",
    "\n",
    "def make_example_str(example, with_instruction=False):\n",
    "    instruction, l, query, ans = example\n",
    "    if type(ans) not in [Sequence, list]: ans = [ans]\n",
    "    ans = [verbalize(a) for a in ans]\n",
    "    return '%s -> %s' % (' '.join(l) + make_query_str(instruction if with_instruction else None, query), ' '.join(ans))\n",
    "\n",
    "def sample_rand_len(vocab, k): return sample(vocab, k=randint(1, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "5d1d0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _str(l, sep=' : '):\n",
    "#     if l is None: return ''\n",
    "#     if isinstance(l, str) or not isinstance(l, collections.abc.Iterable): l = [l]\n",
    "#     l = [e for e in l if not my_isinstance(e, Sequence)] #type(e).__name__ != 'Sequence']\n",
    "#     if isinstance(l, (dict, OrderedDict)): l = [f'{k}: {v}' for k, v in l.items()]\n",
    "#     return sep.join(str(i) for i in l)\n",
    "\n",
    "# def options2str(options): return '[' + ' | '.join(options) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "1cbf4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _str(l, sep=' '):\n",
    "    if l is None: return ''\n",
    "    if isinstance(l, str) or not isinstance(l, collections.abc.Iterable): l = [l]\n",
    "    l = [e for e in l if not my_isinstance(e, Sequence)] #type(e).__name__ != 'Sequence']\n",
    "    if isinstance(l, (dict, OrderedDict)): l = [f'{k}: {v}' for k, v in l.items()]\n",
    "    return sep.join(str(i) for i in l)\n",
    "\n",
    "def options2str(options): return '[' + ' | '.join(options) + ']'\n",
    "# def options2str(options): return ' or '.join(options) + '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "id": "df63ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptize(s):\n",
    "#     return prompt_token * len(s.split())\n",
    "    return bop_str + s + eop_str\n",
    "    \n",
    "def make_examples(task, nrows=4, ncols=4, full_vocab=None):\n",
    "    if full_vocab is None: full_vocab = string.ascii_uppercase + string.digits\n",
    "    transform_fn, vocab_fn, sample_fn, query_fn = task[:4]\n",
    "    # instruction = transform_fn.__name__.replace('_', ' ')\n",
    "    if vocab_fn is None: vocab_fn = lambda: full_vocab\n",
    "    if query_fn is None: query_fn = lambda *_: (None, None)\n",
    "\n",
    "    examples = []\n",
    "    qa_set = set() # for dedup\n",
    "    for i in range(nrows):\n",
    "        vocab = vocab_fn()\n",
    "        cxt = sample_fn(vocab)#, k=ncols)\n",
    "        query, options = query_fn(cxt, vocab)#, ncols)\n",
    "        ans = transform_fn(cxt, query)\n",
    "        if (query, ans) not in qa_set:\n",
    "            qa_set.add((query, ans))\n",
    "            examples.append([cxt, query, options, ans])\n",
    "    return examples\n",
    "\n",
    "def make_input_str(task, examples, options_position=None):\n",
    "    task += (_str,) * (4 + 3 - len(task))\n",
    "    cxt2str, query2str, ans2str = task[4:]\n",
    "\n",
    "    def example2str(example, with_instruction=False):\n",
    "        cxt, query, options, ans = example\n",
    "        strs = [cxt2str(cxt), query2str(query)]\n",
    "        if options_position is not None: strs.insert(options_position, options2str(options))\n",
    "        # strs = [options2str(options)] + strs if options_position == 'pre' else strs + [options2str(options)]\n",
    "        return '. '.join(s for s in strs if s != '') + ' -> ' + ans2str(ans)\n",
    "\n",
    "    desc = promptize(instruction) + '\\n' if False else ''\n",
    "    text = '\\n'.join(example2str(e) for e in examples)\n",
    "    text = desc + text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "id": "7a87572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_element(cxt, query=None): return seq(cxt).slice(1, 2)\n",
    "def besides(cxt, query): return seq(cxt).difference(query)[0]\n",
    "# def besides_query(cxt, vocab): return cxt.a(sample, 2), cxt.list()\n",
    "def get_poset(e): return tuple([p for p in posets if e in p][0])\n",
    "def special(cxt, query): return seq(cxt).group_by(get_poset).map(_[1]).find(lambda x: len(x) == 1)[0]\n",
    "# def special_cxt(vocab, k=3): sample(vocab[0], k - 1) + sample(vocab[1], 1)\n",
    "\n",
    "def after_query(r, p):\n",
    "    e = r.dom().init().a(choice)\n",
    "    options = r.image(e).map(beside)[0].a(sample, 2)\n",
    "    return e, options\n",
    "\n",
    "def before_query(r, p):\n",
    "    # e = r.dom().tail().a(choice)\n",
    "    e = choice(r.dom().init().tail().list())\n",
    "    options = r.image(e).map(beside)[0].a(sample, 2)\n",
    "    return e, options\n",
    "\n",
    "def after(r, q): return r.image(q).map(next())[0]\n",
    "def before(r, q): return r.image(q).map(prev())[0]\n",
    "def between(r, q): \n",
    "    return r.image(q[0]).map(nexts)[0].intersection(r.image(q[1]).map(prevs)[0]).union(\n",
    "        r.image(q[0]).map(prevs)[0].intersection(r.image(q[1]).map(nexts)[0]))\n",
    "    \n",
    "def monotone_map_cxt(vocab):\n",
    "    P, p = vocab\n",
    "    R = p2r(P)\n",
    "    E1 = R.dom().init().tail().a(choice)\n",
    "    E2 = R.image(E1).map(beside)[0].a(choice)\n",
    "    return R, E1, E2\n",
    "\n",
    "def monotone_map_query(cxt, vocab):\n",
    "    P, p = vocab\n",
    "    r = p2r(p)\n",
    "    e1 = r.dom().init().tail().a(choice)\n",
    "    options = r.image(e1).map(beside)[0]\n",
    "    return (r, e1), options\n",
    "\n",
    "def monotone_map(cxt, query, reverse=False):\n",
    "    R, E1, E2 = cxt\n",
    "    r, e1 = query\n",
    "    return r.image(e1).map(\n",
    "        seq([prev(), next()]).find(lambda f: (E2 in R.image(E1).map(f)[0]) != reverse)  # reverse = not in. too tricky\n",
    "    )[0]\n",
    "    \n",
    "tasks = [\n",
    "    (ith_element, None, partial(sample, k=3), None),\n",
    "    (besides, None, partial(sample, k=3), lambda cxt, vocab: (sample(cxt, 2), cxt)),\n",
    "    (special, lambda: sample(posets[1:3], 2), lambda vocab: sample(sample(vocab[0], 2) + sample(vocab[1], 1), 2 + 1), None),\n",
    "    \n",
    "    (after, lambda: choice(closed_posets), p2r, after_query, lambda r: ''),\n",
    "    (before, lambda: choice(closed_posets), p2r, before_query, lambda r: ''),\n",
    "    (between, lambda: choice(posets), p2r, lambda r, p: r.image(r.dom().init().tail().a(choice)).map(beside)[0].a(sample, 2), lambda r: ''),\n",
    "    (partial(monotone_map, reverse=False), lambda: sample(posets, 2), monotone_map_cxt, monotone_map_query),\n",
    "    (partial(monotone_map, reverse=True), lambda: sample(closed_posets, 2), monotone_map_cxt, monotone_map_query),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "id": "298d0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons = ['triangle', 'quadrangle', 'pentagon', 'hexagon', 'heptagon', 'octagon', 'nonagon', 'decagon',]# 'undecagon', 'dodecagon']\n",
    "times_of_day = ['dawn', 'morning', 'noon', 'afternoon', 'evening', 'night',]# 'midnight']\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "seasons = ['spring', 'summer', 'autumn', 'winter']\n",
    "# ages_of_life = ['baby', 'child', 'teenager', 'young', 'adult', 'elder']\n",
    "ages_of_life = ['baby', 'child', 'adolescent', 'adult']\n",
    "times_of_history = ['ancient', 'medieval', 'modern', 'contemporary'] #'renaissance', \n",
    "# units_of_time = ['nanosecond', 'microsecond', 'millisecond', ][:0] + ['second', 'minute', 'hour', 'day', 'week', 'month', 'year', 'decade', 'century', 'millennium'] # first 3 multi-token\n",
    "units_of_length = ['nanometer', 'micrometer', 'millimeter', 'meter', 'kilometer', 'mile']\n",
    "units_of_mass = ['nanogram', 'microgram', 'milligram', 'gram', 'kilogram', 'ton']\n",
    "# SI_prefixes_small = ['pico', 'nana', 'micro', 'milli', 'centi', 'deci']\n",
    "# SI_prefixes_large = ['kilo', 'mega', 'giga', 'tera', 'peta', 'exa', 'zetta', 'yotta']\n",
    "\n",
    "things = ['atom', 'molecule', 'cell', 'tissue', 'organ', 'system', 'person', 'community', 'city', 'state', 'country', 'continent', 'planet', 'star', 'galaxy', 'universe']\n",
    "sizes = ['tiny', 'small', 'large', 'huge',]# 'medium', 'gigantic']\n",
    "# degrees = ['bachelor', 'master', 'doctor', 'postdoc']\n",
    "posets = [list(string.ascii_uppercase)[:14], list(string.ascii_lowercase)[:14], list(string.ascii_uppercase)[14:], list(string.ascii_lowercase)[14:], digits, cardinals, ordinals,\n",
    "    times_of_day, days_of_week, months, seasons, ages_of_life, times_of_history, #units_of_time, \n",
    "    things, sizes]# units_of_length, units_of_mass, SI_prefixes_small, SI_prefixes_large]\n",
    "closed_posets = [list(string.ascii_uppercase)[:], list(string.ascii_lowercase)[:], digits, cardinals, ordinals, \n",
    "    days_of_week, months, ]#seasons, times_of_history, ages_of_life, sizes]\n",
    "open_posets = [times_of_day, ages_of_life, times_of_history, units_of_length, units_of_mass, things, sizes, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "id": "4059d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saturday -> Friday\n",
      "2 -> 1\n",
      "M -> L\n",
      "V -> U\n",
      "July -> June\n",
      "I -> H\n",
      "6 -> 5\n",
      "k -> j\n",
      "seven -> six\n",
      "February -> January\n",
      "Wednesday -> Tuesday\n",
      "Y -> X\n",
      "seventh -> sixth\n",
      "y -> x\n",
      "third -> second\n",
      "5 -> 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task = tasks[4]\n",
    "# _examples = make_examples(task, nrows=16)\n",
    "\n",
    "text = make_input_str(task, _examples, options_position=None)\n",
    "bos_token, eos_token ='Ġ->', 'Ċ'\n",
    "examples = text.strip().split('\\n')\n",
    "k_shot = 3\n",
    "print(text)\n",
    "inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "input_ids = inputs.input_ids\n",
    "options_ids = [[tokenizer.encode(' ' + option)[0] for option in options] \n",
    "    for cxt, query, options, ans in _examples if options if not None]\n",
    "qlen = input_ids.size(1)\n",
    "# tokenize without tokenization artifact -> needed for visualization, from unseal\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens = list(map(tokenizer.convert_tokens_to_string, map(lambda x: [x], tokens))) \n",
    "\n",
    "bos_indices, eos_indices, answers, labels = locate_answers(input_ids, tokenizer)\n",
    "labels[:, :bos_indices[k_shot]] = -100  # 只算k_shot个示例后的loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "id": "fbc67151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neo-1.3B ... done 0:00:00\n",
      "  U 0.014 {' V': 0.237, ' W': 0.187, ' M': 0.065} \t V -> U\n",
      "  June 0.044 {' August': 0.194, ' July': 0.109, ' June': 0.044} \t July -> June\n",
      "  H 0.056 {' I': 0.219, ' II': 0.106, ' J': 0.06} \t I -> H\n",
      "* 5 0.709 {' 5': 0.709, ' 4': 0.049, ' 6': 0.049} \t 6 -> 5\n",
      "  j 0.004 {' l': 0.264, ' k': 0.129, ' 4': 0.06} \t k -> j\n",
      "* six 0.658 {' six': 0.658, ' seven': 0.117, ' eight': 0.06} \t seven -> six\n",
      "* January 0.389 {' January': 0.389, ' February': 0.376, ' jan': 0.031} \t February -> January\n",
      "* Tuesday 0.396 {' Tuesday': 0.396, ' Wednesday': 0.346, ' Thursday': 0.131} \t Wednesday -> Tuesday\n",
      "  X 0.028 {' N': 0.191, ' Z': 0.162, ' M': 0.05} \t Y -> X\n",
      "  sixth 0.404 {' seventh': 0.427, ' sixth': 0.404, ' eighth': 0.054} \t seventh -> sixth\n",
      "  x 0.062 {' y': 0.132, ' z': 0.123, ' e': 0.094} \t y -> x\n",
      "* second 0.553 {' second': 0.553, ' third': 0.394, ' two': 0.014} \t third -> second\n",
      "* 4 0.986 {' 4': 0.986, ' 3': 0.004, ' 5': 0.002} \t 5 -> 4\n",
      "tensor(2.0254, grad_fn=<NllLossBackward>)\n",
      "EleutherAI/gpt-j-6B ... done 0:00:01\n",
      "  U 0.047 {' A': 0.087, ' W': 0.076, ' N': 0.058} \t V -> U\n",
      "  June 0.152 {' August': 0.488, ' June': 0.152, ' September': 0.062} \t July -> June\n",
      "  H 0.027 {' O': 0.441, ' J': 0.071, ' E': 0.053} \t I -> H\n",
      "* 5 0.414 {' 5': 0.414, ' 7': 0.094, ' 4': 0.082} \t 6 -> 5\n",
      "* j 0.168 {' j': 0.168, ' h': 0.136, ' g': 0.074} \t k -> j\n",
      "* six 0.409 {' six': 0.409, ' eight': 0.241, ' seven': 0.043} \t seven -> six\n",
      "* January 0.696 {' January': 0.696, ' March': 0.113, ' February': 0.056} \t February -> January\n",
      "* Tuesday 0.817 {' Tuesday': 0.817, ' Thursday': 0.132, ' Monday': 0.018} \t Wednesday -> Tuesday\n",
      "* X 0.358 {' X': 0.358, ' N': 0.141, ' Z': 0.084} \t Y -> X\n",
      "* sixth 0.834 {' sixth': 0.834, ' six': 0.068, ' eighth': 0.035} \t seventh -> sixth\n",
      "* x 0.956 {' x': 0.956, ' z': 0.012, ' X': 0.008} \t y -> x\n",
      "* second 0.873 {' second': 0.873, ' first': 0.098, ' fourth': 0.006} \t third -> second\n",
      "* 4 0.82 {' 4': 0.82, ' 3': 0.066, ' 1': 0.045} \t 5 -> 4\n",
      "tensor(1.0985, grad_fn=<NllLossBackward>)\n",
      "1.561967134475708\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for model_name, (model, tokenizer) in models.items():\n",
    "    if any(model_name.startswith(s) for s in ['gpt2-', 'KoboldAI/fairseq-dense', 'text-davinci-001', ]): continue\n",
    "    if not isinstance(model, types.FunctionType): _ = model.eval()\n",
    "    with Timer(model_name): outputs = model(**inputs)\n",
    "    options_ids_list = [[tokenizer.encode(' ' + option)[0] for option in options] for cxt, query, options, ans in _examples]\n",
    "    mask_logits_fn = partial(mask_logits, indices=bos_indices, kept_ids=options_ids_list)\n",
    "    loss, _ = show_predictions(text, examples, tokenizer, outputs.logits, bos_indices, eos_indices, answers, labels,\n",
    "                    mask_logits_fn=None, topk=3, loss_reduction='mean', show_range=range(k_shot, len(examples)), sep='\\t')\n",
    "    print(loss)\n",
    "    losses.append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "    if model_name == 'EleutherAI/gpt-j-6B': break\n",
    "print(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "28fd5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2r(p): p = seq(p); return p.zip(p.inits().zip(p.tails()))#.slice(1, p.len() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "bc90b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_functions = [prev(), next()]\n",
    "rel_fns = [prevs, nexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3c6cab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour(direction, k=1): return lambda x: x[direction][k]\n",
    "def prev(k=1): return neighbour(0, k)\n",
    "def next(k=1): return neighbour(1, k)\n",
    "prevs, nexts = _[0][1:], _[1][1:]\n",
    "beside = lambda x: (x[0][1], x[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834550",
   "metadata": {},
   "source": [
    "**TODO: read children books for more posets**  \n",
    "**TODO: Prompt gpt3 to elicit the posets it knows**  \n",
    "$x \\to f(x)$ where $f \\in \\{\\text{prev/next in posets of numbers/letters/months/days, antonym, hypernym, hyponym, ...}\\}$  \n",
    "$x \\to f^2(x)$  \n",
    "one poset or mixed posets  \n",
    "$x, f(x).~y \\to Ff^{[-1]}(y)$ one poset or mixed posets  \n",
    "$x, f^k(x).~y \\to Ff^{[-1]}(y)~/Ff^{[-]k}(y)$  \n",
    "$x, f(f(x))~/f(f(x)), x \\to f(x)$ in between, the simplest form of sequence completion  \n",
    "$x, f(x) \\to Gf$ where $Gf \\in \\{<, >\\}$  \n",
    "$x, f(x); y, g(y) \\to Ff \\stackrel{?}{=} g^{[-1]}$ where $\\text{output} \\in \\{\\text{True}, \\text{False}\\}$  \n",
    "sort\n",
    "\n",
    "There is a *natural* monotone map/functor $F$ between posets/sets $A$ and $B$.  Compose the computation (set operations, sorting etc.) between $A$ and $B$ with $F$ to make harder tasks.  \n",
    "$P(A) ,P(B) \\to F(P(A)) \\setminus ~/ \\cap ~/ \\triangle P(B)$. Harder form of set difference/intersection.  \n",
    "$P(A) \\to F(\\text{sorted}(P(A)))$. Harder form of sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "17373019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: replace with the other. For example:\n",
      "G H G G G -> H G H H H\n",
      "I I I I M -> M M M M I\n",
      "A A F A A -> F F A F F\n",
      "9 9 9 I I -> I I I 9 9\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "V Q Q V V -> Q V V Q Q\n",
      "G L L G L -> L G G L G\n",
      "G 2 2 2 G -> 2 G G G 2\n",
      "I I Z Z Z -> Z Z I I I\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "R H H H R -> H R R R H\n",
      "B 9 9 B B -> 9 B B 9 9\n",
      "D 2 2 2 D -> 2 D D D 2\n",
      "A A A A W -> W W W W A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_total, n_valid = 192, 64\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s.count('Yes') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6ebf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,\n",
    "    lr_scheduler_type='constant', learning_rate=5e-3, num_train_epochs=4,\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,\n",
    "    no_cuda=True, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
