{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7292808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from transformers.trainer_utils import EvaluationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c8f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _\n",
    "from collections import namedtuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "58cba5e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "for model_name in ['gpt2-large']:#, 'gpt2-xl', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name not in models:\n",
    "        with Timer(model_name):\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)  \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c64283f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", there was a little girl who loved to read. She would spend hours upon hours reading books of\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# openai.api_key = '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'\n",
    "openai.api_key = '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'\n",
    "text = 'Once upon a time'\n",
    "engines = ['davinci', 'text-davinci-002']\n",
    "response = openai.Completion.create(engine=engines[1], prompt=text, temperature=0.2, max_tokens=20, echo=False, logprobs=10)\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052c1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = 'Ġ!'; prompt_id = tokenizer._convert_token_to_id(prompt_token)\n",
    "bop_str = 'Instruction: '; bop_id = tokenizer.encode(bop_str)[0]  # 'Inst'\n",
    "eop_str = '. For example:'; eop_id = tokenizer.encode(eop_str)[2] # 'Ġexample'\n",
    "bos_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "eos_id = tokenizer._convert_token_to_id('Ċ')\n",
    "\n",
    "\n",
    "class CHILDDataset(Dataset):\n",
    "    def __init__(self, input_strs, tokenizer):\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = '!'\n",
    "        self.inputs = tokenizer.batch_encode_plus(input_strs, add_special_tokens=False, padding=True, return_tensors='pt')\n",
    "        input_ids = self.inputs.input_ids\n",
    "        self.labels = torch.ones_like(input_ids) * (-100)\n",
    "        for bi in range(input_ids.size(0)):\n",
    "            bop_idx = (input_ids[bi] == bop_id).nonzero().squeeze(1)\n",
    "            eop_idx = (input_ids[bi] == eop_id).nonzero().squeeze(1)\n",
    "            if len(bop_idx) > 0:\n",
    "                assert len(bop_idx) == 1 and len(eop_idx) == 1\n",
    "                bop_idx, eop_idx = bop_idx.item(), eop_idx.item()\n",
    "                input_ids[bi, bop_idx: eop_idx + 2] *= -1  # use prompt embedding for prompt tokens\n",
    "            \n",
    "            bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "            eos_indices = (input_ids[bi] == eos_id).nonzero()[-len(bos_indices):].squeeze(1)\n",
    "            for i, (bos_i, eos_i) in enumerate(zip(bos_indices.tolist(), eos_indices.tolist())):\n",
    "                assert eos_i > bos_i + 1\n",
    "                if i >= 2: self.labels[bi, bos_i + 1: eos_i] = input_ids[bi, bos_i + 1: eos_i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(sel f.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.inputs['input_ids'][i],\n",
    "                'attention_mask': self.inputs['attention_mask'][i],\n",
    "                'labels': self.labels[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "56fcd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                prompt_id: int = None,\n",
    "                prompt_len: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(WrappedEmbedding, self).__init__()\n",
    "#         self.wte = wte\n",
    "#         self.prompt_id = prompt_id\n",
    "#         self.prompt_len = prompt_len\n",
    "        self.__dict__.update(locals()); del self.self\n",
    "        if self.prompt_id is not None:\n",
    "            self.prompt_embedding = nn.parameter.Parameter(\n",
    "                self.initialize_embedding(random_range, initialize_from_vocab)).to(self.wte.weight.device)\n",
    "        else:\n",
    "            self.prompt_embedding = nn.Embedding(self.prompt_len, self.wte.weight.size(1)).to(self.wte.weight.device)\n",
    "            assert initialize_from_vocab\n",
    "            self.init_prompt_embedding_()\n",
    "#             self.prompt_embedding.weight.data = self.initialize_embedding(random_range, initialize_from_vocab)     \n",
    "            \n",
    "    def initialize_embedding(self, random_range: float = 0.5, initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab: return self.wte.weight[:self.prompt_len].clone().detach()\n",
    "        return torch.FloatTensor(self.prompt_len, self.wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "    \n",
    "    def init_prompt_embedding_(self):\n",
    "        self.prompt_embedding.weight.data[:] = self.wte.weight[:self.prompt_len]\n",
    "            \n",
    "    def forward(self, input_ids):\n",
    "        if self.prompt_id is not None:\n",
    "            input_embeds = self.wte(input_ids)\n",
    "            input_embeds[input_ids == self.prompt_id] = self.prompt_embedding.expand(input_embeds.size(0), -1, -1)\n",
    "        else: # adapted from cpm-2\n",
    "            prompt_mask = input_ids < 0\n",
    "            prompt_ids = -input_ids * prompt_mask\n",
    "            assert torch.all(prompt_ids < self.prompt_len)\n",
    "            p_embeds = self.prompt_embedding(prompt_ids) * prompt_mask.float().unsqueeze(-1)\n",
    "            input_ids = input_ids * ~prompt_mask\n",
    "            w_embeds = self.wte(input_ids) * (~prompt_mask).float().unsqueeze(-1)\n",
    "            input_embeds = w_embeds + p_embeds\n",
    "        return input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b17b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from cpm-2: https://github.com/TsinghuaAI/CPM-2-Finetune/blob/master/utils.py#L133-L164\n",
    "def get_params_for_prompt_optimization(module: nn.Module):\n",
    "    params = []\n",
    "    for t in module.named_modules():\n",
    "        if \"prompt_embedding\" in t[0]:\n",
    "            params.append({'params': [p for p in list(t[1]._parameters.values()) if p is not None]})\n",
    "    for t in module.named_parameters():\n",
    "        if \"prompt\" not in t[0]:\n",
    "            t[1].requires_grad_(False)    \n",
    "    return params\n",
    "\n",
    "def create_optimizer(model, training_args):\n",
    "    from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "    while isinstance(model, (DDP, )): model = model.module\n",
    "    we.init_prompt_embedding_()\n",
    "    param_groups = get_params_for_prompt_optimization(model)\n",
    "    optimizer = AdamW(param_groups, lr=training_args.learning_rate, \n",
    "                      betas=(training_args.adam_beta1, training_args.adam_beta2),eps=training_args.adam_epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "39b4f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = model.get_input_embeddings()\n",
    "if hasattr(wte, 'wte'): wte = wte.wte  # already been wrapped\n",
    "we = WrappedEmbedding(wte, prompt_len=10000)\n",
    "model.set_input_embeddings(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ae3bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalize(obj):\n",
    "    if type(obj) == bool: return 'Yes' if obj else 'No'\n",
    "    return str(obj)\n",
    "    \n",
    "def make_query_str(instruction, query):\n",
    "    if instruction is None and query is None: return ''\n",
    "    s = '.'\n",
    "    if instruction is not None: s = s + ' ' + instruction\n",
    "    if query is not None:\n",
    "        if type(query) in [int, bool, str]: query = [query]\n",
    "        if type(query) == dict:\n",
    "    #         return '. ' + '{' + ','.join([' %s: %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "            s = s + ' ' + '{' + ','.join([' replace %s with %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "        elif type(query) in [list,]:\n",
    "            s = s + ' ' + ' '.join([str(i) for i in query])\n",
    "    return s\n",
    "\n",
    "def make_example_str(example, with_instruction=False):\n",
    "    instruction, l, query, ans = example\n",
    "    if type(ans) not in [Sequence, list]: ans = [ans]\n",
    "    ans = [verbalize(a) for a in ans]\n",
    "    return '%s -> %s' % (' '.join(l) + make_query_str(instruction if with_instruction else None, query), ' '.join(ans))\n",
    "\n",
    "def sample_rand_len(vocab, k): return sample(vocab, k=randint(1, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "df63ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptize(s):\n",
    "#     return prompt_token * len(s.split())\n",
    "    return bop_str + s + eop_str\n",
    "    \n",
    "def make_input_str(task, nrows=4, ncols=4, full_vocab=None):\n",
    "    if full_vocab is None: full_vocab = string.ascii_uppercase + string.digits\n",
    "    transform_fn, vocab_fn, sample_fn, query_fn = task\n",
    "    instruction = transform_fn.__name__.replace('_', ' ')\n",
    "    if vocab_fn is None: vocab_fn = lambda: full_vocab\n",
    "    if query_fn is None: query_fn = lambda *_: None\n",
    "        \n",
    "    examples = []\n",
    "    query = None\n",
    "    for i in range(nrows):\n",
    "        vocab = vocab_fn()\n",
    "        l = sample_fn(vocab, k=ncols)\n",
    "        query = query_fn(l, vocab, ncols)\n",
    "        examples.append([instruction, l, query, transform_fn(l, query=query)])\n",
    "\n",
    "    desc = promptize(instruction) if True else ''\n",
    "    text = '\\n'.join([make_example_str(e, with_instruction=False) for e in examples])\n",
    "    text = desc + '\\n' + text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "ce0d0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(examples, ans_vocab=[True, False]):\n",
    "    groups = seq(examples).group_by(_[-1]).map(_[1])  # 按ans分组\n",
    "    assert groups.len() == len(ans_vocab)  # 保证每种ans都出现\n",
    "    min_cnt = groups.map(lambda x: len(x)).min()\n",
    "    examples = groups.map(lambda x: sample(x, min_cnt)).flatten().list() # 每组都采样最小个数后去分组\n",
    "    return sample(examples, len(examples))  # 重新打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "856bb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_a(cxt, query):\n",
    "    SC, CD = cxt  # SC paris: studeng-course relation, CD pairs: course-department function\n",
    "    ss, d = query  # ss: 学生子集（可以*不止两个学生*），d: 课程\n",
    "#     return seq(ss).map(lambda s: seq(SC).filter(_[0] == s).map(_[1]).intersection(CD.filter(_[1] == d).map(_[0])).non_empty()).all()\n",
    "    return (seq(ss)\n",
    "            .map(lambda s: seq(SC).filter(_[0] == s).map(_[1])  # 学生s选的所有课程\n",
    "                 .intersection(\n",
    "                     seq(CD).filter(_[1] == d).map(_[0])) # d系的课程\n",
    "                 .non_empty())  # s选了d系的课程\n",
    "            .all())  # 学生子集ss都选了d系的课程\n",
    "\n",
    "def all_a_sample(vocab, k):\n",
    "    S_vocab, C_vocab, D_vocab = vocab  # vocabs of students, courses, departments\n",
    "    k_S, k_C, k_D, k_SC = k  # default values: k_S = 3, k_C = 3, k_D = 2, k_SC = 5\n",
    "    S, C, D = sample(S_vocab, k_S), sample(C_vocab, k_C), sample(D_vocab, k_D)\n",
    "    \n",
    "    while len(set(CD := choices(D, k=k_C))) < k_D: continue  # ds里每个系的课都要出现\n",
    "    CD = list(zip(C, CD))  # 得到每门课所属的系\n",
    "    \n",
    "    all_SC = list(itertools.product(S, C))  # or seq(S).cartesian(C).list()\n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue  # ss里每个学生都要选课\n",
    "    return SC, CD\n",
    "\n",
    "def select_distinct(tuples, col): return seq(tuples).map(_[col]).distinct().list()\n",
    "    \n",
    "def all_a_query(cxt,vocab,k):\n",
    "    SC, CD = cxt\n",
    "    k_S, k_C, k_D, k_SC = k\n",
    "    S, D = select_distinct(SC, 0), select_distinct(CD, 1)\n",
    "    k_ss = randint(2, len(S))\n",
    "    ss = sample(S, k_ss)\n",
    "    d = choice(D)\n",
    "    return ss, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "62aee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_element(l, query=None): return seq(l).slice(1, 2)\n",
    "def ith_group(l, query=None): return seq(l).group_by(_).select(_[1]).slice(1, 2).flatten()#.distinct()# davinci F w/ and wo dist\n",
    "# def element_at_index(l, query): return seq(l).slice(query, query + 1) # davinci F\n",
    "def element_at_index(l, query): return seq(l).enumerate().filter(_[0] == query).select(_[1])\n",
    "def replace(l, query): return seq(l).map(lambda x: query.get(x, x))\n",
    "def replace_with_the_other(l, query): # davinci F\n",
    "    query = {k: (set(l) - {k}).pop() for k in l}\n",
    "    return replace(l, query)\n",
    "def replace_all_with(l, query): return seq(l).map(lambda x: query)  # davinci F?!\n",
    "def interleave_with(l, query): return seq(l).flat_map(lambda x: [x, query])  # davinci T!!\n",
    "def unique_elements(l, query=None): return seq(l).distinct() # davinci F\n",
    "def how_many_unique_elements(l, query=None): return seq(l).distinct().len()  # davinci F\n",
    "def how_many(l, query): return seq(l).filter(_ == query).len() # davinci F\n",
    "def select_same_as(l, query): return seq(l).filter(_ == query) # simpler version of how_many. davinci F\n",
    "def select_same_number_as(l, query): return seq(l).group_by(_).select(_[1]).filter(lambda x: len(x) == len(query)).flatten() # F\n",
    "def includes(l, query): return seq(l).union(seq(query)).distinct().len() == seq(l).distinct().len() # davinci F\n",
    "def is_included_by(l, query): return seq(l).difference(seq(query)).empty() # davinci F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f199b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    (ith_element,            None,                               sample,    None),\n",
    "    (ith_group,              None, lambda vocab, k: seq(sample(vocab, k)).map(lambda x:[x]*randint(1, 3)).flatten().list(),None),\n",
    "    (element_at_index,       lambda: upper_letters,              sample,    lambda l,vocab,k: randint(0, min(2,len(l)-1))),\n",
    "    (replace,                None,                               sample,    lambda l,vocab,k: {choice(l): choice(vocab)}),\n",
    "    (replace_with_the_other, lambda: sample(full_vocab, 2),   lambda vocab,k: sample(vocab+choices(vocab, k=k-2),k), None),\n",
    "    (replace_all_with,       None,                               sample_rand_len, lambda l,vocab,k: choice(vocab)),\n",
    "    (interleave_with,        None,                               sample_rand_len, lambda l,vocab,k: choice(vocab)),\n",
    "    (unique_elements,        lambda: sample(upper_letters, 3),   choices,   None),\n",
    "    (how_many_unique_elements,lambda: sample(upper_letters, 3),  choices,   None),\n",
    "    (how_many,               lambda: sample(upper_letters, 3),   choices,   lambda l,vocab,k: choice(list(set(l)))),\n",
    "    (select_same_as,         lambda: sample(upper_letters, 3),   choices,   lambda l,vocab,k: choice(list(set(l)))),\n",
    "    (select_same_number_as,  None, lambda vocab, k: seq(sample(vocab, k)).map(lambda x:[x]*randint(1, 3)).flatten().list(),   \n",
    "     lambda l,vocab,k: [choice(vocab)]*randint(1, 3)),\n",
    "    (includes,               lambda: sample(upper_letters, 6),   sample,    lambda l,vocab,k: sample(vocab, 3)),\n",
    "    (is_included_by,         lambda: sample(upper_letters, 6),   sample,    lambda l,vocab,k: sample(vocab, 5)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "298d0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons = ['triangle', 'quadrangle', 'pentagon', 'hexagon', 'heptagon', 'octagon', 'nonagon', 'decagon',]# 'undecagon', 'dodecagon']\n",
    "times_of_day = ['dawn', 'morning', 'noon', 'afternoon', 'evening', 'night', 'midnight']\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "seasons = ['spring', 'summer', 'autumn', 'winter']\n",
    "ages_of_life = ['baby', 'child', 'teenager', 'young', 'adult', 'elder']\n",
    "times_of_history = ['primitive', 'medieval', 'renaissance', 'modern', 'contemporary']\n",
    "units_of_time = ['nanosecond', 'microsecond', 'millisecond', 'second', 'minute', 'hour', 'day', 'week', 'month', 'year', 'decade', 'century', 'millennium'] # first 3 multi-token\n",
    "units_of_length = ['nanometer', 'micrometer', 'millimeter', 'meter', 'kilometer', 'mile']\n",
    "units_of_mass = ['nanogram', 'microgram', 'milligram', 'gram', 'kilogram', 'ton']\n",
    "SI_prefixes_small = ['pico', 'nana', 'micro', 'milli', 'centi', 'deci']\n",
    "SI_prefixes_large = ['kilo', 'mega', 'giga', 'tera', 'peta', 'exa', 'zetta', 'yotta']\n",
    "\n",
    "things = ['atom', 'molecule', 'cell', 'tissue', 'organ', 'system', 'person', 'community', 'city', 'state', 'country', 'continent', 'planet', 'star', 'galaxy', 'universe']\n",
    "\n",
    "posets = [list(string.ascii_uppercase), list(string.ascii_lowercase), digits, times_of_day, days_of_week, months, seasons, ages_of_life, times_of_history, units_of_time, units_of_length, units_of_mass, things]#, SI_prefixes_small, SI_prefixes_large]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b2984bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nanometer, micrometer, millimeter, meter, kilometer'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(units_of_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc8d026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġnan', 'ose', 'cond']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġmicro', 'second']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġmillisec', 'ond']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġsecond']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġminute']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġhour']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġday']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġweek']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġmonth']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġyear']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġdecade']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġcentury']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Ġmillennium']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in periods: tokenizer.tokenize(' ' + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834550",
   "metadata": {},
   "source": [
    "**TODO: read children books for more posets**  \n",
    "**TODO: Prompt gpt3 to elicit the posets it knows**  \n",
    "$x \\to f(x)$ where $f \\in \\{\\text{prev/next in posets of numbers/letters/months/days, antonym, hypernym, hyponym, ...}\\}$  \n",
    "$x \\to f^2(x)$  \n",
    "one poset or mixed posets  \n",
    "$x, f(x).~y \\to Ff^{[-1]}(y)$ one poset or mixed posets  \n",
    "$x, f^k(x).~y \\to Ff^{[-1]}(y)~/Ff^{[-]k}(y)$  \n",
    "$x, f(f(x))~/f(f(x)), x \\to f(x)$ in between, the simplest form of sequence completion  \n",
    "$x, f(x) \\to Gf$ where $Gf \\in \\{<, >\\}$  \n",
    "$x, f(x); y, g(y) \\to Ff \\stackrel{?}{=} g^{[-1]}$ where $\\text{output} \\in \\{\\text{True}, \\text{False}\\}$  \n",
    "sort\n",
    "\n",
    "There is a *natural* monotone map/functor $F$ between posets/sets $A$ and $B$.  Compose the computation (set operations, sorting etc.) between $A$ and $B$ with $F$ to make harder tasks.  \n",
    "$P(A) ,P(B) \\to F(P(A)) \\setminus ~/ \\cap ~/ \\triangle P(B)$. Harder form of set difference/intersection.  \n",
    "$P(A) \\to F(\\text{sorted}(P(A)))$. Harder form of sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6f135452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = seq(choice(posets))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "28fd5077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Tuesday  </td><td>((&#x27;Tuesday&#x27;, &#x27;Monday&#x27;), (&#x27;Tuesday&#x27;, &#x27;Wednesday&#x27;, &#x27;Thursday&#x27;, &#x27;Friday&#x27;, &#x27;Saturday&#x27;, &#x27;Sunday&#x27;))  </td></tr>\n",
       "<tr><td>Wednesday</td><td>((&#x27;Wednesday&#x27;, &#x27;Tuesday&#x27;, &#x27;Monday&#x27;), (&#x27;Wednesday&#x27;, &#x27;Thursday&#x27;, &#x27;Friday&#x27;, &#x27;Saturday&#x27;, &#x27;Sunday&#x27;))</td></tr>\n",
       "<tr><td>Thursday </td><td>((&#x27;Thursday&#x27;, &#x27;Wednesday&#x27;, &#x27;Tuesday&#x27;, &#x27;Monday&#x27;), (&#x27;Thursday&#x27;, &#x27;Friday&#x27;, &#x27;Saturday&#x27;, &#x27;Sunday&#x27;)) </td></tr>\n",
       "<tr><td>Friday   </td><td>((&#x27;Friday&#x27;, &#x27;Thursday&#x27;, &#x27;Wednesday&#x27;, &#x27;Tuesday&#x27;, &#x27;Monday&#x27;), (&#x27;Friday&#x27;, &#x27;Saturday&#x27;, &#x27;Sunday&#x27;))   </td></tr>\n",
       "<tr><td>Saturday </td><td>((&#x27;Saturday&#x27;, &#x27;Friday&#x27;, &#x27;Thursday&#x27;, &#x27;Wednesday&#x27;, &#x27;Tuesday&#x27;, &#x27;Monday&#x27;), (&#x27;Saturday&#x27;, &#x27;Sunday&#x27;)) </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Tuesday', (('Tuesday', 'Monday'), ('Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'))), ('Wednesday', (('Wednesday', 'Tuesday', 'Monday'), ('Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'))), ('Thursday', (('Thursday', 'Wednesday', 'Tuesday', 'Monday'), ('Thursday', 'Friday', 'Saturday', 'Sunday'))), ('Friday', (('Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday'), ('Friday', 'Saturday', 'Sunday'))), ('Saturday', (('Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday'), ('Saturday', 'Sunday')))]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = p.zip(p.inits().zip(p.tails())).slice(1, p.len() - 1)\n",
    "# r = OrderedDict(r)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "60151283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wednesday'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e = choice(list(r.keys()))\n",
    "e = r.dom().a(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "bc90b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_functions = [pred(), succ()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e66d2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_fn = choice(relational_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "1cbe83ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wednesday'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Tuesday'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e\n",
    "rel_fn(r.image(e)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f8aba74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred()(r.image('C')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3c6cab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour(direction, k=1): return lambda x: x[direction][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a63ca6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(k=1): return neighbour(0, k)\n",
    "def succ(k=1): return neighbour(1, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bbcf3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'C'\n",
    "l.zip(l.reverse().tails().reverse().tail().zip(l.tails())).filter(_[0] == query).map(_[1]).map(succ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e4848490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: select same as. For example:\n",
      "A L L A. A -> A A\n",
      "W B R W. R -> R\n",
      "J J J C. C -> C\n",
      "F F F F. F -> F F F F\n",
      "I M W W. W -> W W\n",
      "X X X X. X -> X X X X\n",
      "P P P P. P -> P P P P\n",
      "H G L L. H -> H\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-4], nrows=8, ncols=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "17373019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: replace with the other. For example:\n",
      "G H G G G -> H G H H H\n",
      "I I I I M -> M M M M I\n",
      "A A F A A -> F F A F F\n",
      "9 9 9 I I -> I I I 9 9\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "V Q Q V V -> Q V V Q Q\n",
      "G L L G L -> L G G L G\n",
      "G 2 2 2 G -> 2 G G G 2\n",
      "I I Z Z Z -> Z Z I I I\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "R H H H R -> H R R R H\n",
      "B 9 9 B B -> 9 B B 9 9\n",
      "D 2 2 2 D -> 2 D D D 2\n",
      "A A A A W -> W W W W A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_total, n_valid = 192, 64\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s.count('Yes') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6ebf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,\n",
    "    lr_scheduler_type='constant', learning_rate=5e-3, num_train_epochs=4,\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,\n",
    "    no_cuda=True, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
