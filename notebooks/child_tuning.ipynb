{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7292808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d03e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src')\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid3/xd/.cache/torch'  # deliberately set this wrong path to avoid migrating cache\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce, partial\n",
    "from collections.abc import Iterable\n",
    "import traceback\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a886cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid3/xd/.cache/torch/hub\n"
     ]
    }
   ],
   "source": [
    "# from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer#, pipeline\n",
    "# from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "# from transformers import T5Tokenizer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "# from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c8f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence    \n",
    "from fn import _ as __\n",
    "from collections import namedtuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cba5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *\n",
    "# from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# cache_dir = '/mnt/nvme1/xd/.cache/torch/transformers/'  # for gpt-j-6B on elderberry\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ab655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neox-20b ... In huggingface_hub.file_download.cached_download: url = https://huggingface.co/EleutherAI/gpt-neox-20b/resolve/main/config.json\n",
      "done 0:01:43.205725\n"
     ]
    }
   ],
   "source": [
    "# curl -x http://192.168.50.1:1081 -L -O [-C -] https://huggingface.co/google/ul2/resolve/main/pytorch_model.bin  # -C for 断点续传\n",
    "s2s_model_names = ['google/t5-xl-lm-adapt', 'google/t5-xxl-lm-adapt', 'bigscience/T0p', 'bigscience/T0_3B', \n",
    "    'allenai/tk-instruct-3b-pos', 'allenai/tk-instruct-3b-def-pos', 'google/ul2']\n",
    "gpt_model_names = ['EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', \n",
    "                   'KoboldAI/fairseq-dense-6.7B', 'KoboldAI/fairseq-dense-13B']\n",
    "for model_name in s2s_model_names[:0] + gpt_model_names[1:2]:#, 'gpt2-xl', 'EleutherAI/gpt-neo-1.3B', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name in models: continue\n",
    "    with Timer(model_name):\n",
    "        model_cls = AutoModelForCausalLM if any(s in model_name for s in ['gpt', 'fairseq-dense']) else T5ForConditionalGeneration\n",
    "        # _cache_dir = cache_dir.replace('/nas/', '/nas2/') if 'gpt' not in model_name else cache_dir\n",
    "#         model = model_cls.from_pretrained(model_name, cache_dir=cache_dir, proxies=proxies)#, low_cpu_mem_usage=True)\n",
    "        model = model_cls.from_pretrained(model_name, cache_dir=cache_dir, device_map=\"auto\", load_in_8bit=True)\n",
    "        if hasattr(model.config, 'use_cache'): model.config.use_cache = False\n",
    "        # if model_name in ['EleutherAI/gpt-neox-20b']: model = model.half()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faf5f812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 144])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Pip and conda also differ in how dependency relationships within an environment are fulfilled. When installing packages, pip installs dependencies in a recursive, serial loop. No effort is made to ensure that the dependencies of all packages are fulfilled simultaneously. This can lead to environments that are broken in subtle ways, if packages installed earlier in the order have incompatible dependency versions relative to packages installed later in the order. In contrast, conda uses a satisfiability (SAT) solver to verify that all requirements of all packages installed in an environment are met. This check can take extra time but helps prevent the creation of broken environments. As long as package metadata about dependencies is correct, conda will predictably produce working environments.'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "input_ids = torch.cat([input_ids] * 1, dim=-1)\n",
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1edf7c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'int'>\n",
      "bb\n",
      "last_hidden_state <class 'str'>\n",
      "aa\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): outputs = model(input_ids.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08c617b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = OrderedDict([('a', 1), ('b', 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59e66221",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e586b691db67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "d.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db50e2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('logits', tensor([[[37.1250,  2.7246, 38.6250,  ...,  4.0781,  3.6250,  4.1758],\n",
       "         [57.4688,  5.2422, 60.7188,  ...,  5.6406,  5.4258,  6.2266],\n",
       "         [61.0000,  3.9336, 60.7500,  ...,  4.3867,  4.6641,  4.8594],\n",
       "         ...,\n",
       "         [62.0312,  2.8086, 60.6250,  ...,  3.3965,  3.6895,  4.1445],\n",
       "         [70.5625,  3.2188, 71.0000,  ...,  3.2285,  2.9238,  4.2148],\n",
       "         [70.3750,  3.9570, 61.0000,  ...,  4.5234,  4.2070,  5.2070]]],\n",
       "       device='cuda:0', dtype=torch.float16))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7a87452",
   "metadata": {},
   "outputs": [],
   "source": [
    "del outputs; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "c64283f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open('/nas/xd/projects/openai_api_keys.txt').readlines()[-1].split()[0]\n",
    "# response = openai.Completion.create(engine='text-davinci-002', prompt=text, #'Once upon a time',\n",
    "#     max_tokens=20, echo=True, logprobs=5)\n",
    "# print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e42d23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_model(engine):\n",
    "    def forward(input_ids):#, attention_mask=None):\n",
    "        text = tokenizer.decode(input_ids[0])\n",
    "        response = openai.Completion.create(engine=engine, prompt=text, max_tokens=0, echo=True, logprobs=5)\n",
    "        return Outputs(logits=response.choices[0].logprobs)\n",
    "    return forward\n",
    "    \n",
    "# tokenizer0 = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "engines = ['text-babbage-001', 'text-curie-001', 'text-davinci-002']\n",
    "for engine in engines: models[engine] = get_openai_model(engine), tokenizer0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052c1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = 'Ġ!'; prompt_id = tokenizer._convert_token_to_id(prompt_token)\n",
    "bop_str = 'Instruction: '; bop_id = tokenizer.encode(bop_str)[0]  # 'Inst'\n",
    "eop_str = '. For example:'; eop_id = tokenizer.encode(eop_str)[2] # 'Ġexample'\n",
    "bos_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "eos_id = tokenizer._convert_token_to_id('Ċ')\n",
    "\n",
    "\n",
    "class CHILDDataset(Dataset):\n",
    "    def __init__(self, input_strs, tokenizer):\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = '!'\n",
    "        self.inputs = tokenizer.batch_encode_plus(input_strs, add_special_tokens=False, padding=True, return_tensors='pt')\n",
    "        input_ids = self.inputs.input_ids\n",
    "        self.labels = torch.ones_like(input_ids) * (-100)\n",
    "        for bi in range(input_ids.size(0)):\n",
    "            bop_idx = (input_ids[bi] == bop_id).nonzero().squeeze(1)\n",
    "            eop_idx = (input_ids[bi] == eop_id).nonzero().squeeze(1)\n",
    "            if len(bop_idx) > 0:\n",
    "                assert len(bop_idx) == 1 and len(eop_idx) == 1\n",
    "                bop_idx, eop_idx = bop_idx.item(), eop_idx.item()\n",
    "                input_ids[bi, bop_idx: eop_idx + 2] *= -1  # use prompt embedding for prompt tokens\n",
    "            \n",
    "            bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "            eos_indices = (input_ids[bi] == eos_id).nonzero()[-len(bos_indices):].squeeze(1)\n",
    "            for i, (bos_i, eos_i) in enumerate(zip(bos_indices.tolist(), eos_indices.tolist())):\n",
    "                assert eos_i > bos_i + 1\n",
    "                if i >= 2: self.labels[bi, bos_i + 1: eos_i] = input_ids[bi, bos_i + 1: eos_i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(sel f.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.inputs['input_ids'][i],\n",
    "                'attention_mask': self.inputs['attention_mask'][i],\n",
    "                'labels': self.labels[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "56fcd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                prompt_id: int = None,\n",
    "                prompt_len: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(WrappedEmbedding, self).__init__()\n",
    "#         self.wte = wte\n",
    "#         self.prompt_id = prompt_id\n",
    "#         self.prompt_len = prompt_len\n",
    "        self.__dict__.update(locals()); del self.self\n",
    "        if self.prompt_id is not None:\n",
    "            self.prompt_embedding = nn.parameter.Parameter(\n",
    "                self.initialize_embedding(random_range, initialize_from_vocab)).to(self.wte.weight.device)\n",
    "        else:\n",
    "            self.prompt_embedding = nn.Embedding(self.prompt_len, self.wte.weight.size(1)).to(self.wte.weight.device)\n",
    "            assert initialize_from_vocab\n",
    "            self.init_prompt_embedding_()\n",
    "#             self.prompt_embedding.weight.data = self.initialize_embedding(random_range, initialize_from_vocab)     \n",
    "            \n",
    "    def initialize_embedding(self, random_range: float = 0.5, initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab: return self.wte.weight[:self.prompt_len].clone().detach()\n",
    "        return torch.FloatTensor(self.prompt_len, self.wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "    \n",
    "    def init_prompt_embedding_(self):\n",
    "        self.prompt_embedding.weight.data[:] = self.wte.weight[:self.prompt_len]\n",
    "            \n",
    "    def forward(self, input_ids):\n",
    "        if self.prompt_id is not None:\n",
    "            input_embeds = self.wte(input_ids)\n",
    "            input_embeds[input_ids == self.prompt_id] = self.prompt_embedding.expand(input_embeds.size(0), -1, -1)\n",
    "        else: # adapted from cpm-2\n",
    "            prompt_mask = input_ids < 0\n",
    "            prompt_ids = -input_ids * prompt_mask\n",
    "            assert torch.all(prompt_ids < self.prompt_len)\n",
    "            p_embeds = self.prompt_embedding(prompt_ids) * prompt_mask.float().unsqueeze(-1)\n",
    "            input_ids = input_ids * ~prompt_mask\n",
    "            w_embeds = self.wte(input_ids) * (~prompt_mask).float().unsqueeze(-1)\n",
    "            input_embeds = w_embeds + p_embeds\n",
    "        return input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b17b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from cpm-2: https://github.com/TsinghuaAI/CPM-2-Finetune/blob/master/utils.py#L133-L164\n",
    "def get_params_for_prompt_optimization(module: nn.Module):\n",
    "    params = []\n",
    "    for t in module.named_modules():\n",
    "        if \"prompt_embedding\" in t[0]:\n",
    "            params.append({'params': [p for p in list(t[1]._parameters.values()) if p is not None]})\n",
    "    for t in module.named_parameters():\n",
    "        if \"prompt\" not in t[0]:\n",
    "            t[1].requires_grad_(False)    \n",
    "    return params\n",
    "\n",
    "def create_optimizer(model, training_args):\n",
    "    from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "    while isinstance(model, (DDP, )): model = model.module\n",
    "    we.init_prompt_embedding_()\n",
    "    param_groups = get_params_for_prompt_optimization(model)\n",
    "    optimizer = AdamW(param_groups, lr=training_args.learning_rate, \n",
    "                      betas=(training_args.adam_beta1, training_args.adam_beta2),eps=training_args.adam_epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "39b4f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = model.get_input_embeddings()\n",
    "if hasattr(wte, 'wte'): wte = wte.wte  # already been wrapped\n",
    "we = WrappedEmbedding(wte, prompt_len=10000)\n",
    "model.set_input_embeddings(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalize(obj):\n",
    "    if type(obj) == bool: return 'Yes' if obj else 'No'\n",
    "    return str(obj)\n",
    "    \n",
    "def make_query_str(instruction, query):\n",
    "    if instruction is None and query is None: return ''\n",
    "    s = '.'\n",
    "    if instruction is not None: s = s + ' ' + instruction\n",
    "    if query is not None:\n",
    "        if type(query) in [int, bool, str]: query = [query]\n",
    "        if type(query) == dict:\n",
    "    #         return '. ' + '{' + ','.join([' %s: %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "            s = s + ' ' + '{' + ','.join([' replace %s with %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "        elif type(query) in [list,]:\n",
    "            s = s + ' ' + ' '.join([str(i) for i in query])\n",
    "    return s\n",
    "\n",
    "def make_example_str(example, with_instruction=False):\n",
    "    instruction, l, query, ans = example\n",
    "    if type(ans) not in [Sequence, list]: ans = [ans]\n",
    "    ans = [verbalize(a) for a in ans]\n",
    "    return '%s -> %s' % (' '.join(l) + make_query_str(instruction if with_instruction else None, query), ' '.join(ans))\n",
    "\n",
    "def sample_rand_len(vocab, k): return sample(vocab, k=randint(1, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2r(p): p = seq(p); return p.zip(p.inits().zip(p.tails()))#.slice(1, p.len() - 1)\n",
    "def neighbour(direction, k=1): return lambda x: x[direction][k]\n",
    "def prev(k=1): return neighbour(0, k)\n",
    "def next(k=1): return neighbour(1, k)\n",
    "prevs, nexts = __[0][1:], __[1][1:]\n",
    "beside = lambda x: (x[0][1], x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_element(cxt, query): return seq(cxt).slice(1, 2)[0]\n",
    "def besides(cxt, query): return seq(cxt).difference(query)[0]\n",
    "# def besides_query(cxt, vocab): return cxt.a(sample, 2), cxt.list()\n",
    "def get_poset(e): return tuple([p for p in posets if e in p][0])\n",
    "def special(cxt, query): return seq(cxt).group_by(get_poset).map(__[1]).find(lambda x: len(x) == 1)[0]\n",
    "# def special_cxt(vocab, k=3): sample(vocab[0], k - 1) + sample(vocab[1], 1)\n",
    "\n",
    "def after_query(r, p):\n",
    "    # e = r.dom().init().a(choice)\n",
    "    e = choice(r.dom().init().tail().list())\n",
    "    options = r.image(e).map(beside)[0].a(sample, 2)\n",
    "    return e, options\n",
    "\n",
    "def before_query(r, p):\n",
    "    # e = r.dom().tail().a(choice)\n",
    "    e = choice(r.dom().init().tail().list())\n",
    "    options = r.image(e).map(beside)[0].a(sample, 2)\n",
    "    return e, options\n",
    "\n",
    "def after(r, q): return r.image(q).map(next())[0]\n",
    "def before(r, q): return r.image(q).map(prev())[0]\n",
    "def between(r, q): \n",
    "    return r.image(q[0]).map(nexts)[0].intersection(r.image(q[1]).map(prevs)[0]).union(\n",
    "        r.image(q[0]).map(prevs)[0].intersection(r.image(q[1]).map(nexts)[0]))\n",
    "    \n",
    "def monotone_map_cxt(vocab):\n",
    "    P, p = vocab\n",
    "    R = p2r(P)\n",
    "    E1 = R.dom().init().tail().a(choice)\n",
    "    E2 = R.image(E1).map(beside)[0].a(choice)\n",
    "    return R, E1, E2\n",
    "\n",
    "def monotone_map_query(cxt, vocab):\n",
    "    P, p = vocab\n",
    "    r = p2r(p)\n",
    "    e1 = r.dom().init().tail().a(choice)\n",
    "    options = r.image(e1).map(beside)[0]\n",
    "    return (r, e1), options\n",
    "\n",
    "def monotone_map(cxt, query, reverse=False):\n",
    "    R, E1, E2 = cxt\n",
    "    r, e1 = query\n",
    "    return r.image(e1).map(\n",
    "        seq([prev(), next()]).find(lambda f: (E2 in R.image(E1).map(f)[0]) != reverse)  # reverse = not in. too tricky\n",
    "    )[0]\n",
    "    \n",
    "tasks = [\n",
    "    (ith_element, None, partial(sample, k=3), None),\n",
    "    (besides, None, partial(sample, k=3), lambda cxt, vocab: (sample(cxt, 2), cxt)),\n",
    "    (special, lambda: sample(posets[1:3], 2), lambda vocab: sample(sample(vocab[0], 2) + sample(vocab[1], 1), 2 + 1), None),\n",
    "    \n",
    "    (after, lambda: choice(closed_posets[:]), p2r, after_query, lambda r: ''),\n",
    "    (before, lambda: choice(closed_posets[:]), p2r, before_query, lambda r: ''),\n",
    "    (between, lambda: choice(posets), p2r, lambda r, p: r.image(r.dom().init().tail().a(choice)).map(beside)[0].a(sample, 2), lambda r: ''),\n",
    "    (partial(monotone_map, reverse=False), lambda: sample(posets, 2), monotone_map_cxt, monotone_map_query),\n",
    "    (partial(monotone_map, reverse=True), lambda: sample(closed_posets, 2), monotone_map_cxt, monotone_map_query),\n",
    "    (lookup, )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cbf4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _str(l, vocab=None, sep=' '):\n",
    "    if l is None: return ''\n",
    "    if isinstance(l, str) or not isinstance(l, Iterable): l = [l]\n",
    "    l = [e for e in l if not my_isinstance(e, Sequence)] #type(e).__name__ != 'Sequence']\n",
    "    if isinstance(l, (dict, OrderedDict)): l = [f'{k}: {v}' for k, v in l.items()]\n",
    "    return sep.join(str(i) for i in l)\n",
    "\n",
    "def options2str(options): return '[' + ' | '.join(options) + ']'\n",
    "# def options2str(options): return ' or '.join(options) + '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I = Identity; M = Mophism; A = Aggregation; C = CMP; G = GroupBy; l = local\n",
    "patterns = ['M', 'A?', 'IA', 'MA',\n",
    "    'IlI', 'MlI', 'IlM', 'MlM', 'IlMlI',\n",
    "    'IlA', 'MlA', 'IlC', 'MlC', 'AlI', 'GIlI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "df63ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptize(s): return bop_str + s + eop_str\n",
    "    \n",
    "def make_examples(task, nrows=4, vocab_for_each_row=True, options_position=None):\n",
    "    # if full_vocab is None: full_vocab = string.ascii_uppercase + string.ascii_lowercase #+ string.digits\n",
    "    # transform_fn, vocab_fn, sample_fn, query_fn = task[:4]\n",
    "    # if query_fn is None: query_fn = lambda *_: (None, None)\n",
    "    # if vocab_fn is None: vocab_fn = lambda: full_vocab\n",
    "    vocab_fn, example_gen_fn = task[:2]\n",
    "\n",
    "    vocabs, examples = [], []\n",
    "    qa_set = set() # for dedup\n",
    "    if not vocab_for_each_row: vocab = vocab_fn()\n",
    "    for i in range(nrows * 2):\n",
    "        if vocab_for_each_row: vocab = vocab_fn()\n",
    "        # cxt = sample_fn(vocab)\n",
    "        # query, options = query_fn(cxt, vocab)\n",
    "        # ans = transform_fn(cxt, query)\n",
    "        cxt, query, candidates, ans = example_gen_fn(vocab)\n",
    "        if isinstance(query, list): query = tuple(query)\n",
    "        if (query, ans) not in qa_set:\n",
    "            qa_set.add((query, ans))\n",
    "            vocabs.append(vocab)\n",
    "            examples.append([cxt, query, candidates, ans])\n",
    "        if len(examples) == nrows: break\n",
    "    return vocabs, examples\n",
    "\n",
    "def make_input_str(task, vocabs, examples, options_position=None):\n",
    "    task += (_str,) * (2 + 3 - len(task))\n",
    "    cxt2str, query2str, ans2str = task[2:]\n",
    "    def example2str(vocab, example, with_instruction=False):\n",
    "        cxt, query, options, ans = example\n",
    "        strs = [cxt2str(cxt, vocab), query2str(query, vocab)]\n",
    "        if options_position is not None: strs.insert(options_position, options2str(options))\n",
    "        return '. '.join(s for s in strs if s != '') + ' -> ' + ans2str(ans)\n",
    "\n",
    "    text = '\\n'.join(example2str(v, e) for v, e in zip(vocabs, examples)) + '\\n'\n",
    "    return text\n",
    "\n",
    "def make_input_str_old(task, examples, options_position=None):\n",
    "    task += (_str,) * (2 + 3 - len(task))\n",
    "    cxt2str, query2str, ans2str = task[2:]\n",
    "\n",
    "    def example2str(example, with_instruction=False):\n",
    "        cxt, query, options, ans = example\n",
    "        strs = [cxt2str(cxt), query2str(query)]\n",
    "        if options_position is not None: strs.insert(options_position, options2str(options))\n",
    "        # strs = [options2str(options)] + strs if options_position == 'pre' else strs + [options2str(options)]\n",
    "        return '. '.join(s for s in strs if s != '') + ' -> ' + ans2str(ans)\n",
    "\n",
    "    desc = promptize(instruction) + '\\n' if False else ''\n",
    "    text = '\\n'.join(example2str(e) for e in examples)\n",
    "    text = '\\n' + desc + text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "410a844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MlM_gen(vocab, cxt_len=None, rel_names=None):\n",
    "    s0, s1 = vocab\n",
    "    rel0, rel0_dom, rel0_codom, is_rel0 = s0.get_rel_fns(rel_names[0])\n",
    "    rel1, rel1_dom, rel1_codom, is_rel1 = s1.get_rel_fns(rel_names[1])\n",
    "    distracting_rel_names = [] #[r for r in set(s0.rel_names) - {rel_names[0]} if random.random() > 0.5]\n",
    "    dom = set.intersection(*[set(getattr(s0, r + '_dom')()) for r in [rel_names[0]] + distracting_rel_names])\n",
    "    query = choice(list(dom))\n",
    "\n",
    "    all_rels = [rel0] #[getattr(s0, r) for r in s0.rel_names]\n",
    "    candidate_rels = [rel0] + [getattr(s0, r) for r in distracting_rel_names]\n",
    "    candidates0 = [choice(r(query)) for r in candidate_rels]\n",
    "    candidates0 += sample(list(set(s0.el()) - set(join_lists([r(query) for r in all_rels]))), \n",
    "        cxt_len - len(candidates0))\n",
    "    candidates1 = sample(rel1_dom(), cxt_len)\n",
    "    cxt = list(zip(sample(candidates0, cxt_len), candidates1))\n",
    "    def transform_fn(cxt, query): return choice(rel1(seq(cxt).find(lambda x: is_rel0(query, x[0]))[1]))\n",
    "    ans = transform_fn(cxt, query)\n",
    "    return cxt, query, candidates1, ans\n",
    "\n",
    "def MlM_gen(rels, cxt_len=None):\n",
    "    hop = 0\n",
    "    query = choice(list(rels[hop][0].dom()))\n",
    "    candidates0 = [choice(r.f(query)) for r in rels[hop][:1]]\n",
    "    # candidates0 = [choice(r.f(query)) for i, r in enumerate(rels[hop]) if i == 0 or random() > 0.5] # w/ distractors\n",
    "    candidates0 += sample(list(rels[hop][0].codom() - set(join_lists([r.f(query) for r in rels[hop]]))), \n",
    "        cxt_len - len(candidates0))\n",
    "\n",
    "    hop = 1\n",
    "    candidates1 = sample(list(rels[hop][0].dom()), cxt_len)\n",
    "    cxt = sample(list(zip(candidates0, candidates1)), cxt_len)\n",
    "\n",
    "    def transform_fn(cxt, query):\n",
    "        hop = 0; ans = seq(cxt).find(lambda x: rels[hop][0].b(query, x[0]))[1]\n",
    "        hop = 1; ans = choice(rels[hop][0].f(ans))\n",
    "        return ans\n",
    "    ans = transform_fn(cxt, query)\n",
    "    return cxt, query, [x[1] for x in cxt], ans\n",
    "\n",
    "def IlMlI_gen(rels, cxt_len=None):\n",
    "    hop = 0\n",
    "    query = choice(list(rels[hop][0].dom()))\n",
    "    candidates0 = [choice(r.f(query)) for r in rels[hop][:1]]\n",
    "    candidates0 += sample(list(rels[hop][0].codom() - set(join_lists([r.f(query) for r in rels[hop]]))), \n",
    "        cxt_len - len(candidates0))\n",
    "    candidates0 = candidates0[:1] + sample(candidates0[1:], cxt_len - 1)\n",
    "\n",
    "    hop = 1\n",
    "    query1 = choice(list(rels[hop][0].dom()))\n",
    "    candidates1 = [query1] + [choice(r.f(query1)) for r in rels[hop][:1]]\n",
    "    candidates1 += sample(list(rels[hop][0].codom() - {query1} - set(join_lists([r.f(query1) for r in rels[hop]]))), \n",
    "        cxt_len - len(candidates1))\n",
    "    # assert len(candidates1) == len(set(candidates1)), str(candidates1)\n",
    "    cxt = sample(list(zip(candidates0, candidates1)), cxt_len)\n",
    "    \n",
    "    def transform_fn(cxt, query):\n",
    "        hop = 0; ans = seq(cxt).find(lambda x: rels[hop][0].b(query, x[0]))[1]\n",
    "        hop = 1; ans = seq(cxt).find(lambda x: x[0] != query and rels[hop][0].b(ans, x[1]))[0]\n",
    "        hop = 2; ans = choice(rels[hop][0].f(ans))\n",
    "        return ans\n",
    "    ans = transform_fn(cxt, query)\n",
    "    return cxt, query, [x[0] for x in cxt], ans\n",
    "\n",
    "def g2bc(g_fn, rels, cxt_len=None, labels=['T', 'F']):\n",
    "    cxt, query, candidates, ans = g_fn(rels, cxt_len=cxt_len)\n",
    "    return (cxt, choice(list(set(candidates) - {query})), labels, labels[1]) \\\n",
    "        if random() > 0.5 else (cxt, query, labels, labels[0])\n",
    "\n",
    "def g2bc(g_fn, labels=['T', 'F']):\n",
    "    def wrapped(*args,**kwargs):\n",
    "        cxt, query, candidates, ans = g_fn(*args,**kwargs)\n",
    "        return (cxt, (query, choice(list(set(candidates) - {query, ans}))), labels, labels[1]) \\\n",
    "            if random() > 0.5 else (cxt, (query, ans), labels, labels[0])\n",
    "    return wrapped\n",
    "\n",
    "def _item2str(item, vocab=None): return f'{item[0]}: {item[1]}'\n",
    "def lookup_item2str(item, vocab=None):\n",
    "    if vocab[0] in [clock_of_day, days_of_week, months]:\n",
    "        if vocab[0] == clock_of_day: prep = 'at'\n",
    "        elif vocab[0] == days_of_week: prep = 'on'\n",
    "        elif vocab[0] == months: prep = 'in'\n",
    "        return f'{item[1]} came {prep} {item[0]}'\n",
    "    elif vocab[0] == digits:\n",
    "        return f'{item[1]} is {item[0]}'\n",
    "def _cxt2str(cxt, vocab=None, item2str=_item2str): return ', '.join([item2str(item, vocab=vocab) for item in cxt])\n",
    "def _query2str(query, vocab=None): return f'{query} same as'\n",
    "def lookup_query2str(query, vocab=None, rel_name=None):\n",
    "    return f'just after {query}'\n",
    "    if vocab[0] in [clock_of_day, days_of_week, months]:\n",
    "        if vocab[0] == clock_of_day: prep = 'at'\n",
    "        elif vocab[0] == days_of_week: prep = 'on'\n",
    "        elif vocab[0] == months: prep = 'in'\n",
    "        prep = {'prev': 'just before', 'next': 'just after', 'same': prep}[rel_name]\n",
    "        return f'Who came {prep} {query}?'\n",
    "    elif vocab[0] == digits:\n",
    "        prep = {'prev': 'a year younger than', 'next': 'a year younger than', 'same': ''}[rel_name]\n",
    "        return f'Who is {prep} {query}'\n",
    "tasks = [\n",
    "    # (lambda: (Poset(choice([days_of_week, months, digits, years, uppercase[:7]])), EqSet(persons)),\n",
    "    (lambda: [[EqSet(persons).equal], [SymSet(frames).equal]],\n",
    "        partial(g2bc(MlM_gen), cxt_len=3),\n",
    "        partial(_cxt2str, item2str=lambda item, vocab: f'{item[1]} {item[0]}'),\n",
    "        lambda query, vocab: f'{query[0]} is {query[1]}?'\n",
    "    ), \n",
    "    (lambda: [[EqSet(persons).equal], [PoSet(digits).equal], [EqSet(persons).equal]],\n",
    "        partial(g2bc(IlMlI_gen), cxt_len=3),\n",
    "        partial(_cxt2str, item2str=lambda item, vocab: f'{item[0]} {item[1]}'),\n",
    "        lambda query, vocab: f'{query[0]} and {query[1]} are same?'\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "403baf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "manual Ricardo, normal Sara, cerebral Jaxx. Jaxx is cerebral? -> T\n",
      "interesting Catherine, compatible Magnus, effective Alayna. Alayna is compatible? -> F\n",
      "rational Dangelo, confident Meghan, legal Luna. Meghan is legal? -> F\n",
      "partial Rylee, unpopular John, extraordinary Ayla. Ayla is extraordinary? -> T\n",
      "boring Quincy, gradual Amina, smart Sara. Sara is gradual? -> F\n",
      "warm Ada, urban Kori, sensitive Krew. Kori is urban? -> T\n",
      "rural Juliette, immortal Amiyah, intelligent Demi. Amiyah is intelligent? -> F\n",
      "abundant Noelle, central Brooklyn, foreign Harmony. Noelle is abundant? -> T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task = tasks[0]; nrows=32; k_shot=5\n",
    "# vocabs, _examples = make_examples(task, nrows=nrows)\n",
    "\n",
    "# answer_indices = [cands.index(ans) for _, _, cands, ans in _examples]\n",
    "# Counter(answer_indices).most_common()\n",
    "# label_probs = F.one_hot(torch.LongTensor(answer_indices))\n",
    "# _ = plt.figure(figsize=(10, 0.7));\n",
    "# _ = sns.heatmap(label_probs.T, cbar=False); plt.show()\n",
    "\n",
    "# text = make_input_str(task, vocabs, _examples)\n",
    "nrows_ = 8\n",
    "answer_indices_ = [cands.index(ans) for _, _, cands, ans in _examples[:nrows_]]\n",
    "text_ = make_input_str(task, vocabs, _examples[:nrows_])\n",
    "print(len(tokenizer.tokenize(text_))); print(text_[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "9e68100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ĠT 0.001 {' Yes': 0.223, ' J': 0.047, '\\n': 0.04} \t manual Ricardo, normal Sara, cerebral Jaxx. Jaxx is cerebral? -> T\n",
      "*ĠF 0.556 {' F': 0.556, ' T': 0.361, ' Y': 0.014} \t interesting Catherine, compatible Magnus, effective Alayna. Alayna is compatible? -> F\n",
      " ĠF 0.49 {' T': 0.494, ' F': 0.49, ' ?': 0.003} \t rational Dangelo, confident Meghan, legal Luna. Meghan is legal? -> F\n",
      "*ĠT 0.995 {' T': 0.995, ' F': 0.002, 'T': 0.002} \t partial Rylee, unpopular John, extraordinary Ayla. Ayla is extraordinary? -> T\n",
      "*ĠF 0.996 {' F': 0.996, ' T': 0.002, 'F': 0.001} \t boring Quincy, gradual Amina, smart Sara. Sara is gradual? -> F\n",
      "*ĠT 0.999 {' T': 0.999, 'T': 0.001, ' F': 0.0} \t warm Ada, urban Kori, sensitive Krew. Kori is urban? -> T\n",
      "*ĠF 0.62 {' F': 0.62, ' T': 0.373, 'T': 0.001} \t rural Juliette, immortal Amiyah, intelligent Demi. Amiyah is intelligent? -> F\n",
      "*ĠT 0.999 {' T': 0.999, 'T': 0.001, ' F': 0.0} \t abundant Noelle, central Brooklyn, foreign Harmony. Noelle is abundant? -> T\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x172.8 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-davinci-002 160 0.09767799912 1.0\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = open('/nas/xd/projects/openai_api_keys.txt').readlines()[-2].split()[0]\n",
    "model_names = ['EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', #'KoboldAI/fairseq-dense-6.7B', 'KoboldAI/fairseq-dense-13B', \n",
    "    'text-curie-001', 'text-davinci-002'][-1:]\n",
    "for model_name in model_names:\n",
    "    model, tokenizer = models[model_name]\n",
    "    examples, input_ids, tokens, bos_indices, eos_indices, answers, labels = make_data_tuple(\n",
    "        text if 'text' not in model_name else text_, tokenizer, k_shot=k_shot)\n",
    "    candidates = [[tokenizer.encode(' ' + token)[0] for token in e[2]] for e in _examples]\n",
    "    with torch.no_grad(): logits = model(input_ids).logits\n",
    "\n",
    "    loss, top1_corrects, answer_probs, candidate_probs = show_predictions(\n",
    "        examples, tokenizer, logits, bos_indices, eos_indices, answers, labels,\n",
    "        candidates=candidates, topk=3, verbose=len(model_names) <= 1)\n",
    "\n",
    "    f, (ax0, ax1) = plt.subplots(2, 1, figsize=(10, 2.4), sharex=True)\n",
    "    x = [i + 0.5 for i in range(nrows if 'text' not in model_name else nrows_)] # to align with sns.heatmap\n",
    "    _ = ax0.bar(x, top1_corrects, width=0.9, alpha=0.5);\n",
    "    _ = ax0.plot(x, answer_probs, color='r');\n",
    "    label_probs = F.one_hot(torch.LongTensor(answer_indices if 'text' not in model_name else answer_indices_))\n",
    "    _ = sns.heatmap(torch.cat([torch.Tensor(candidate_probs), label_probs], dim=1).T, cbar=False, ax=ax1);\n",
    "    plt.show()\n",
    "    print(model_name, len(tokens), loss, np.array(top1_corrects[k_shot:]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e5780be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁The', 37),\n",
       " ('▁capital', 1784),\n",
       " ('▁of', 13),\n",
       " ('▁Canada', 1894),\n",
       " ('▁is', 19),\n",
       " ('</s>', 1)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Ottawa. It is the largest city in Canada'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The capital of Canada is'\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids\n",
    "list(zip(tokenizer.convert_ids_to_tokens(input_ids[0]), input_ids[0].numpy()))\n",
    "outputs = model.generate(input_ids, max_length=10)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "8c9c689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = 5; k_shot = nrows // 2 + 1\n",
    "# for pairs in [drop_first_and_last, ]:\n",
    "nrows = 6;  k_shot = 3\n",
    "for pairs in reversible_transformations + irreversible_transformations:\n",
    "    seps = [' -> ', '->'] if random.random() < 0.5 else ['->', ' -> ']\n",
    "    # seps = [' -> ', ' -> ']\n",
    "    samples = ['\\n' + '\\n'.join(a + seps[0] + b for a, b in sample(pairs, nrows)) + '\\n']\n",
    "    for s in samples: data_tuples.append(list(make_data_tuple(s, tokenizer, k_shot=k_shot, bos_token=tokenizer.tokenize(seps[0])[0])))\n",
    "    samples = ['\\n' + '\\n'.join(b + seps[1] + a for a, b in sample(pairs, nrows)) + '\\n' if pairs in reversible_transformations else \n",
    "                '\\n' + '\\n'.join(a + seps[1] + b for a, b in sample(pairs, nrows)) + '\\n']\n",
    "    for s in samples: data_tuples.append(list(make_data_tuple(s, tokenizer, k_shot=k_shot, bos_token=tokenizer.tokenize(seps[1])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sys.path.insert(0, '/nas/xd/projects/ec')\n",
    "# from child_utils import loadPBETasks, retrieveJSONTasks\n",
    "# challenge, challengeCheating = loadPBETasks('/nas/xd/projects/ec/PBE_Strings_Track')\n",
    "# challenge2, challengeCheating2 = loadPBETasks('/nas/xd/projects/ec/data/sygus')\n",
    "# tasks = retrieveJSONTasks(\"/nas/xd/projects/ec/data/list_tasks.json\")\n",
    "# tasks2 = retrieveJSONTasks(\"/nas/xd/projects/ec/data/list_tasks2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxy_utils import get_examples_behind, get_examples_before, get_examples_query_before, \\\n",
    "    get_examples_query_behid, get_examples_query_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432fcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversible_transformations = [list(digit2cardinal.items()), noun2adj, lxy, verb_form, country2capital, en2fr, antonyms]\n",
    "irreversible_transformations = [capabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cbab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fbc67151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ... done 0:00:01\n",
      "* C 0.842 {' C': 0.842, ' A': 0.041, ' B': 0.04} \t D -> C\n",
      "* Thursday 0.778 {' Thursday': 0.778, ' Wednesday': 0.064, ' Friday': 0.063} \t Friday -> Thursday\n",
      "* a 0.742 {' a': 0.742, ' c': 0.051, ' A': 0.036} \t b -> a\n",
      "* four 0.472 {' four': 0.472, ' three': 0.246, ' one': 0.105} \t five -> four\n",
      "tensor(0.3677, grad_fn=<NllLossBackward>) True \n",
      "\n",
      "0.3677041530609131\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for model_name, (model, tokenizer) in models.items():\n",
    "    if any(model_name.startswith(s) for s in ['gpt2-', 'KoboldAI/fairseq-dense', 'text-davinci-001', ]): continue\n",
    "    if not model_name == 'EleutherAI/gpt-j-6B': continue\n",
    "    if not isinstance(model, types.FunctionType): _ = model.eval()\n",
    "    with Timer(model_name): outputs = model(**inputs)\n",
    "    options_ids_list = [[tokenizer.encode(' ' + option)[0] for option in options] for cxt, query, options, ans in _examples]\n",
    "    mask_logits_fn = partial(mask_logits, indices=bos_indices, kept_ids=options_ids_list)\n",
    "    loss, all_top1_correct = show_predictions(text, examples, tokenizer, outputs.logits, bos_indices, eos_indices, answers, labels,\n",
    "                    mask_logits_fn=None, topk=3, loss_reduction='mean', show_range=range(k_shot, len(examples)), sep='\\t')\n",
    "    print(loss, all_top1_correct, '\\n')\n",
    "    losses.append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "    if model_name == 'EleutherAI/gpt-j-6B': break\n",
    "print(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc90b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_functions = [prev(), next()]\n",
    "rel_fns = [prevs, nexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834550",
   "metadata": {},
   "source": [
    "**TODO: read children books for more posets**  \n",
    "**TODO: Prompt gpt3 to elicit the posets it knows**  \n",
    "$x \\to f(x)$ where $f \\in \\{\\text{prev/next in posets of numbers/letters/months/days, antonym, hypernym, hyponym, ...}\\}$  \n",
    "$x \\to f^2(x)$  \n",
    "one poset or mixed posets  \n",
    "$x, f(x).~y \\to Ff^{[-1]}(y)$ one poset or mixed posets  \n",
    "$x, f^k(x).~y \\to Ff^{[-1]}(y)~/Ff^{[-]k}(y)$  \n",
    "$x, f(f(x))~/f(f(x)), x \\to f(x)$ in between, the simplest form of sequence completion  \n",
    "$x, f(x) \\to Gf$ where $Gf \\in \\{<, >\\}$  \n",
    "$x, f(x); y, g(y) \\to Ff \\stackrel{?}{=} g^{[-1]}$ where $\\text{output} \\in \\{\\text{True}, \\text{False}\\}$  \n",
    "sort\n",
    "\n",
    "There is a *natural* monotone map/functor $F$ between posets/sets $A$ and $B$.  Compose the computation (set operations, sorting etc.) between $A$ and $B$ with $F$ to make harder tasks.  \n",
    "$P(A) ,P(B) \\to F(P(A)) \\setminus ~/ \\cap ~/ \\triangle P(B)$. Harder form of set difference/intersection.  \n",
    "$P(A) \\to F(\\text{sorted}(P(A)))$. Harder form of sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "17373019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: replace with the other. For example:\n",
      "G H G G G -> H G H H H\n",
      "I I I I M -> M M M M I\n",
      "A A F A A -> F F A F F\n",
      "9 9 9 I I -> I I I 9 9\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "V Q Q V V -> Q V V Q Q\n",
      "G L L G L -> L G G L G\n",
      "G 2 2 2 G -> 2 G G G 2\n",
      "I I Z Z Z -> Z Z I I I\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "R H H H R -> H R R R H\n",
      "B 9 9 B B -> 9 B B 9 9\n",
      "D 2 2 2 D -> 2 D D D 2\n",
      "A A A A W -> W W W W A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_total, n_valid = 192, 64\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s.count('Yes') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6ebf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,\n",
    "    lr_scheduler_type='constant', learning_rate=5e-3, num_train_epochs=4,\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,\n",
    "    no_cuda=True, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
