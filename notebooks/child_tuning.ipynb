{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7292808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "54a886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce, partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed, AdamW\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from transformers.trainer_utils import EvaluationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "49c8f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/nas/xd/projects/PyFunctional')\n",
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _\n",
    "from collections import namedtuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "58cba5e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from child_utils import *\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f62ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}\n",
    "# cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "# proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "for model_name in ['gpt2-large']:#, 'gpt2-xl', 'KoboldAI/fairseq-dense-6.7B']:\n",
    "    if model_name not in models:\n",
    "        with Timer(model_name):\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)  \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "fd0bbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_model(engine):\n",
    "    def forward(input_ids, attention_mask=None):\n",
    "        text = tokenizer.decode(input_ids[0])\n",
    "        response = openai.Completion.create(engine=engine, prompt=text, max_tokens=0, echo=True, logprobs=5)\n",
    "        return Outputs(logits=response.choices[0].logprobs)\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "ebbfc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large\n",
      "gpt2-xl\n",
      "EleutherAI/gpt-neo-1.3B\n",
      "EleutherAI/gpt-j-6B\n",
      "KoboldAI/fairseq-dense-6.7B\n",
      "KoboldAI/fairseq-dense-13B\n",
      "text-babbage-001\n",
      "text-curie-001\n",
      "text-davinci-001\n",
      "text-davinci-002\n"
     ]
    }
   ],
   "source": [
    "for model_name in models: print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "4427a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = ['text-babbage-001', 'text-curie-001', 'text-davinci-001', 'text-davinci-002']\n",
    "for engine in engines:\n",
    "    model = get_openai_model(engine)\n",
    "    models[engine] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "c64283f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult young. Saturday -> Friday\n",
      "microgram nanogram. doctor -> master\n",
      "master bachelor. micrometer -> nanometer\n",
      "c d. large -> huge\n",
      "modern renaissance. doctor -> master\n",
      "g h. child -> teenager\n",
      "planet continent. h -> g\n",
      "d e. microgram -> milligram\n",
      "magnitude percent. large -> huge\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = open('/nas/xd/projects/openai_api_keys.txt').readlines()[-1].split()[0]\n",
    "# text = 'Once upon a time'\n",
    "response = openai.Completion.create(engine=engines[1], prompt=text, max_tokens=20, echo=True, logprobs=5)\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052c1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = 'Ġ!'; prompt_id = tokenizer._convert_token_to_id(prompt_token)\n",
    "bop_str = 'Instruction: '; bop_id = tokenizer.encode(bop_str)[0]  # 'Inst'\n",
    "eop_str = '. For example:'; eop_id = tokenizer.encode(eop_str)[2] # 'Ġexample'\n",
    "bos_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "eos_id = tokenizer._convert_token_to_id('Ċ')\n",
    "\n",
    "\n",
    "class CHILDDataset(Dataset):\n",
    "    def __init__(self, input_strs, tokenizer):\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = '!'\n",
    "        self.inputs = tokenizer.batch_encode_plus(input_strs, add_special_tokens=False, padding=True, return_tensors='pt')\n",
    "        input_ids = self.inputs.input_ids\n",
    "        self.labels = torch.ones_like(input_ids) * (-100)\n",
    "        for bi in range(input_ids.size(0)):\n",
    "            bop_idx = (input_ids[bi] == bop_id).nonzero().squeeze(1)\n",
    "            eop_idx = (input_ids[bi] == eop_id).nonzero().squeeze(1)\n",
    "            if len(bop_idx) > 0:\n",
    "                assert len(bop_idx) == 1 and len(eop_idx) == 1\n",
    "                bop_idx, eop_idx = bop_idx.item(), eop_idx.item()\n",
    "                input_ids[bi, bop_idx: eop_idx + 2] *= -1  # use prompt embedding for prompt tokens\n",
    "            \n",
    "            bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "            eos_indices = (input_ids[bi] == eos_id).nonzero()[-len(bos_indices):].squeeze(1)\n",
    "            for i, (bos_i, eos_i) in enumerate(zip(bos_indices.tolist(), eos_indices.tolist())):\n",
    "                assert eos_i > bos_i + 1\n",
    "                if i >= 2: self.labels[bi, bos_i + 1: eos_i] = input_ids[bi, bos_i + 1: eos_i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(sel f.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.inputs['input_ids'][i],\n",
    "                'attention_mask': self.inputs['attention_mask'][i],\n",
    "                'labels': self.labels[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "56fcd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                prompt_id: int = None,\n",
    "                prompt_len: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(WrappedEmbedding, self).__init__()\n",
    "#         self.wte = wte\n",
    "#         self.prompt_id = prompt_id\n",
    "#         self.prompt_len = prompt_len\n",
    "        self.__dict__.update(locals()); del self.self\n",
    "        if self.prompt_id is not None:\n",
    "            self.prompt_embedding = nn.parameter.Parameter(\n",
    "                self.initialize_embedding(random_range, initialize_from_vocab)).to(self.wte.weight.device)\n",
    "        else:\n",
    "            self.prompt_embedding = nn.Embedding(self.prompt_len, self.wte.weight.size(1)).to(self.wte.weight.device)\n",
    "            assert initialize_from_vocab\n",
    "            self.init_prompt_embedding_()\n",
    "#             self.prompt_embedding.weight.data = self.initialize_embedding(random_range, initialize_from_vocab)     \n",
    "            \n",
    "    def initialize_embedding(self, random_range: float = 0.5, initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab: return self.wte.weight[:self.prompt_len].clone().detach()\n",
    "        return torch.FloatTensor(self.prompt_len, self.wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "    \n",
    "    def init_prompt_embedding_(self):\n",
    "        self.prompt_embedding.weight.data[:] = self.wte.weight[:self.prompt_len]\n",
    "            \n",
    "    def forward(self, input_ids):\n",
    "        if self.prompt_id is not None:\n",
    "            input_embeds = self.wte(input_ids)\n",
    "            input_embeds[input_ids == self.prompt_id] = self.prompt_embedding.expand(input_embeds.size(0), -1, -1)\n",
    "        else: # adapted from cpm-2\n",
    "            prompt_mask = input_ids < 0\n",
    "            prompt_ids = -input_ids * prompt_mask\n",
    "            assert torch.all(prompt_ids < self.prompt_len)\n",
    "            p_embeds = self.prompt_embedding(prompt_ids) * prompt_mask.float().unsqueeze(-1)\n",
    "            input_ids = input_ids * ~prompt_mask\n",
    "            w_embeds = self.wte(input_ids) * (~prompt_mask).float().unsqueeze(-1)\n",
    "            input_embeds = w_embeds + p_embeds\n",
    "        return input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b17b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from cpm-2: https://github.com/TsinghuaAI/CPM-2-Finetune/blob/master/utils.py#L133-L164\n",
    "def get_params_for_prompt_optimization(module: nn.Module):\n",
    "    params = []\n",
    "    for t in module.named_modules():\n",
    "        if \"prompt_embedding\" in t[0]:\n",
    "            params.append({'params': [p for p in list(t[1]._parameters.values()) if p is not None]})\n",
    "    for t in module.named_parameters():\n",
    "        if \"prompt\" not in t[0]:\n",
    "            t[1].requires_grad_(False)    \n",
    "    return params\n",
    "\n",
    "def create_optimizer(model, training_args):\n",
    "    from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "    while isinstance(model, (DDP, )): model = model.module\n",
    "    we.init_prompt_embedding_()\n",
    "    param_groups = get_params_for_prompt_optimization(model)\n",
    "    optimizer = AdamW(param_groups, lr=training_args.learning_rate, \n",
    "                      betas=(training_args.adam_beta1, training_args.adam_beta2),eps=training_args.adam_epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "39b4f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = model.get_input_embeddings()\n",
    "if hasattr(wte, 'wte'): wte = wte.wte  # already been wrapped\n",
    "we = WrappedEmbedding(wte, prompt_len=10000)\n",
    "model.set_input_embeddings(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ae3bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalize(obj):\n",
    "    if type(obj) == bool: return 'Yes' if obj else 'No'\n",
    "    return str(obj)\n",
    "    \n",
    "def make_query_str(instruction, query):\n",
    "    if instruction is None and query is None: return ''\n",
    "    s = '.'\n",
    "    if instruction is not None: s = s + ' ' + instruction\n",
    "    if query is not None:\n",
    "        if type(query) in [int, bool, str]: query = [query]\n",
    "        if type(query) == dict:\n",
    "    #         return '. ' + '{' + ','.join([' %s: %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "            s = s + ' ' + '{' + ','.join([' replace %s with %s' % (str(k), str(v)) for k, v in query.items()]) + ' }'\n",
    "        elif type(query) in [list,]:\n",
    "            s = s + ' ' + ' '.join([str(i) for i in query])\n",
    "    return s\n",
    "\n",
    "def make_example_str(example, with_instruction=False):\n",
    "    instruction, l, query, ans = example\n",
    "    if type(ans) not in [Sequence, list]: ans = [ans]\n",
    "    ans = [verbalize(a) for a in ans]\n",
    "    return '%s -> %s' % (' '.join(l) + make_query_str(instruction if with_instruction else None, query), ' '.join(ans))\n",
    "\n",
    "def sample_rand_len(vocab, k): return sample(vocab, k=randint(1, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "5d1d0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _str(l, sep=' : '):\n",
    "#     if l is None: return ''\n",
    "#     if isinstance(l, str) or not isinstance(l, collections.abc.Iterable): l = [l]\n",
    "#     l = [e for e in l if not my_isinstance(e, Sequence)] #type(e).__name__ != 'Sequence']\n",
    "#     if isinstance(l, (dict, OrderedDict)): l = [f'{k}: {v}' for k, v in l.items()]\n",
    "#     return sep.join(str(i) for i in l)\n",
    "\n",
    "# def options2str(options): return '[' + ' | '.join(options) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "df63ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptize(s):\n",
    "#     return prompt_token * len(s.split())\n",
    "    return bop_str + s + eop_str\n",
    "    \n",
    "def make_examples(task, nrows=4, ncols=4, full_vocab=None):\n",
    "    if full_vocab is None: full_vocab = string.ascii_uppercase + string.digits\n",
    "    transform_fn, vocab_fn, sample_fn, query_fn = task[:4]\n",
    "    # instruction = transform_fn.__name__.replace('_', ' ')\n",
    "    if vocab_fn is None: vocab_fn = lambda: full_vocab\n",
    "    if query_fn is None: query_fn = lambda *_: None\n",
    "\n",
    "    examples = []\n",
    "    query = None\n",
    "    for i in range(nrows):\n",
    "        vocab = vocab_fn()\n",
    "        cxt = sample_fn(vocab)#, k=ncols)\n",
    "        query, options = query_fn(cxt, vocab)#, ncols)\n",
    "        examples.append([cxt, query, options, transform_fn(cxt, query)])\n",
    "    return examples\n",
    "\n",
    "def make_input_str(task, examples, options_position=None):\n",
    "    task += (_str,) * (4 + 3 - len(task))\n",
    "    cxt2str, query2str, ans2str = task[4:]\n",
    "\n",
    "    def example2str(example, with_instruction=False):\n",
    "        cxt, query, options, ans = example\n",
    "        strs = [cxt2str(cxt), query2str(query)]\n",
    "        if options_position is not None: strs.insert(options_position, options2str(options))\n",
    "        # strs = [options2str(options)] + strs if options_position == 'pre' else strs + [options2str(options)]\n",
    "        return '. '.join(s for s in strs if s != '') + ' -> ' + ans2str(ans)\n",
    "\n",
    "    desc = promptize(instruction) + '\\n' if False else ''\n",
    "    text = '\\n'.join(example2str(e) for e in examples)\n",
    "    text = desc + text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "ce0d0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(examples, ans_vocab=[True, False]):\n",
    "    groups = seq(examples).group_by(_[-1]).map(_[1])  # 按ans分组\n",
    "    assert groups.len() == len(ans_vocab)  # 保证每种ans都出现\n",
    "    min_cnt = groups.map(lambda x: len(x)).min()\n",
    "    examples = groups.map(lambda x: sample(x, min_cnt)).flatten().list() # 每组都采样最小个数后去分组\n",
    "    return sample(examples, len(examples))  # 重新打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "856bb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_a(cxt, query):\n",
    "    SC, CD = cxt  # SC paris: studeng-course relation, CD pairs: course-department function\n",
    "    ss, d = query  # ss: 学生子集（可以*不止两个学生*），d: 课程\n",
    "#     return seq(ss).map(lambda s: seq(SC).filter(_[0] == s).map(_[1]).intersection(CD.filter(_[1] == d).map(_[0])).non_empty()).all()\n",
    "    return (seq(ss)\n",
    "            .map(lambda s: seq(SC).filter(_[0] == s).map(_[1])  # 学生s选的所有课程\n",
    "                 .intersection(\n",
    "                     seq(CD).filter(_[1] == d).map(_[0])) # d系的课程\n",
    "                 .non_empty())  # s选了d系的课程\n",
    "            .all())  # 学生子集ss都选了d系的课程\n",
    "\n",
    "def all_a_sample(vocab, k):\n",
    "    S_vocab, C_vocab, D_vocab = vocab  # vocabs of students, courses, departments\n",
    "    k_S, k_C, k_D, k_SC = k  # default values: k_S = 3, k_C = 3, k_D = 2, k_SC = 5\n",
    "    S, C, D = sample(S_vocab, k_S), sample(C_vocab, k_C), sample(D_vocab, k_D)\n",
    "    \n",
    "    while len(set(CD := choices(D, k=k_C))) < k_D: continue  # ds里每个系的课都要出现\n",
    "    CD = list(zip(C, CD))  # 得到每门课所属的系\n",
    "    \n",
    "    all_SC = list(itertools.product(S, C))  # or seq(S).cartesian(C).list()\n",
    "    while seq(SC := sample(all_SC, k_SC)).map(_[0]).distinct().len() < k_S: continue  # ss里每个学生都要选课\n",
    "    return SC, CD\n",
    "\n",
    "def select_distinct(tuples, col): return seq(tuples).map(_[col]).distinct().list()\n",
    "    \n",
    "def all_a_query(cxt,vocab,k):\n",
    "    SC, CD = cxt\n",
    "    k_S, k_C, k_D, k_SC = k\n",
    "    S, D = select_distinct(SC, 0), select_distinct(CD, 1)\n",
    "    k_ss = randint(2, len(S))\n",
    "    ss = sample(S, k_ss)\n",
    "    d = choice(D)\n",
    "    return ss, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "62aee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_element(l, query=None): return seq(l).slice(1, 2)\n",
    "def ith_group(l, query=None): return seq(l).group_by(_).select(_[1]).slice(1, 2).flatten()#.distinct()# davinci F w/ and wo dist\n",
    "# def element_at_index(l, query): return seq(l).slice(query, query + 1) # davinci F\n",
    "def element_at_index(l, query): return seq(l).enumerate().filter(_[0] == query).select(_[1])\n",
    "def replace(l, query): return seq(l).map(lambda x: query.get(x, x))\n",
    "def replace_with_the_other(l, query): # davinci F\n",
    "    query = {k: (set(l) - {k}).pop() for k in l}\n",
    "    return replace(l, query)\n",
    "def replace_all_with(l, query): return seq(l).map(lambda x: query)  # davinci F?!\n",
    "def interleave_with(l, query): return seq(l).flat_map(lambda x: [x, query])  # davinci T!!\n",
    "def unique_elements(l, query=None): return seq(l).distinct() # davinci F\n",
    "def how_many_unique_elements(l, query=None): return seq(l).distinct().len()  # davinci F\n",
    "def how_many(l, query): return seq(l).filter(_ == query).len() # davinci F\n",
    "def select_same_as(l, query): return seq(l).filter(_ == query) # simpler version of how_many. davinci F\n",
    "def select_same_number_as(l, query): return seq(l).group_by(_).select(_[1]).filter(lambda x: len(x) == len(query)).flatten() # F\n",
    "def includes(l, query): return seq(l).union(seq(query)).distinct().len() == seq(l).distinct().len() # davinci F\n",
    "def is_included_by(l, query): return seq(l).difference(seq(query)).empty() # davinci F\n",
    "\n",
    "tasks = [\n",
    "    (ith_element,            None,                               sample,    None),\n",
    "    (ith_group,              None, lambda vocab, k: seq(sample(vocab, k)).map(lambda x:[x]*randint(1, 3)).flatten().list(),None),\n",
    "    (element_at_index,       lambda: upper_letters,              sample,    lambda l,vocab,k: randint(0, min(2,len(l)-1))),\n",
    "    (replace,                None,                               sample,    lambda l,vocab,k: {choice(l): choice(vocab)}),\n",
    "    (replace_with_the_other, lambda: sample(full_vocab, 2),   lambda vocab,k: sample(vocab+choices(vocab, k=k-2),k), None),\n",
    "    (replace_all_with,       None,                               sample_rand_len, lambda l,vocab,k: choice(vocab)),\n",
    "    (interleave_with,        None,                               sample_rand_len, lambda l,vocab,k: choice(vocab)),\n",
    "    (unique_elements,        lambda: sample(upper_letters, 3),   choices,   None),\n",
    "    (how_many_unique_elements,lambda: sample(upper_letters, 3),  choices,   None),\n",
    "    (how_many,               lambda: sample(upper_letters, 3),   choices,   lambda l,vocab,k: choice(list(set(l)))),\n",
    "    (select_same_as,         lambda: sample(upper_letters, 3),   choices,   lambda l,vocab,k: choice(list(set(l)))),\n",
    "    (select_same_number_as,  None, lambda vocab, k: seq(sample(vocab, k)).map(lambda x:[x]*randint(1, 3)).flatten().list(),   \n",
    "     lambda l,vocab,k: [choice(vocab)]*randint(1, 3)),\n",
    "    (includes,               lambda: sample(upper_letters, 6),   sample,    lambda l,vocab,k: sample(vocab, 3)),\n",
    "    (is_included_by,         lambda: sample(upper_letters, 6),   sample,    lambda l,vocab,k: sample(vocab, 5)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "7a87572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_side_query(r, p):\n",
    "    e = r.dom().init().tail().a(choice)\n",
    "    options = r.image(e).map(beside)[0].a(sample, 2)\n",
    "    return e, options\n",
    "\n",
    "def after(r, q): return r.image(q).map(next())[0]\n",
    "def before(r, q): return r.image(q).map(prev())[0]\n",
    "def between(r, q): \n",
    "    return r.image(q[0]).map(nexts)[0].intersection(r.image(q[1]).map(prevs)[0]).union(\n",
    "    r.image(q[0]).map(prevs)[0].intersection(r.image(q[1]).map(nexts)[0]))\n",
    "    \n",
    "def monotone_map_cxt(vocab):\n",
    "    P, p = vocab\n",
    "    R = p2r(P)\n",
    "    E1 = R.dom().init().tail().a(choice)\n",
    "    E2 = R.image(E1).map(beside)[0].a(choice)\n",
    "    return R, E1, E2\n",
    "\n",
    "def monotone_map_query(cxt, vocab):\n",
    "    P, p = vocab\n",
    "    r = p2r(p)\n",
    "    e1 = r.dom().init().tail().a(choice)\n",
    "    options = r.image(e1).map(beside)[0]\n",
    "    return (r, e1), options\n",
    "\n",
    "def monotone_map(cxt, query, reverse=False):\n",
    "    R, E1, E2 = cxt\n",
    "    r, e1 = query\n",
    "    return r.image(e1).map(\n",
    "        seq([prev(), next()]).find(lambda f: (E2 in R.image(E1).map(f)[0]) != reverse)  # reverse = not in. too tricky\n",
    "    )[0]\n",
    "    \n",
    "tasks = [\n",
    "    (after, lambda: choice(closed_posets), p2r, which_side_query, lambda r: ''),\n",
    "    (before, lambda: choice(closed_posets), p2r, which_side_query, lambda r: ''),\n",
    "    (between, lambda: choice(posets), p2r, lambda r, p: r.image(r.dom().init().tail().a(choice)).map(beside)[0].a(sample, 2), lambda r: ''),\n",
    "    (partial(monotone_map, reverse=False), lambda: sample(posets, 2), monotone_map_cxt, monotone_map_query),\n",
    "    (partial(monotone_map, reverse=True), lambda: sample(closed_posets, 2), monotone_map_cxt, monotone_map_query),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "298d0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons = ['triangle', 'quadrangle', 'pentagon', 'hexagon', 'heptagon', 'octagon', 'nonagon', 'decagon',]# 'undecagon', 'dodecagon']\n",
    "times_of_day = ['dawn', 'morning', 'noon', 'afternoon', 'evening', 'night',]# 'midnight']\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "seasons = ['spring', 'summer', 'autumn', 'winter']\n",
    "ages_of_life = ['baby', 'child', 'teenager', 'young', 'adult', 'elder']\n",
    "times_of_history = ['ancient', 'medieval', 'renaissance', 'modern', 'contemporary']\n",
    "units_of_time = ['nanosecond', 'microsecond', 'millisecond', ][:0] + ['second', 'minute', 'hour', 'day', 'week', 'month', 'year', 'decade', 'century', 'millennium'] # first 3 multi-token\n",
    "units_of_length = ['nanometer', 'micrometer', 'millimeter', 'meter', 'kilometer', 'mile']\n",
    "units_of_mass = ['nanogram', 'microgram', 'milligram', 'gram', 'kilogram', 'ton']\n",
    "# SI_prefixes_small = ['pico', 'nana', 'micro', 'milli', 'centi', 'deci']\n",
    "# SI_prefixes_large = ['kilo', 'mega', 'giga', 'tera', 'peta', 'exa', 'zetta', 'yotta']\n",
    "\n",
    "things = ['atom', 'molecule', 'cell', 'tissue', 'organ', 'system', 'person', 'community', 'city', 'state', 'country', 'continent', 'planet', 'star', 'galaxy', 'universe']\n",
    "adjectives = ['tiny', 'small', 'medium', 'large', 'huge',]# 'gigantic']\n",
    "degrees = ['bachelor', 'master', 'doctor', 'postdoc']\n",
    "posets = [list(string.ascii_uppercase)[:14], list(string.ascii_lowercase)[:14], digits, times_of_day, days_of_week, months, seasons, ages_of_life, times_of_history, \n",
    "    units_of_time, things, adjectives, degrees]# units_of_length, units_of_mass, SI_prefixes_small, SI_prefixes_large]\n",
    "closed_posets = [list(string.ascii_uppercase)[:14], list(string.ascii_lowercase)[:14], digits, days_of_week, months, seasons, units_of_time]\n",
    "open_posets = [times_of_day, ages_of_life, times_of_history, units_of_length, units_of_mass, things, adjectives, degrees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834550",
   "metadata": {},
   "source": [
    "**TODO: read children books for more posets**  \n",
    "**TODO: Prompt gpt3 to elicit the posets it knows**  \n",
    "$x \\to f(x)$ where $f \\in \\{\\text{prev/next in posets of numbers/letters/months/days, antonym, hypernym, hyponym, ...}\\}$  \n",
    "$x \\to f^2(x)$  \n",
    "one poset or mixed posets  \n",
    "$x, f(x).~y \\to Ff^{[-1]}(y)$ one poset or mixed posets  \n",
    "$x, f^k(x).~y \\to Ff^{[-1]}(y)~/Ff^{[-]k}(y)$  \n",
    "$x, f(f(x))~/f(f(x)), x \\to f(x)$ in between, the simplest form of sequence completion  \n",
    "$x, f(x) \\to Gf$ where $Gf \\in \\{<, >\\}$  \n",
    "$x, f(x); y, g(y) \\to Ff \\stackrel{?}{=} g^{[-1]}$ where $\\text{output} \\in \\{\\text{True}, \\text{False}\\}$  \n",
    "sort\n",
    "\n",
    "There is a *natural* monotone map/functor $F$ between posets/sets $A$ and $B$.  Compose the computation (set operations, sorting etc.) between $A$ and $B$ with $F$ to make harder tasks.  \n",
    "$P(A) ,P(B) \\to F(P(A)) \\setminus ~/ \\cap ~/ \\triangle P(B)$. Harder form of set difference/intersection.  \n",
    "$P(A) \\to F(\\text{sorted}(P(A)))$. Harder form of sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "1cbf4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _str(l, sep=' '):\n",
    "    if l is None: return ''\n",
    "    if isinstance(l, str) or not isinstance(l, collections.abc.Iterable): l = [l]\n",
    "    l = [e for e in l if not my_isinstance(e, Sequence)] #type(e).__name__ != 'Sequence']\n",
    "    if isinstance(l, (dict, OrderedDict)): l = [f'{k}: {v}' for k, v in l.items()]\n",
    "    return sep.join(str(i) for i in l)\n",
    "\n",
    "def options2str(options): return '[' + ' | '.join(options) + ']'\n",
    "# def options2str(options): return ' or '.join(options) + '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "4059d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks[1]\n",
    "_examples = make_examples(task, nrows=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "id": "cf1d8261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autumn -> summer\n",
      "Saturday -> Friday\n",
      "M -> L\n",
      "April -> March\n",
      "C -> B\n",
      "k -> j\n",
      "2 -> 1\n",
      "B -> A\n",
      "hour -> minute\n",
      "i -> h\n",
      "month -> week\n",
      "F -> E\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = make_input_str(task, _examples, options_position=None)\n",
    "examples = text.strip().split('\\n')\n",
    "k_shot = 2\n",
    "print(text)\n",
    "inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "input_ids = inputs.input_ids\n",
    "options_ids = [[tokenizer.encode(' ' + option)[0] for option in options] for cxt, query, options, ans in _examples]\n",
    "bos_indices, eos_indices, answers, labels = locate_answers(input_ids, tokenizer)\n",
    "# logits_mask = make_logits_mask(bos_indices, options_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "fbc67151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large ... done 0:00:00\n",
      "  L 0.453 {' N': 0.547, ' L': 0.453, '\"': 0.0}\tM -> L\n",
      "  March 0.203 {' May': 0.797, ' March': 0.203, '\"': 0.0}\tApril -> March\n",
      "  B 0.203 {' D': 0.797, ' B': 0.203, '\"': 0.0}\tC -> B\n",
      "* j 0.53 {' j': 0.53, ' l': 0.47, '\"': 0.0}\tk -> j\n",
      "  1 0.327 {' 3': 0.673, ' 1': 0.327, '\"': 0.0}\t2 -> 1\n",
      "  A 0.418 {' C': 0.582, ' A': 0.418, '\"': 0.0}\tB -> A\n",
      "* minute 0.95 {' minute': 0.95, ' day': 0.05, '\"': 0.0}\thour -> minute\n",
      "  h 0.03 {' j': 0.97, ' h': 0.03, '\"': 0.0}\ti -> h\n",
      "  week 0.111 {' year': 0.889, ' week': 0.111, '\"': 0.0}\tmonth -> week\n",
      "  E 0.175 {' G': 0.825, ' E': 0.175, '\"': 0.0}\tF -> E\n",
      "tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "gpt2-xl ... done 0:00:00\n",
      "  L 0.171 {' N': 0.829, ' L': 0.171, '\"': 0.0}\tM -> L\n",
      "  March 0.012 {' May': 0.988, ' March': 0.012, '\"': 0.0}\tApril -> March\n",
      "  B 0.191 {' D': 0.809, ' B': 0.191, '\"': 0.0}\tC -> B\n",
      "* j 0.817 {' j': 0.817, ' l': 0.183, '\"': 0.0}\tk -> j\n",
      "* 1 0.955 {' 1': 0.955, ' 3': 0.045, '\"': 0.0}\t2 -> 1\n",
      "* A 0.911 {' A': 0.911, ' C': 0.089, '\"': 0.0}\tB -> A\n",
      "* minute 0.76 {' minute': 0.76, ' day': 0.24, '\"': 0.0}\thour -> minute\n",
      "  h 0.033 {' j': 0.967, ' h': 0.033, '\"': 0.0}\ti -> h\n",
      "  week 0.155 {' year': 0.845, ' week': 0.155, '\"': 0.0}\tmonth -> week\n",
      "  E 0.271 {' G': 0.729, ' E': 0.271, '\"': 0.0}\tF -> E\n",
      "tensor(1.5717, grad_fn=<NllLossBackward>)\n",
      "EleutherAI/gpt-neo-1.3B ... done 0:00:00\n",
      "  L 0.208 {' N': 0.792, ' L': 0.208, '\"': 0.0}\tM -> L\n",
      "  March 0.182 {' May': 0.818, ' March': 0.182, '\"': 0.0}\tApril -> March\n",
      "  B 0.155 {' D': 0.845, ' B': 0.155, '\"': 0.0}\tC -> B\n",
      "  j 0.086 {' l': 0.914, ' j': 0.086, '\"': 0.0}\tk -> j\n",
      "* 1 0.81 {' 1': 0.81, ' 3': 0.19, '\"': 0.0}\t2 -> 1\n",
      "* A 0.919 {' A': 0.919, ' C': 0.081, '\"': 0.0}\tB -> A\n",
      "* minute 0.787 {' minute': 0.787, ' day': 0.213, '\"': 0.0}\thour -> minute\n",
      "  h 0.251 {' j': 0.749, ' h': 0.251, '\"': 0.0}\ti -> h\n",
      "  week 0.331 {' year': 0.669, ' week': 0.331, '\"': 0.0}\tmonth -> week\n",
      "* E 0.517 {' E': 0.517, ' G': 0.483, '\"': 0.0}\tF -> E\n",
      "tensor(1.3546, grad_fn=<NllLossBackward>)\n",
      "EleutherAI/gpt-j-6B ... done 0:00:01\n",
      "  L 0.14 {' N': 0.86, ' L': 0.14, '\"': 0.0}\tM -> L\n",
      "* March 0.877 {' March': 0.877, ' May': 0.123, '\"': 0.0}\tApril -> March\n",
      "* B 0.834 {' B': 0.834, ' D': 0.166, '\"': 0.0}\tC -> B\n",
      "* j 0.839 {' j': 0.839, ' l': 0.161, '\"': 0.0}\tk -> j\n",
      "* 1 0.88 {' 1': 0.88, ' 3': 0.12, '\"': 0.0}\t2 -> 1\n",
      "* A 0.845 {' A': 0.845, ' C': 0.155, '\"': 0.0}\tB -> A\n",
      "* minute 0.783 {' minute': 0.783, ' day': 0.217, '\"': 0.0}\thour -> minute\n",
      "  h 0.053 {' j': 0.947, ' h': 0.053, '\"': 0.0}\ti -> h\n",
      "  week 0.24 {' year': 0.76, ' week': 0.24, '\"': 0.0}\tmonth -> week\n",
      "* E 0.932 {' E': 0.932, ' G': 0.068, '\"': 0.0}\tF -> E\n",
      "tensor(0.9088, grad_fn=<NllLossBackward>)\n",
      "KoboldAI/fairseq-dense-6.7B ... done 0:00:01\n",
      "  L 0.07 {' N': 0.93, ' L': 0.07, '\"': 0.0}\tM -> L\n",
      "  March 0.185 {' May': 0.815, ' March': 0.185, '\"': 0.0}\tApril -> March\n",
      "  B 0.254 {' D': 0.746, ' B': 0.254, '\"': 0.0}\tC -> B\n",
      "* j 0.558 {' j': 0.558, ' l': 0.442, '\"': 0.0}\tk -> j\n",
      "* 1 0.674 {' 1': 0.674, ' 3': 0.326, '\"': 0.0}\t2 -> 1\n",
      "  A 0.318 {' C': 0.682, ' A': 0.318, '\"': 0.0}\tB -> A\n",
      "* minute 0.679 {' minute': 0.679, ' day': 0.321, '\"': 0.0}\thour -> minute\n",
      "  h 0.066 {' j': 0.934, ' h': 0.066, '\"': 0.0}\ti -> h\n",
      "  week 0.167 {' year': 0.833, ' week': 0.167, '\"': 0.0}\tmonth -> week\n",
      "  E 0.498 {' G': 0.502, ' E': 0.498, '\"': 0.0}\tF -> E\n",
      "tensor(1.6397, grad_fn=<NllLossBackward>)\n",
      "KoboldAI/fairseq-dense-13B ... done 0:00:02\n",
      "  L 0.024 {' N': 0.976, ' L': 0.024, '\"': 0.0}\tM -> L\n",
      "  March 0.346 {' May': 0.654, ' March': 0.346, '\"': 0.0}\tApril -> March\n",
      "  B 0.192 {' D': 0.808, ' B': 0.192, '\"': 0.0}\tC -> B\n",
      "  j 0.083 {' l': 0.917, ' j': 0.083, '\"': 0.0}\tk -> j\n",
      "* 1 0.618 {' 1': 0.618, ' 3': 0.382, '\"': 0.0}\t2 -> 1\n",
      "* A 0.692 {' A': 0.692, ' C': 0.308, '\"': 0.0}\tB -> A\n",
      "* minute 0.865 {' minute': 0.865, ' day': 0.135, '\"': 0.0}\thour -> minute\n",
      "* h 0.537 {' h': 0.537, ' j': 0.463, '\"': 0.0}\ti -> h\n",
      "  week 0.341 {' year': 0.659, ' week': 0.341, '\"': 0.0}\tmonth -> week\n",
      "  E 0.253 {' G': 0.747, ' E': 0.253, '\"': 0.0}\tF -> E\n",
      "tensor(1.3818, grad_fn=<NllLossBackward>)\n",
      "text-babbage-001 ... done 0:00:01\n",
      "  L 0.001 {' A': 0.288, ' W': 0.154, ' T': 0.136}\tM -> L\n",
      "* March 0.928 {' March': 0.928, 'March': 0.034, ' May': 0.014}\tApril -> March\n",
      "  B 0.001 {' M': 0.614, ' S': 0.218, ' F': 0.068}\tC -> B\n",
      "  j 0.0 {' M': 0.389, ' Q': 0.106, ' I': 0.074}\tk -> j\n",
      "* 1 0.999 {' 1': 0.999, ' 0': 0.0, ' 4': 0.0}\t2 -> 1\n",
      "* A 0.984 {' A': 0.984, ' R': 0.007, ' S': 0.002}\tB -> A\n",
      "  minute 0.44 {' minutes': 0.48, ' minute': 0.44, ' Minute': 0.024}\thour -> minute\n",
      "  h 0.001 {' j': 0.916, ' o': 0.023, ' e': 0.012}\ti -> h\n",
      "  week 0.256 {' year': 0.693, ' week': 0.256, ' day': 0.012}\tmonth -> week\n",
      "  E 0.084 {' G': 0.649, ' S': 0.15, ' E': 0.084}\tF -> E\n",
      "3.3322113827\n",
      "text-curie-001 ... done 0:00:00\n",
      "  L 0.008 {' F': 0.986, ' L': 0.008, ' A': 0.003}\tM -> L\n",
      "* March 0.997 {' March': 0.997, ' November': 0.001, 'March': 0.001}\tApril -> March\n",
      "  B 0.132 {' A': 0.38, ' F': 0.212, ' B': 0.132}\tC -> B\n",
      "* j 0.767 {' j': 0.767, ' h': 0.181, ' a': 0.011}\tk -> j\n",
      "* 1 0.995 {' 1': 0.995, ' 4': 0.002, ' 0': 0.001}\t2 -> 1\n",
      "* A 0.999 {' A': 0.999, 'A': 0.001, ' K': 0.0}\tB -> A\n",
      "* minute 0.934 {' minute': 0.934, ' minutes': 0.046, 'minute': 0.013}\thour -> minute\n",
      "  h 0.007 {' o': 0.441, ' e': 0.188, ' l': 0.156}\ti -> h\n",
      "  week 0.013 {' day': 0.966, ' year': 0.015, ' week': 0.013}\tmonth -> week\n",
      "  E 0.384 {' D': 0.52, ' E': 0.384, ' S': 0.061}\tF -> E\n",
      "1.75385597511\n",
      "text-davinci-001 ... done 0:00:01\n",
      "  L 0.005 {' F': 0.69, ' W': 0.252, ' D': 0.031}\tM -> L\n",
      "  March 0.17 {' October': 0.214, ' January': 0.207, ' March': 0.17}\tApril -> March\n",
      "* B 0.921 {' B': 0.921, ' A': 0.042, ' D': 0.03}\tC -> B\n",
      "  j 0.202 {' g': 0.569, ' j': 0.202, ' c': 0.135}\tk -> j\n",
      "* 1 0.998 {' 1': 0.998, ' 3': 0.001, ' 10': 0.001}\t2 -> 1\n",
      "* A 0.999 {' A': 0.999, ' C': 0.001, ' D': 0.0}\tB -> A\n",
      "* minute 0.741 {' minute': 0.741, ' minutes': 0.234, ' o': 0.009}\thour -> minute\n",
      "  h 0.019 {' e': 0.932, ' y': 0.019, ' h': 0.019}\ti -> h\n",
      "  week 0.114 {' day': 0.88, ' week': 0.114, ' year': 0.003}\tmonth -> week\n",
      "* E 0.974 {' E': 0.974, ' D': 0.017, ' S': 0.009}\tF -> E\n",
      "1.5261686931\n",
      "text-davinci-002 ... done 0:00:00\n",
      "* L 0.234 {' L': 0.234, ' D': 0.102, ' M': 0.084}\tM -> L\n",
      "* March 0.878 {' March': 0.878, ' February': 0.037, ' May': 0.019}\tApril -> March\n",
      "* B 0.982 {' B': 0.982, ' A': 0.007, ' D': 0.006}\tC -> B\n",
      "* j 0.97 {' j': 0.97, ' i': 0.005, ' h': 0.004}\tk -> j\n",
      "* 1 0.988 {' 1': 0.988, ' 3': 0.004, ' 2': 0.002}\t2 -> 1\n",
      "* A 0.996 {' A': 0.996, ' C': 0.001, 'A': 0.0}\tB -> A\n",
      "* minute 0.717 {' minute': 0.717, ' half': 0.071, ' day': 0.05}\thour -> minute\n",
      "* h 0.99 {' h': 0.99, ' g': 0.004, ' e': 0.001}\ti -> h\n",
      "  week 0.223 {' day': 0.687, ' week': 0.223, ' second': 0.026}\tmonth -> week\n",
      "* E 0.989 {' E': 0.989, ' D': 0.006, ' G': 0.001}\tF -> E\n",
      "0.34995168480000005\n",
      "1.532851472345338\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for model_name, (model, tokenizer) in models.items():\n",
    "    if any(model_name.startswith(s) for s in ['gpt2-', 'KoboldAI/fairseq-dense', 'text-davinci-001', ]): continue\n",
    "    if not isinstance(model, types.FunctionType): _ = model.eval()\n",
    "    with Timer(model_name): o = model(**inputs)\n",
    "    options_ids_list = [[tokenizer.encode(' ' + option)[0] for option in options] for cxt, query, options, ans in _examples]\n",
    "    mask_logits_fn = partial(mask_logits, indices=bos_indices, kept_ids=options_ids_list)\n",
    "    loss, _ = show_predictions(text, examples, tokenizer, o.logits, bos_indices, eos_indices, answers, labels,\n",
    "                    mask_logits_fn=mask_logits_fn, topk=3, loss_reduction='mean', show_range=range(k_shot, len(examples)), sep='\\t')\n",
    "    print(loss)\n",
    "    losses.append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "    # break\n",
    "print(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "id": "1792d9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-davinci-002'"
      ]
     },
     "execution_count": 1229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "6f135452",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = seq(choice(posets))\n",
    "p = times_of_day\n",
    "r = p2r(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "28fd5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2r(p): p = seq(p); return p.zip(p.inits().zip(p.tails()))#.slice(1, p.len() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "f13deacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.zip(p.inits().zip(p.tails())).dom().init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "60151283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wednesday'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = r.dom().a(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "bc90b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_functions = [prev(), next()]\n",
    "rel_fns = [prevs, nexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e66d2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_fn = choice(relational_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "1cbe83ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wednesday'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Thursday', 'Friday', 'Saturday', 'Sunday']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e\n",
    "nexts(r.image(e)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "6f8aba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1, e2 = r.dom().a(sample, 2)\n",
    "e1, e2 = 'morning', 'afternoon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "e9673b53",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-809-d1e8ccf06f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nas/xd/projects/PyFunctional/functional/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/xd/projects/PyFunctional/functional/pipeline.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, delete_lineage)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lineage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCACHE_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lineage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCACHE_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdelete_lineage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "r.image(e1).map(seq(rel_fns).find(lambda f: e2 in r.image(e1).map(f)[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3c6cab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour(direction, k=1): return lambda x: x[direction][k]\n",
    "def prev(k=1): return neighbour(0, k)\n",
    "def next(k=1): return neighbour(1, k)\n",
    "prevs, nexts = _[0][1:], _[1][1:]\n",
    "beside = lambda x: (x[0][1], x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bbcf3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'C'\n",
    "l.zip(l.reverse().tails().reverse().tail().zip(l.tails())).filter(_[0] == query).map(_[1]).map(succ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e4848490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: select same as. For example:\n",
      "A L L A. A -> A A\n",
      "W B R W. R -> R\n",
      "J J J C. C -> C\n",
      "F F F F. F -> F F F F\n",
      "I M W W. W -> W W\n",
      "X X X X. X -> X X X X\n",
      "P P P P. P -> P P P P\n",
      "H G L L. H -> H\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_input_str(tasks[-4], nrows=8, ncols=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "17373019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: replace with the other. For example:\n",
      "G H G G G -> H G H H H\n",
      "I I I I M -> M M M M I\n",
      "A A F A A -> F F A F F\n",
      "9 9 9 I I -> I I I 9 9\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "V Q Q V V -> Q V V Q Q\n",
      "G L L G L -> L G G L G\n",
      "G 2 2 2 G -> 2 G G G 2\n",
      "I I Z Z Z -> Z Z I I I\n",
      "\n",
      "Instruction: replace with the other. For example:\n",
      "R H H H R -> H R R R H\n",
      "B 9 9 B B -> 9 B B 9 9\n",
      "D 2 2 2 D -> 2 D D D 2\n",
      "A A A A W -> W W W W A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_total, n_valid = 192, 64\n",
    "n_train = n_total - n_valid\n",
    "\n",
    "input_strs = [make_input_str(tasks[4], nrows=4, ncols=5) for __ in range(n_total)]\n",
    "for s in sample(input_strs, 3): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f7d6edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s.count('Yes') for s in input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e2f80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CHILDDataset(input_strs[:-n_valid], tokenizer)\n",
    "eval_dataset = CHILDDataset(input_strs[-n_valid:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3185653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_total == 1:\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=False)\n",
    "\n",
    "    # assert inputs.input_ids.size(0) == 1\n",
    "    input_ids = inputs.input_ids\n",
    "    logits = outputs.logits\n",
    "\n",
    "    bsz = input_ids.size(0); assert bsz == 1\n",
    "    labels = torch.ones_like(input_ids) * (-100)\n",
    "    for bi in range(bsz):\n",
    "        bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "        eos_indices = (input_ids[bi] == eos_id).nonzero()[-nrows:].squeeze(1)\n",
    "        for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "            print(' ' + make_example_str(example))\n",
    "            ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "            if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "            ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "            ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "            ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "            for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "                top1_correct = (dist.argmax() == ans_id).item()\n",
    "                print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                      show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6ebf074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01, adam_beta2=0.98, adam_epsilon=1e-6,\n",
    "    lr_scheduler_type='constant', learning_rate=5e-3, num_train_epochs=4,\n",
    "    logging_strategy ='epoch', evaluation_strategy ='epoch', save_steps=0,\n",
    "    no_cuda=True, report_to='none',  # to avoid report to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d89c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\n",
    "                  optimizers=(create_optimizer(model, training_args), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b37a9874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.place_model_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(elem):\n",
    "    i, v = elem\n",
    "    return _l[i - 1] if i > 0 else None\n",
    "\n",
    "false = lambda *_: False\n",
    "true  = lambda *_: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element = namedtuple('Element', 'index value')\n",
    "_l = 'A B C B'.split()\n",
    "n = len(_l)\n",
    "# l = [Element._make(e) for e in enumerate(l)]\n",
    "l = seq(_l)\n",
    "l = l.enumerate().map(Element._make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f22f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.map(lambda x: {'B': 'D'}.get(x, x))\n",
    "\n",
    "l.filter(lambda x: get_prev(x) == 'B').select(_.value)\n",
    "\n",
    "find_fn = _.index == 1\n",
    "l.filter(find_fn).select(_.value).map(lower)\n",
    "\n",
    "find_fn = _.value == 'C'\n",
    "l.filter(find_fn).select(_.index)\n",
    "\n",
    "# move x to first\n",
    "update_filter = _.value == 'C'\n",
    "get_new = lambda x: -1\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# swap first and last\n",
    "update_filter = true\n",
    "get_new = lambda x: {0: n - 1, n - 1: 0}.get(x.index, x.index)\n",
    "l.map(lambda x: Element(update_fn(x, 'index'), x.value)).order_by(_.index).select(_.value)\n",
    "\n",
    "# get inbetween == drop_while + take_while?\n",
    "\n",
    "# update by index to its prev\n",
    "update_filter = _.index == 1\n",
    "get_new = lambda x: get_prev(x)\n",
    "def update_fn(x, update_field): return get_new(x) if update_filter(x) else getattr(x, update_field)\n",
    "l.map(lambda x: Element(x.index, update_fn(x, 'value')))\n",
    "\n",
    "# if two adjacent elements by indices are equal\n",
    "l.filter(lambda x: x.index in [0, 1]).select(_.value).distinct().len() == 1\n",
    "\n",
    "seq('A B C B C'.split()).group_by(_).select(_[1]).flatten()\n",
    "\n",
    "# count occurance till current\n",
    "seq('A B A C B A'.split()).inits().reverse().tail().map(lambda x: x.filter(_ == x.last()).len())\n",
    "\n",
    "# find special\n",
    "seq('A B A A'.split()).count_by_value().filter(_[1] == 1).select(_[0])\n",
    "\n",
    "# generalized find special\n",
    "seq('A A B C C D D'.split()).group_by(_).map(lambda x: (x[0], len(x[1]))).filter(_[1] == 1).select(_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
